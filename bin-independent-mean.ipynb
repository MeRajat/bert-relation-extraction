{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Relation extraction with BERT\n",
    "\n",
    "---\n",
    "\n",
    "The goal of this notebook is to show how to use [BERT](https://arxiv.org/abs/1810.04805)\n",
    "to [extract relation](https://en.wikipedia.org/wiki/Relationship_extraction) from text.\n",
    "\n",
    "Used libraries:\n",
    "- [PyTorch](https://pytorch.org/)\n",
    "- [PyTorch-Lightning](https://pytorch-lightning.readthedocs.io/en/latest/)\n",
    "- [Transformers](https://huggingface.co/transformers/index.html)\n",
    "\n",
    "Used datasets:\n",
    "- SemEval 2010 Task 8 - [paper](https://arxiv.org/pdf/1911.10422.pdf) - [download](https://github.com/sahitya0000/Relation-Classification/blob/master/corpus/SemEval2010_task8_all_data.zip?raw=true)\n",
    "- Google IISc Distant Supervision (GIDS) - [paper](https://arxiv.org/pdf/1804.06987.pdf) - [download](https://drive.google.com/open?id=1gTNAbv8My2QDmP-OHLFtJFlzPDoCG4aI)\n",
    "- Riedel's New York Times - [paper](https://www.researchgate.net/publication/220698997_Modeling_Relations_and_Their_Mentions_without_Labeled_Text) - [download](https://drive.google.com/uc?id=1D7bZPvrSAbIPaFSG7ZswYQcPA3tmouCw&export=download)"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Install dependencies\n",
    "\n",
    "This project uses [Python 3.7+](https://www.python.org/downloads/release/python-378/)"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "!pip install requests==2.23.0 numpy==1.18.5 pandas==1.0.3 \\\n",
    "    scikit-learn==0.23.1 pytorch-lightning==0.8.4 torch==1.5.1 \\\n",
    "    transformers==3.0.2 sklearn==0.0 tqdm==4.45.0 neptune-client==0.4.119 \\\n",
    "    matplotlib==3.1.0 scikit-plot==0.3.7"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests==2.23.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (2.23.0)\r\n",
      "Requirement already satisfied: numpy==1.18.5 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (1.18.5)\r\n",
      "Requirement already satisfied: pandas==1.0.3 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (1.0.3)\r\n",
      "Requirement already satisfied: scikit-learn==0.23.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (0.23.1)\r\n",
      "Requirement already satisfied: pytorch-lightning==0.8.4 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (0.8.4)\r\n",
      "Requirement already satisfied: torch==1.5.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (1.5.1)\r\n",
      "Requirement already satisfied: transformers==3.0.2 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (3.0.2)\r\n",
      "Requirement already satisfied: sklearn==0.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (0.0)\r\n",
      "Requirement already satisfied: tqdm==4.45.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (4.45.0)\r\n",
      "Requirement already satisfied: neptune-client==0.4.119 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (0.4.119)\r\n",
      "Collecting matplotlib==3.1.0\r\n",
      "  Downloading matplotlib-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 13.1 MB 9.8 MB/s eta 0:00:01    |████████████████                | 6.6 MB 2.1 MB/s eta 0:00:04\r\n",
      "\u001B[?25hRequirement already satisfied: scikit-plot==0.3.7 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (0.3.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests==2.23.0) (2020.6.20)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests==2.23.0) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests==2.23.0) (1.25.9)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests==2.23.0) (2.10)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from pandas==1.0.3) (2020.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from pandas==1.0.3) (2.8.1)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from scikit-learn==0.23.1) (0.16.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from scikit-learn==0.23.1) (2.1.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from scikit-learn==0.23.1) (1.5.0)\r\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (5.3.1)\r\n",
      "Requirement already satisfied: tensorboard>=1.14 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (2.2.2)\r\n",
      "Requirement already satisfied: future>=0.17.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (0.18.2)\r\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (0.1.91)\r\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (0.8.1rc1)\r\n",
      "Requirement already satisfied: packaging in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (20.4)\r\n",
      "Requirement already satisfied: sacremoses in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (0.0.43)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (2020.6.8)\r\n",
      "Requirement already satisfied: filelock in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (3.0.12)\r\n",
      "Requirement already satisfied: click>=7.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from neptune-client==0.4.119) (7.1.2)\r\n",
      "Requirement already satisfied: PyJWT in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from neptune-client==0.4.119) (1.7.1)\r\n",
      "Requirement already satisfied: bravado in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from neptune-client==0.4.119) (10.6.2)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from neptune-client==0.4.119) (1.15.0)\r\n",
      "Requirement already satisfied: Pillow>=1.1.6 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from neptune-client==0.4.119) (7.2.0)\r\n",
      "Requirement already satisfied: GitPython>=2.0.8 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from neptune-client==0.4.119) (3.1.7)\r\n",
      "Requirement already satisfied: oauthlib>=2.1.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from neptune-client==0.4.119) (3.0.1)\r\n",
      "Requirement already satisfied: requests-oauthlib>=1.0.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from neptune-client==0.4.119) (1.2.0)\r\n",
      "Requirement already satisfied: websocket-client>=0.35.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from neptune-client==0.4.119) (0.57.0)\r\n",
      "Requirement already satisfied: py3nvml in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from neptune-client==0.4.119) (0.2.6)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from matplotlib==3.1.0) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from matplotlib==3.1.0) (0.10.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from matplotlib==3.1.0) (2.4.7)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.27.2)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.0.1)\r\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.34.2)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.4.1)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.17.2)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (49.1.0.post20200704)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.9.0)\r\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (3.12.3)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (3.2.2)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.6.0.post3)\r\n",
      "Requirement already satisfied: typing-extensions in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from bravado->neptune-client==0.4.119) (3.7.4.2)\r\n",
      "Requirement already satisfied: bravado-core>=5.16.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from bravado->neptune-client==0.4.119) (5.17.0)\r\n",
      "Requirement already satisfied: simplejson in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from bravado->neptune-client==0.4.119) (3.17.2)\r\n",
      "Requirement already satisfied: msgpack-python in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from bravado->neptune-client==0.4.119) (0.5.6)\r\n",
      "Requirement already satisfied: monotonic in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from bravado->neptune-client==0.4.119) (1.5)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from GitPython>=2.0.8->neptune-client==0.4.119) (4.0.5)\r\n",
      "Requirement already satisfied: xmltodict in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from py3nvml->neptune-client==0.4.119) (0.12.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (4.6)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (0.2.7)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (4.1.1)\r\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning==0.8.4) (1.7.0)\r\n",
      "Requirement already satisfied: jsonref in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.4.119) (0.2)\r\n",
      "Requirement already satisfied: msgpack>=0.5.2 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.4.119) (1.0.0)\r\n",
      "Requirement already satisfied: jsonschema[format]>=2.5.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.4.119) (3.2.0)\r\n",
      "Requirement already satisfied: swagger-spec-validator>=2.0.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.4.119) (2.7.3)\r\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client==0.4.119) (3.0.4)\r\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (0.4.8)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning==0.8.4) (3.1.0)\r\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.4.119) (19.3.0)\r\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.4.119) (0.16.0)\r\n",
      "Requirement already satisfied: rfc3987; extra == \"format\" in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.4.119) (1.3.8)\r\n",
      "Requirement already satisfied: strict-rfc3339; extra == \"format\" in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.4.119) (0.7)\r\n",
      "Requirement already satisfied: jsonpointer>1.13; extra == \"format\" in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.4.119) (2.0)\r\n",
      "Requirement already satisfied: webcolors; extra == \"format\" in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.4.119) (1.11.1)\r\n",
      "Installing collected packages: matplotlib\r\n",
      "  Attempting uninstall: matplotlib\r\n",
      "    Found existing installation: matplotlib 3.3.0\r\n",
      "    Uninstalling matplotlib-3.3.0:\r\n",
      "      Successfully uninstalled matplotlib-3.3.0\r\n",
      "Successfully installed matplotlib-3.1.0\r\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Import needed modules"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "from abc import ABC, abstractmethod\n",
    "from scikitplot.metrics import plot_confusion_matrix\n",
    "from typing import TextIO, Iterable, Tuple\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from pandas import DataFrame\n",
    "from pytorch_lightning import LightningModule, seed_everything\n",
    "from pytorch_lightning import Trainer as LightningTrainer\n",
    "from pytorch_lightning.logging.neptune import NeptuneLogger\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import column_or_1d\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import *"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define constants"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# --- Random seed ---\n",
    "SEED = 2020\n",
    "seed_everything(SEED)\n",
    "\n",
    "# --- Neptune logger ---\n",
    "NEPTUNE_API_TOKEN=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiMTU3YTAxMjctYWQwOC00YTU4LTk5Y2ItM2JmNjJmNDJjY2VkIn0=\"\n",
    "NEPTUNE_PROJECT_NAME=\"hung/bert-relation-extraction\"\n",
    "\n",
    "# --- Directory ---\n",
    "ROOT_DIR = os.path.abspath('.')\n",
    "PROCESSED_DATA_DIR = os.path.join(ROOT_DIR, 'data/processed') \n",
    "METADATA_FILE_NAME = os.path.join(PROCESSED_DATA_DIR, 'metadata.json')\n",
    "CHECKPOINT_DIR = os.path.join(ROOT_DIR, 'checkpoint')\n",
    "\n",
    "# in local environment\n",
    "RAW_DATA_DIR =  os.path.join(ROOT_DIR, 'data/raw')\n",
    "\n",
    "# in Kaggle environment\n",
    "# 3 datasets should already been added to the notebook\n",
    "# RAW_DATA_DIR = os.path.join(ROOT_DIR, '../input')\n",
    "\n",
    "# --- Datasets ---\n",
    "DATASET_MAPPING = {\n",
    "    'SemEval2010Task8': {\n",
    "        'dir': os.path.join(RAW_DATA_DIR,'semeval2010-task-8'),\n",
    "        'extract_dir': os.path.join(RAW_DATA_DIR, 'SemEval2010_task8_all_data'),\n",
    "        'url': 'https://github.com/sahitya0000/Relation-Classification/'\n",
    "               'blob/master/corpus/SemEval2010_task8_all_data.zip?raw=true',\n",
    "        'fit_in_memory': True\n",
    "    },\n",
    "    'GIDS': {\n",
    "        'dir': os.path.join(RAW_DATA_DIR,'gids-dataset'),\n",
    "        'extract_dir': os.path.join(RAW_DATA_DIR, 'gids_data'),\n",
    "        'url': 'https://drive.google.com/uc?id=1gTNAbv8My2QDmP-OHLFtJFlzPDoCG4aI&export=download',\n",
    "        'fit_in_memory': True\n",
    "    },\n",
    "    'NYT': {\n",
    "        'dir': os.path.join(RAW_DATA_DIR,'nyt-relation-extraction'),\n",
    "        'extract_dir': os.path.join(RAW_DATA_DIR, 'riedel_data'),\n",
    "        'url': 'https://drive.google.com/uc?id=1D7bZPvrSAbIPaFSG7ZswYQcPA3tmouCw&export=download',\n",
    "        'fit_in_memory': False\n",
    "    }\n",
    "}\n",
    "\n",
    "# change this variable to switch dataset in later tasks\n",
    "DATASET_NAME = 'SemEval2010Task8'\n",
    "\n",
    "# --- BERT ---\n",
    "SUB_START_CHAR = '['\n",
    "SUB_END_CHAR = ']'\n",
    "OBJ_START_CHAR = '{'\n",
    "OBJ_END_CHAR = '}'\n",
    "\n",
    "# --- BERT Model ---\n",
    "# See https://huggingface.co/transformers/pretrained_models.html for the full list\n",
    "AVAILABLE_PRETRAINED_MODELS = [\n",
    "    'distilbert-base-uncased', \n",
    "    'distilbert-base-cased', \n",
    "    'distilgpt2', \n",
    "    'bert-base-uncased',\n",
    "    'roberta-base'\n",
    "]\n",
    "\n",
    "# change this variable to switch pretrained language model in later tasks\n",
    "PRETRAINED_MODEL = AVAILABLE_PRETRAINED_MODELS[0]\n",
    "\n",
    "# if e1 is not related to e2, should \"e2 not related to e1\" be added to the trainning set\n",
    "ADD_REVERSE_RELATIONSHIP = False"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Download data\n",
    "\n",
    "This part **CAN BE SKIPPED** if this notebook is running on Kaggle environment since the dataset has already been included."
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First, we install `gdown` to download files from Google Drive"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": [
    "!pip install gdown==3.11.1\n",
    "import gdown"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Some download util functions:"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": [
    "def download_from_url(url: str, save_path: str, chunk_size: int = 2048):\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        print(f\"Downloading...\\nFrom: {url}\\nTo: {save_path}\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        for data in tqdm(response.iter_content(chunk_size=chunk_size)):\n",
    "            f.write(data)\n",
    "\n",
    "def download_from_google_drive(url: str, save_path: str):\n",
    "    gdown.download(url, save_path, use_cookies=False)\n",
    "\n",
    "def extract_zip(zip_file_path: str, extract_dir: str, remove_zip_file: bool = True):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        print(\"Extracting to \" + extract_dir)\n",
    "        for member in tqdm(zip_ref.infolist()):\n",
    "            zip_ref.extract(member, extract_dir)\n",
    "\n",
    "    if remove_zip_file:\n",
    "        print(\"Removing zip file\")\n",
    "        os.unlink(zip_file_path)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "The download function itself:"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": [
    "def download(dataset_name: str, dataset_url: str, dataset_dir: str, dataset_extract_dir: str, force_redownload: bool):\n",
    "    print(f\"\\n---> Downloading dataset {dataset_name} <---\")\n",
    "    \n",
    "    # create raw data dir\n",
    "    if not os.path.exists(RAW_DATA_DIR):\n",
    "        print(\"Creating raw data directory \" + RAW_DATA_DIR)\n",
    "        os.makedirs(RAW_DATA_DIR)\n",
    "    \n",
    "    # check data has been downloaded\n",
    "    if os.path.exists(dataset_dir):\n",
    "        if force_redownload:\n",
    "            print(f\"Removing old raw data {dataset_dir}\")\n",
    "            shutil.rmtree(dataset_dir)\n",
    "        else:\n",
    "            print(f\"Directory {dataset_dir} exists, skip downloading.\")\n",
    "            return\n",
    "\n",
    "\n",
    "    # download\n",
    "    tmp_file_path = os.path.join(RAW_DATA_DIR, dataset_name + '.zip')\n",
    "    if urlparse(dataset_url).netloc == 'drive.google.com':\n",
    "        download_from_google_drive(dataset_url, tmp_file_path)\n",
    "    else:\n",
    "        download_from_url(dataset_url, tmp_file_path)\n",
    "\n",
    "    # unzip\n",
    "    extract_zip(tmp_file_path, RAW_DATA_DIR)\n",
    "\n",
    "    # rename\n",
    "    os.rename(dataset_extract_dir, dataset_dir)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "Download all datasets:"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": [
    "def download_all_dataset():\n",
    "    for dataset_name, dataset_info in DATASET_MAPPING.items():\n",
    "        download(\n",
    "            dataset_name,\n",
    "            dataset_url=dataset_info['url'],\n",
    "            dataset_dir=dataset_info['dir'],\n",
    "            dataset_extract_dir=dataset_info['extract_dir'],\n",
    "            force_redownload=False\n",
    "        )\n",
    "\n",
    "download_all_dataset()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Preprocess"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "class OrdinalLabelEncoder(LabelEncoder):\n",
    "    def fit_transform(self, y):\n",
    "        self.fit(y)\n",
    "        return self.transform(y)\n",
    "\n",
    "    def fit(self, y):\n",
    "        if not hasattr(self, 'classes_'):\n",
    "            self.classes_ = np.array([])\n",
    "        y = column_or_1d(y, warn=True)\n",
    "        new_classes = pd.Series(y).unique()\n",
    "        new_classes = np.array([cls for cls in new_classes if cls not in self.classes_])\n",
    "        self.classes_ = np.concatenate((self.classes_, new_classes))\n",
    "        return self\n",
    "\n",
    "class AbstractPreprocessor(ABC):\n",
    "    DATASET_NAME = ''\n",
    "    VAL_DATA_PROPORTION = 0.2\n",
    "    NO_RELATION_LABEL = ''\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.SUB_START_ID, self.SUB_END_ID, self.OBJ_START_ID, self.OBJ_END_ID \\\n",
    "            = tokenizer.convert_tokens_to_ids([SUB_START_CHAR, SUB_END_CHAR, OBJ_START_CHAR, OBJ_END_CHAR])\n",
    "\n",
    "    def preprocess_data(self, reprocess: bool):\n",
    "        print(f\"\\n---> Preprocessing {self.DATASET_NAME} dataset <---\")\n",
    "        \n",
    "        # create processed data dir\n",
    "        if not os.path.exists(PROCESSED_DATA_DIR):\n",
    "            print(\"Creating processed data directory \" + PROCESSED_DATA_DIR)\n",
    "            os.makedirs(PROCESSED_DATA_DIR)\n",
    "\n",
    "        # stop preprocessing if file existed\n",
    "        json_file_names = [self.get_json_file_name(k) for k in ('train', 'val', 'test')]\n",
    "        existed_files = [fn for fn in json_file_names if os.path.exists(fn)]\n",
    "        if existed_files:\n",
    "            file_text = \"- \" + \"\\n- \".join(existed_files)\n",
    "            if not reprocess:\n",
    "                print(\"The following files already exist:\")\n",
    "                print(file_text)\n",
    "                print(\"Preprocessing is skipped. See option --reprocess.\")\n",
    "                return\n",
    "            else:\n",
    "                print(\"The following files will be overwritten:\")\n",
    "                print(file_text)\n",
    "\n",
    "        self._preprocess_data()\n",
    "\n",
    "        self._create_secondary_data_files()\n",
    "\n",
    "    @abstractmethod\n",
    "    def _preprocess_data(self):\n",
    "        pass\n",
    "\n",
    "    def _create_secondary_data_files(self):\n",
    "        for key in ['train', 'test', 'val']:\n",
    "            print(f\"Creating secondary files for {key} data\")\n",
    "\n",
    "            origin_file = open(self.get_json_file_name(key))\n",
    "            bin_file = open(self.get_json_file_name(f'{key}_binary'), \"w\")\n",
    "            related_file = open(self.get_json_file_name(f'{key}_related_only'), \"w\")\n",
    "\n",
    "            for line in tqdm(origin_file):\n",
    "                data = json.loads(line)\n",
    "                if data['label'] != 0:\n",
    "                    data['label'] -= 1\n",
    "                    related_file.write(json.dumps(data) + \"\\n\")\n",
    "                    data['label'] = 1\n",
    "                    bin_file.write(json.dumps(data) + \"\\n\")\n",
    "                else:\n",
    "                    bin_file.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "            origin_file.close()\n",
    "            bin_file.close()\n",
    "            related_file.close()\n",
    "\n",
    "    def _find_sub_obj_pos(self, input_ids_list: Iterable) -> DataFrame:\n",
    "        sub_start_pos = [self._index(s, self.SUB_START_ID) + 1 for s in input_ids_list]\n",
    "        sub_end_pos = [self._index(s, self.SUB_END_ID, sub_start_pos[i]) for i, s in enumerate(input_ids_list)]\n",
    "        obj_start_pos = [self._index(s, self.OBJ_START_ID) + 1 for s in input_ids_list]\n",
    "        obj_end_pos = [self._index(s, self.OBJ_END_ID, obj_start_pos[i]) for i, s in enumerate(input_ids_list)]\n",
    "        return DataFrame({\n",
    "            'sub_start_pos': sub_start_pos,\n",
    "            'sub_end_pos': sub_end_pos,\n",
    "            'obj_start_pos': obj_start_pos,\n",
    "            'obj_end_pos': obj_end_pos,\n",
    "        })\n",
    "\n",
    "    def _index(self, lst: list, ele: int, start: int = 0) -> int:\n",
    "        try:\n",
    "            return lst.index(ele, start)\n",
    "        except ValueError:\n",
    "            return -1\n",
    "\n",
    "    def _remove_invalid_sentences(self, data: DataFrame) -> DataFrame:\n",
    "        seq_max_len = self.tokenizer.model_max_length\n",
    "        return data.loc[\n",
    "            (data['sub_end_pos'] < seq_max_len)\n",
    "            & (data['obj_end_pos'] < seq_max_len)\n",
    "            & (data['sub_end_pos'] > -1)\n",
    "            & (data['obj_end_pos'] > -1)\n",
    "        ]\n",
    "\n",
    "    def _get_label_mapping(self, le: LabelEncoder):\n",
    "        # ignore id 0, which represent no relation\n",
    "        id_to_label = dict(enumerate(le.classes_))\n",
    "        label_to_id = {v: k for k, v in id_to_label.items()}\n",
    "        id_to_label_related_only = {k - 1: v for k, v in id_to_label.items() if k != 0}\n",
    "        label_to_id_related_only = {v: k for k, v in id_to_label_related_only.items()}\n",
    "        return {\n",
    "            'id_to_label': id_to_label,\n",
    "            'label_to_id': label_to_id,\n",
    "            'id_to_label_related_only': id_to_label_related_only,\n",
    "            'label_to_id_related_only': label_to_id_related_only,            \n",
    "        }\n",
    "\n",
    "    def _append_data_to_file(self, data: DataFrame, file: TextIO):\n",
    "        lines = \"\"\n",
    "        for _, row in data.iterrows():\n",
    "            lines += row.to_json() + \"\\n\"\n",
    "        file.write(lines)\n",
    "\n",
    "    def _save_metadata(self, metadata: dict):\n",
    "        # create metadata file\n",
    "        if not os.path.exists(METADATA_FILE_NAME):\n",
    "            print(f\"Create metadata file at {METADATA_FILE_NAME}\")\n",
    "            with open(METADATA_FILE_NAME, 'w') as f:\n",
    "                f.write(\"{}\\n\")\n",
    "\n",
    "        # add metadata\n",
    "        print(\"Saving metadata\")\n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            root_metadata = json.load(f)\n",
    "        with open(METADATA_FILE_NAME, 'w') as f:\n",
    "            root_metadata[self.DATASET_NAME] = metadata\n",
    "            json.dump(root_metadata, f, indent=4)\n",
    "\n",
    "    def _get_label_encoder(self) -> LabelEncoder:\n",
    "        le = OrdinalLabelEncoder()\n",
    "        le.fit_transform([self.NO_RELATION_LABEL]) # make sure that no relation has id 0\n",
    "        return le\n",
    "\n",
    "    @classmethod\n",
    "    def get_json_file_name(cls, key: str) -> str:\n",
    "        return os.path.join(PROCESSED_DATA_DIR, f'{cls.DATASET_NAME.lower()}_{key}.json')\n",
    "\n",
    "class SemEval2010Task8Preprocessor(AbstractPreprocessor):\n",
    "    DATASET_NAME = 'SemEval2010Task8'\n",
    "    NO_RELATION_LABEL = 'Other'\n",
    "    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING['SemEval2010Task8']['dir'],\n",
    "                                       'SemEval2010_task8_training/TRAIN_FILE.TXT')\n",
    "    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING['SemEval2010Task8']['dir'],\n",
    "                                      'SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT')\n",
    "    RAW_TRAIN_DATA_SIZE = 8000\n",
    "    RAW_TEST_DATA_SIZE = 2717\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        print(\"Processing training data\")\n",
    "        train_data = self._get_data_from_file(\n",
    "            self.RAW_TRAIN_FILE_NAME,\n",
    "            self.RAW_TRAIN_DATA_SIZE\n",
    "        )\n",
    "\n",
    "        print(\"Processing test data\")\n",
    "        test_data = self._get_data_from_file(\n",
    "            self.RAW_TEST_FILE_NAME,\n",
    "            self.RAW_TEST_DATA_SIZE\n",
    "        )\n",
    "\n",
    "        print(\"Encoding labels to integers\")\n",
    "        le = self._get_label_encoder()\n",
    "        train_data['label'] = le.fit_transform(train_data['label'])\n",
    "        test_data['label'] = le.transform(test_data['label'])\n",
    "\n",
    "        print(\"Splitting train & validate data\")\n",
    "        train_data, val_data = train_test_split(train_data, shuffle=True, random_state=SEED)\n",
    "\n",
    "        print(\"Saving to json files\")\n",
    "        with open(self.get_json_file_name('train'), 'w') as f:\n",
    "            self._append_data_to_file(train_data, f)\n",
    "        with open(self.get_json_file_name('val'), 'w') as f:\n",
    "            self._append_data_to_file(val_data, f)\n",
    "        with open(self.get_json_file_name('test'), 'w') as f:\n",
    "            self._append_data_to_file(test_data, f)\n",
    "\n",
    "        self._save_metadata({\n",
    "            'train_size': len(train_data),\n",
    "            'val_size': len(val_data),\n",
    "            'test_size': len(test_data),\n",
    "            'no_relation_label': self.NO_RELATION_LABEL,\n",
    "            **self._get_label_mapping(le)\n",
    "        })\n",
    "\n",
    "    def _get_data_from_file(self, file_name: str, dataset_size: int, reverse: bool = ADD_REVERSE_RELATIONSHIP) -> DataFrame:\n",
    "        raw_sentences = []\n",
    "        labels = []\n",
    "        with open(file_name) as f:\n",
    "            for _ in tqdm(range(dataset_size)):\n",
    "                sent = f.readline()\n",
    "                label, sub, obj = self._process_label(f.readline())\n",
    "                labels.append(label)\n",
    "                raw_sentences.append(self._process_sentence(sent, sub, obj))\n",
    "                if label == 'Other' and reverse:\n",
    "                    labels.append(label)\n",
    "                    raw_sentences.append(self._process_sentence(sent, obj, sub))\n",
    "                f.readline()\n",
    "                f.readline()\n",
    "        tokens = self.tokenizer(raw_sentences, truncation=True, padding='max_length')\n",
    "        data = DataFrame(tokens.data)\n",
    "        data['label'] = labels\n",
    "        sub_obj_position = self._find_sub_obj_pos(data['input_ids'])\n",
    "        data = pd.concat([data, sub_obj_position], axis=1)\n",
    "        data = self._remove_invalid_sentences(data)\n",
    "        return data\n",
    "\n",
    "    def _process_sentence(self, sentence: str, sub: int, obj: int) -> str:\n",
    "        return sentence.split(\"\\t\")[1][1:-2] \\\n",
    "            .replace(f\"<e{sub}>\", SUB_START_CHAR) \\\n",
    "            .replace(f\"</e{sub}>\", SUB_END_CHAR) \\\n",
    "            .replace(f\"<e{obj}>\", OBJ_START_CHAR) \\\n",
    "            .replace(f\"</e{obj}>\", OBJ_END_CHAR)\n",
    "\n",
    "    def _process_label(self, label: str) -> Tuple[str, int, int]:\n",
    "        label = label.strip()\n",
    "        if label == 'Other':\n",
    "            return label, 1, 2\n",
    "        nums = list(filter(str.isdigit, label))\n",
    "        return label, int(nums[0]), int(nums[1])\n",
    "\n",
    "class LargeDatasetPreprocessor(AbstractPreprocessor):\n",
    "    PROCESS_BATCH_SIZE = 2**12\n",
    "    NO_RELATION_LABEL = 'NA'\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        pass\n",
    "\n",
    "    def _process_batch(self, le: LabelEncoder, in_file: TextIO) -> DataFrame:\n",
    "        raw_sentences = []\n",
    "        labels = []\n",
    "\n",
    "        for _ in range(self.PROCESS_BATCH_SIZE):\n",
    "            dt = in_file.readline()\n",
    "            if dt == \"\": break # EOF\n",
    "            dt = json.loads(dt)\n",
    "\n",
    "            # add subject markup\n",
    "            sub = dt['sub']  # TODO keep _ or not?\n",
    "            obj = dt['obj']\n",
    "            new_sub = SUB_START_CHAR + ' ' + sub.replace(\"_\", \"\") + ' ' + SUB_END_CHAR\n",
    "            new_obj = OBJ_START_CHAR + ' ' +  obj.replace(\"_\", \"\") + ' ' + OBJ_END_CHAR\n",
    "            self._replace_once(dt['sent'], sub, new_sub)\n",
    "            self._replace_once(dt['sent'], obj, new_obj)\n",
    "            raw_sentences.append(\" \".join(dt['sent']))\n",
    "            labels.append(dt['rel'])\n",
    "\n",
    "        if not raw_sentences:\n",
    "            return DataFrame()\n",
    "\n",
    "        tokens = self.tokenizer(raw_sentences, truncation=True, padding='max_length')\n",
    "        data = DataFrame(tokens.data)\n",
    "        data['label'] = le.fit_transform(labels)\n",
    "        sub_obj_position = self._find_sub_obj_pos(data['input_ids'])\n",
    "        data = pd.concat([data, sub_obj_position], axis=1)\n",
    "        data = self._remove_invalid_sentences(data)\n",
    "        return data\n",
    "\n",
    "    def _replace_once(self, arr: list, element, replacement):\n",
    "        for i, e in enumerate(arr):\n",
    "            if e == element:\n",
    "                arr[i] = replacement\n",
    "                return\n",
    "            if e[:-1] == element and e[-1] in ',.?!;:':\n",
    "                arr[i] = replacement + e[-1]\n",
    "                return\n",
    "\n",
    "    def _process_subset(self, le: LabelEncoder, in_file_name, out_file_name, data_size) -> int:\n",
    "        total_data_size = 0\n",
    "        with open(in_file_name) as in_file, open(out_file_name, 'w') as out_file:\n",
    "            batch_count = math.ceil(data_size / self.PROCESS_BATCH_SIZE)\n",
    "            for _ in tqdm(range(batch_count)):\n",
    "                data = self._process_batch(le, in_file)\n",
    "                self._append_data_to_file(data, out_file)\n",
    "                total_data_size += len(data)\n",
    "        return total_data_size\n",
    "\n",
    "class GIDSPreprocessor(LargeDatasetPreprocessor):\n",
    "    DATASET_NAME = 'GIDS'\n",
    "    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING['GIDS']['dir'], 'gids_train.json')\n",
    "    RAW_VAL_FILE_NAME = os.path.join(DATASET_MAPPING['GIDS']['dir'], 'gids_dev.json')\n",
    "    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING['GIDS']['dir'], 'gids_test.json')\n",
    "    TRAIN_SIZE = 11297\n",
    "    VAL_SIZE = 1864\n",
    "    TEST_SIZE = 5663\n",
    "    PROCESS_BATCH_SIZE = 1024\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        le = self._get_label_encoder()\n",
    "        \n",
    "        print(\"Process train dataset\")\n",
    "        actual_train_size = self._process_subset(\n",
    "            le,\n",
    "            self.RAW_TRAIN_FILE_NAME,\n",
    "            self.get_json_file_name('train'),\n",
    "            self.TRAIN_SIZE\n",
    "        )\n",
    "\n",
    "        print(\"Process val dataset\")\n",
    "        actual_val_size = self._process_subset(\n",
    "            le,\n",
    "            self.RAW_VAL_FILE_NAME,\n",
    "            self.get_json_file_name('val'),\n",
    "            self.VAL_SIZE\n",
    "        )\n",
    "        \n",
    "        print(\"Process test dataset\")\n",
    "        actual_test_size = self._process_subset(\n",
    "            le, \n",
    "            self.RAW_TEST_FILE_NAME, \n",
    "            self.get_json_file_name('test'),\n",
    "            self.TEST_SIZE\n",
    "        )\n",
    "\n",
    "        self._save_metadata({\n",
    "            'train_size': actual_train_size,\n",
    "            'val_size': actual_val_size,\n",
    "            'test_size': actual_test_size,\n",
    "            'no_relation_label': self.NO_RELATION_LABEL,\n",
    "            **self._get_label_mapping(le)\n",
    "        })\n",
    "\n",
    "class NYTPreprocessor(LargeDatasetPreprocessor):\n",
    "    DATASET_NAME = 'NYT'\n",
    "    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING['NYT']['dir'], 'riedel_train.json')\n",
    "    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING['NYT']['dir'], 'riedel_test.json')\n",
    "    TRAIN_SIZE = 570084\n",
    "    TEST_SIZE = 172448\n",
    "    PROCESS_BATCH_SIZE = 4096 * 4\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        le = self._get_label_encoder()\n",
    "        actual_train_size = 0\n",
    "        actual_val_size = 0\n",
    "\n",
    "        print(\"Process train & val dataset\")\n",
    "        batch_count = math.ceil(self.TRAIN_SIZE / self.PROCESS_BATCH_SIZE)\n",
    "        with open(self.RAW_TRAIN_FILE_NAME) as in_file,\\\n",
    "                open(self.get_json_file_name('train'), 'w') as train_file,\\\n",
    "                open(self.get_json_file_name('val'), 'w') as val_file:\n",
    "                    for _ in tqdm(range(batch_count)):\n",
    "                        data = self._process_batch(le, in_file)\n",
    "                        train_data, val_data = train_test_split(data, shuffle=True, random_state=SEED)\n",
    "                        self._append_data_to_file(train_data, train_file)\n",
    "                        self._append_data_to_file(val_data, val_file)\n",
    "                        actual_train_size += len(train_data)\n",
    "                        actual_val_size += len(val_data)\n",
    "\n",
    "        print(\"Process test dataset\")\n",
    "        actual_test_size = self._process_subset(\n",
    "            le, \n",
    "            self.RAW_TEST_FILE_NAME, \n",
    "            self.get_json_file_name('test'),\n",
    "            self.TEST_SIZE\n",
    "        )\n",
    "\n",
    "        self._save_metadata({\n",
    "            'train_size': actual_train_size,\n",
    "            'val_size': actual_val_size,\n",
    "            'test_size': actual_test_size,\n",
    "            'no_relation_label': self.NO_RELATION_LABEL,\n",
    "            **self._get_label_mapping(le)\n",
    "        })\n",
    "        \n",
    "\n",
    "def get_preprocessor_class(dataset_name: DATASET_NAME):\n",
    "    return globals()[f'{dataset_name}Preprocessor']\n",
    "        \n",
    "def get_preprocessor(dataset_name: DATASET_NAME)-> AbstractPreprocessor:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL, use_fast=True)\n",
    "    preprocessors_class = get_preprocessor_class(dataset_name)\n",
    "    return preprocessors_class(tokenizer)"
   ],
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> Preprocessing GIDS dataset <---\n",
      "The following files will be overwritten:\n",
      "- /media/dthung1602/WORKING/bert-relation-extraction/data/processed/gids_train.json\n",
      "- /media/dthung1602/WORKING/bert-relation-extraction/data/processed/gids_val.json\n",
      "- /media/dthung1602/WORKING/bert-relation-extraction/data/processed/gids_test.json\n",
      "Process train dataset\n",
      "\n",
      "Process val dataset\n",
      "\n",
      "Process test dataset\n",
      "\n",
      "Saving metadata\n",
      "Creating secondary files for train data\n",
      "\n",
      "Creating secondary files for test data\n",
      "\n",
      "Creating secondary files for val data\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e8096128754443abf757141467646be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "844b3da4ee1b44a1a1f61e7c3d63f69b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7767a3db7a0b4f92ac04f00b3c94eca2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51deebd9a606453393ff4401df9043d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "527c770b6c5c4d74b9896418ba1413f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fbc923ba8190435083f780934e0a3b43"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessor = get_preprocessor('GIDS')\n",
    "preprocessor.preprocess_data(reprocess=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "### Dataset"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "class GenericDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, subset: str, batch_size: int, label_transform: str):\n",
    "        assert subset in ['train', 'val', 'test']\n",
    "        assert label_transform in ['none', 'binary', 'related_only']\n",
    "        file_name = subset if label_transform == 'none' else f'{subset}_{label_transform}'\n",
    "\n",
    "        preprocessor_class = get_preprocessor_class()\n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            metadata = json.load(f)[DATASET_NAME]\n",
    "\n",
    "        self.length = math.ceil(metadata[f'{subset}_size'] / batch_size)\n",
    "        self.fit_in_memory = DATASET_MAPPING[DATASET_NAME]['fit_in_memory']\n",
    "        self.file = open(preprocessor_class.get_json_file_name(file_name))\n",
    "\n",
    "    def __del__(self):\n",
    "        self.file.close()\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.fit_in_memory:\n",
    "            lines = self.file.readlines()\n",
    "        else:\n",
    "            lines = self.file\n",
    "\n",
    "        def get_data():\n",
    "            for line in lines:\n",
    "                data = json.loads(line)\n",
    "                input_data = {k: torch.tensor(v) for k, v in data.items() if k != 'label'}\n",
    "                yield input_data, data['label']\n",
    "\n",
    "        return get_data()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ],
   "execution_count": 49,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Torch Lightning Module"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def decay_lr(epoch):\n",
    "    return [1, 1, 0.75, 0.5, 0.25, 0.1, 0.075, 0.05, 0.025, 0.01][epoch]\n",
    "\n",
    "def get_activation_layer(activate_func_name):\n",
    "    return getattr(nn, activate_func_name)()\n",
    "\n",
    "class BaseModule(LightningModule, ABC):\n",
    "    def __init__(self, dataset_label_transform):\n",
    "        super().__init__()\n",
    "        self.proposed_answer = None\n",
    "        self.hparams.dataset_label_transform = self.dataset_label_transform = dataset_label_transform\n",
    "        self.loss_function = F.binary_cross_entropy_with_logits \\\n",
    "            if dataset_label_transform == 'binary' else self.loss_function\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader('train')\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader('val')\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader('test')\n",
    "\n",
    "    def __get_dataloader(self, subset: str) -> DataLoader:\n",
    "        batch_size = self.hparams.batch_size\n",
    "        return DataLoader(\n",
    "            GenericDataset(subset, batch_size, self.dataset_label_transform),\n",
    "            batch_size=batch_size,\n",
    "            num_workers=1\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(\n",
    "            [p for p in self.parameters() if p.requires_grad],\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        scheduler = LambdaLR(optimizer, decay_lr)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def training_step(self, batch, batch_nb) -> dict:\n",
    "        input_data, label = batch\n",
    "        y_hat = self(**input_data)\n",
    "\n",
    "        loss = self.loss_function(y_hat, label)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb) -> dict:\n",
    "        input_data, label = batch\n",
    "        y_hat = self(**input_data)\n",
    "\n",
    "        loss = self.loss_function(y_hat, label)\n",
    "\n",
    "        y_hat = torch.argmax(y_hat, dim=1)\n",
    "\n",
    "        return {\n",
    "            'y_hat': y_hat,\n",
    "            'label': label,\n",
    "            'val_loss': loss,\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> dict:\n",
    "        prefix = self.logger_prefix()\n",
    "        losses = torch.stack([x['val_loss'] for x in outputs])\n",
    "        avg_val_loss = losses.mean()\n",
    "\n",
    "        y_hat = torch.cat([x['y_hat'] for x in outputs]).cpu()\n",
    "        label = torch.cat([x['label'] for x in outputs]).cpu()\n",
    "        fig, ax = plt.subplots(figsize=(16, 12))\n",
    "        plot_confusion_matrix(label, y_hat, ax=ax)\n",
    "        self.logger.experiment.log_image(f'{prefix}val_confusion_matrix', fig)\n",
    "\n",
    "        logs = {\n",
    "            'avg_val_loss': avg_val_loss,\n",
    "            'val_acc': torch.tensor(accuracy_score(label, y_hat)),\n",
    "            'val_pre_micro': torch.tensor(precision_score(label, y_hat, average='micro')),\n",
    "            'val_rec_micro': torch.tensor(recall_score(label, y_hat, average='micro')),\n",
    "            'val_f1_micro': torch.tensor(f1_score(label, y_hat, average='micro')),\n",
    "            'val_pre_macro': torch.tensor(precision_score(label, y_hat, average='macro')),\n",
    "            'val_rec_macro': torch.tensor(recall_score(label, y_hat, average='macro')),\n",
    "            'val_f1_macro': torch.tensor(f1_score(label, y_hat, average='macro')),\n",
    "        }\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.logger.experiment.log_metric(prefix + k, v)\n",
    "\n",
    "        return {'val_loss': avg_val_loss, 'progress_bar': logs}\n",
    "\n",
    "    def test_step(self, batch, batch_nb) -> dict:\n",
    "        input_data, label = batch\n",
    "        y_hat = self(**input_data)\n",
    "\n",
    "        y_hat = torch.argmax(y_hat, dim=1)\n",
    "\n",
    "        return {\n",
    "            'y_hat': y_hat,\n",
    "            'label': label,\n",
    "        }\n",
    "\n",
    "    def test_epoch_end(self, outputs) -> dict:\n",
    "        prefix = self.logger_prefix()\n",
    "        y_hat = torch.cat([x['y_hat'] for x in outputs]).cpu()\n",
    "        label = torch.cat([x['label'] for x in outputs]).cpu()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(16, 12))\n",
    "        plot_confusion_matrix(label, y_hat, ax=ax)\n",
    "        self.logger.experiment.log_image(f'{prefix}test_confusion_matrix', fig)\n",
    "\n",
    "        logs = {\n",
    "            'test_acc': torch.tensor(accuracy_score(label, y_hat)),\n",
    "            'test_pre_micro': torch.tensor(precision_score(label, y_hat, average='micro')),\n",
    "            'test_rec_micro': torch.tensor(recall_score(label, y_hat, average='micro')),\n",
    "            'test_f1_micro': torch.tensor(f1_score(label, y_hat, average='micro')),\n",
    "            'test_pre_macro': torch.tensor(precision_score(label, y_hat, average='macro')),\n",
    "            'test_rec_macro': torch.tensor(recall_score(label, y_hat, average='macro')),\n",
    "            'test_f1_macro': torch.tensor(f1_score(label, y_hat, average='macro')),\n",
    "        }\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.logger.experiment.log_metric(prefix + k, v)\n",
    "\n",
    "        self.proposed_answer = y_hat.cpu().tolist()\n",
    "\n",
    "        return {'progress_bar': logs}\n",
    "\n",
    "    def logger_prefix(self):\n",
    "        return self.__class__.__name__ + \"_\"\n",
    "\n",
    "class BinaryClassifier(BaseModule):\n",
    "\n",
    "    def __init__(self, pretrained_language_model, dataset_name, batch_size, learning_rate,\n",
    "                 linear_size, dropout_p, activation_function, weight_decay):\n",
    "        super().__init__(dataset_label_transform=\"binary\")\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.language_model = AutoModel.from_pretrained(pretrained_language_model)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.linear = nn.Linear(self.language_model.config.hidden_size, linear_size)\n",
    "        self.activation_function = get_activation_layer(activation_function)\n",
    "        self.linear_output = nn.Linear(linear_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, sub_start_pos, sub_end_pos,\n",
    "                obj_start_pos, obj_end_pos) -> Tensor:\n",
    "        language_model_output, = self.language_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        x = torch.mean(language_model_output, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.activation_function(x)\n",
    "        x = self.dropout(x) # ??\n",
    "        logits = self.linear_output(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class RelationClassifier(BaseModule):\n",
    "\n",
    "    def __init__(self, pretrained_language_model, dataset_name, batch_size, learning_rate,\n",
    "                 dropout_p, weight_decay):\n",
    "        super().__init__(dataset_label_transform=\"related_only\")\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            num_classes = len(json.load(f)[dataset_name]['label_to_id'])\n",
    "\n",
    "        self.language_model = AutoModel.from_pretrained(pretrained_language_model)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.linear = nn.Linear(self.language_model.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, sub_start_pos, sub_end_pos,\n",
    "                obj_start_pos, obj_end_pos) -> Tensor:\n",
    "        language_model_output, = self.language_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        x = torch.mean(language_model_output, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        return logits\n"
   ],
   "execution_count": 50,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Claiming back memory & diskspace\n",
    "\n",
    "See [this](https://stackoverflow.com/a/61707643/7342188) and [this](https://stackoverflow.com/a/57860310/7342188)"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "1 / 0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "model = None\n",
    "trainer = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "\n",
    "logger = None"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    shutil.rmtree(CHECKPOINT_DIR)\n",
    "except:\n",
    "    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Logger"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "logger = NeptuneLogger(\n",
    "    api_key=NEPTUNE_API_TOKEN,\n",
    "    project_name=NEPTUNE_PROJECT_NAME,\n",
    "    close_after_fit=False,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Trainer"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "GPUS = 1\n",
    "\n",
    "BIN_MIN_EPOCHS = 1\n",
    "BIN_MAX_EPOCHS = 5\n",
    "\n",
    "bin_trainer = LightningTrainer(\n",
    "    gpus=GPUS,\n",
    "    min_epochs=BIN_MIN_EPOCHS,\n",
    "    max_epochs=BIN_MAX_EPOCHS,\n",
    "#     auto_lr_find='learning_rate',\n",
    "    default_root_dir=CHECKPOINT_DIR,\n",
    "    reload_dataloaders_every_epoch=True, # needed as we loop over a file,\n",
    "    deterministic=True,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 5\n",
    "\n",
    "trainer = LightningTrainer(\n",
    "    gpus=GPUS,\n",
    "    min_epochs=MIN_EPOCHS,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "#     auto_lr_find='learning_rate',\n",
    "    default_root_dir=CHECKPOINT_DIR,\n",
    "    reload_dataloaders_every_epoch=True, # needed as we loop over a file,\n",
    "    deterministic=True,\n",
    "    logger=logger\n",
    ")"
   ],
   "execution_count": 51,
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "\n                You requested GPUs: [0]\n                But your machine only has: []\n            ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mMisconfigurationException\u001B[0m                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-51-347b6cb72d17>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0mreload_dataloaders_every_epoch\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;31m# needed as we loop over a file,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0mdeterministic\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m     \u001B[0mlogger\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlogger\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m )\n\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/bre/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, logger, checkpoint_callback, early_stop_callback, callbacks, default_root_dir, gradient_clip_val, process_position, num_nodes, num_processes, gpus, auto_select_gpus, tpu_cores, log_gpu_memory, progress_bar_refresh_rate, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, limit_train_batches, limit_val_batches, limit_test_batches, val_check_interval, log_save_interval, row_log_interval, distributed_backend, precision, print_nan_grads, weights_summary, weights_save_path, num_sanity_val_steps, truncated_bptt_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, terminate_on_nan, auto_scale_batch_size, prepare_data_per_node, amp_level, num_tpu_cores, use_amp, show_progress_bar, val_percent_check, test_percent_check, train_percent_check, overfit_pct)\u001B[0m\n\u001B[1;32m    518\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgpus\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    519\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 520\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata_parallel_device_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_parse_gpu_ids\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgpus\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    521\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mroot_gpu\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdetermine_root_gpu_device\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata_parallel_device_ids\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    522\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mroot_device\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cpu\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/bre/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\u001B[0m in \u001B[0;36m_parse_gpu_ids\u001B[0;34m(gpus)\u001B[0m\n\u001B[1;32m    444\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mgpus\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    445\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mMisconfigurationException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"GPUs requested but none are available.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 446\u001B[0;31m     \u001B[0mgpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msanitize_gpu_ids\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgpus\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    447\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    448\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mgpus\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/bre/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\u001B[0m in \u001B[0;36msanitize_gpu_ids\u001B[0;34m(gpus)\u001B[0m\n\u001B[1;32m    403\u001B[0m                 \u001B[0mYou\u001B[0m \u001B[0mrequested\u001B[0m \u001B[0mGPUs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mgpus\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    404\u001B[0m                 \u001B[0mBut\u001B[0m \u001B[0myour\u001B[0m \u001B[0mmachine\u001B[0m \u001B[0monly\u001B[0m \u001B[0mhas\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mall_available_gpus\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 405\u001B[0;31m             \"\"\")\n\u001B[0m\u001B[1;32m    406\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mgpus\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    407\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mMisconfigurationException\u001B[0m: \n                You requested GPUs: [0]\n                But your machine only has: []\n            "
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "Create a model object:"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "BIN_BATCH_SIZE = 16\n",
    "BIN_LEARNING_RATE = 2e-05\n",
    "\n",
    "BIN_LINEAR_SIZE = 256\n",
    "\n",
    "BIN_DROPOUT_P = 0.2\n",
    "BIN_ACTIVATION_FUNCTION = \"Tanh\"\n",
    "BIN_WEIGHT_DECAY = 0.01 # default = 0.01\n",
    "\n",
    "\n",
    "binary_classifier = BinaryClassifier(\n",
    "    pretrained_language_model=PRETRAINED_MODEL,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    batch_size=BIN_BATCH_SIZE,\n",
    "    learning_rate=BIN_LEARNING_RATE,\n",
    "    linear_size=BIN_LINEAR_SIZE,\n",
    "    dropout_p=BIN_DROPOUT_P,\n",
    "    activation_function=BIN_ACTIVATION_FUNCTION,\n",
    "    weight_decay=BIN_WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-05\n",
    "\n",
    "LINEAR_SIZE = 256\n",
    "\n",
    "DROPOUT_P = 0.2\n",
    "ACTIVATION_FUNCTION = \"Tanh\"\n",
    "WEIGHT_DECAY = 0.01 # default = 0.01\n",
    "\n",
    "relation_classifier = RelationClassifier(\n",
    "    pretrained_language_model=PRETRAINED_MODEL,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    # linear_size=LINEAR_SIZE,\n",
    "    dropout_p=DROPOUT_P,\n",
    "    # activation_function=ACTIVATION_FUNCTION,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Start training binary classifier:"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "trainer.fit(binary_classifier)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Start training relation classifier:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.fit(relation_classifier)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Testing\n",
    "\n",
    "First, we test the classifiers separately"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bin_trainer.test(binary_classifier)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "trainer.test(relation_classifier)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "Then combine them:"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "relation_classifier.freeze()\n",
    "\n",
    "final_proposed_answer = []\n",
    "answer = []\n",
    "dataset = GenericDataset(subset='test', batch_size=1, label_transform='none')\n",
    "\n",
    "for i, dat in tqdm(dataset):\n",
    "    answer.append(dat[1])\n",
    "\n",
    "    # if binary classifier classified as not related, do nothing further\n",
    "    if binary_classifier.proposed_answer[i] == 0:\n",
    "        final_proposed_answer.append(0)\n",
    "        continue\n",
    "\n",
    "    # further classify relation\n",
    "    model_input = {k: torch.unsqueeze(v, dim=0) for k, v in dat[0].items()}\n",
    "    logits = relation_classifier(model_input)[0]\n",
    "    final_proposed_answer.append(torch.argmax(logits, dim=0) + 1)\n",
    "\n",
    "final_metrics = {\n",
    "    'test_combined_acc': torch.tensor(accuracy_score(answer, final_proposed_answer)),\n",
    "    'test_combined_pre_micro': torch.tensor(precision_score(answer, final_proposed_answer, average='micro')),\n",
    "    'test_combined_rec_micro': torch.tensor(recall_score(answer, final_proposed_answer, average='micro')),\n",
    "    'test_combined_f1_micro': torch.tensor(f1_score(answer, final_proposed_answer, average='micro')),\n",
    "    'test_combined_pre_macro': torch.tensor(precision_score(answer, final_proposed_answer, average='macro')),\n",
    "    'test_combined_rec_macro': torch.tensor(recall_score(answer, final_proposed_answer, average='macro')),\n",
    "    'test_combined_f1_macro': torch.tensor(f1_score(answer, final_proposed_answer, average='macro')),\n",
    "}\n",
    "\n",
    "for k, v in final_metrics:\n",
    "    logger.experiment.log_metric(k, v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the official scorer\n",
    "\n",
    "Some datasets comes with official scorers. We will run them in this session."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "class AbstractScorer(ABC):\n",
    "    @abstractmethod\n",
    "    def score(self, proposed_answer):\n",
    "        pass\n",
    "\n",
    "class SemEval2010Task8Scorer(AbstractScorer):\n",
    "    RESULT_FILE = \"semeval2010_task8_offical_score.txt\"\n",
    "    PROPOSED_ANSWER_FILE = \"semeval2010_task8_proposed_answer.txt\"\n",
    "    SCORER = \"../input/semeval2010-task-8/SemEval2010_task8_scorer-v1.2/semeval2010_task8_scorer-v1.2.pl\"\n",
    "    FORMAT_CHECKER = \"../input/semeval2010-task-8/SemEval2010_task8_scorer-v1.2/semeval2010_task8_format_checker.pl\"\n",
    "    ANSWER_KEY = \"../input/semeval2010-task-8/SemEval2010_task8_testing_keys/TEST_FILE_KEY.TXT\"\n",
    "\n",
    "    def score(self, proposed_answer):\n",
    "\n",
    "        # write test_result to file\n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            metadata = json.load(f)\n",
    "            id_to_label = {int(k): v for k, v in metadata[DATASET_NAME]['id_to_label'].items()}\n",
    "        i = 8001\n",
    "        with open(self.PROPOSED_ANSWER_FILE, \"w\") as f:\n",
    "            for r in proposed_answer:\n",
    "                f.write(f\"{i}\\t{id_to_label[r]}\\n\")\n",
    "                i += 1\n",
    "\n",
    "        # call the official scorer\n",
    "        os.system(f\"perl {self.FORMAT_CHECKER} {self.PROPOSED_ANSWER_FILE}\")\n",
    "        os.system(f\"perl {self.SCORER} {self.PROPOSED_ANSWER_FILE} {self.ANSWER_KEY} > {self.RESULT_FILE}\")\n",
    "\n",
    "        # log the official score\n",
    "        with open(self.RESULT_FILE) as f:\n",
    "            result = f.read()\n",
    "            print(result)\n",
    "        logger.experiment.log_artifact(self.RESULT_FILE)\n",
    "\n",
    "def get_offical_scorer(dataset_name: str) -> AbstractScorer:\n",
    "    return globals()[dataset_name + \"Scorer\"]()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "scorer = get_offical_scorer(DATASET_NAME)\n",
    "scorer.score(final_proposed_answer)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": "## Clean up\n\n",
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "logger.experiment.stop()",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}