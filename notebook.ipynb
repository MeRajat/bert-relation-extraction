{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation extraction with BERT\n",
    "\n",
    "---\n",
    "\n",
    "The goal of this notebook is to show how to use [BERT](https://arxiv.org/abs/1810.04805)\n",
    "to [extract relation](https://en.wikipedia.org/wiki/Relationship_extraction) from text.\n",
    "\n",
    "Used libraries:\n",
    "- [PyTorch](https://pytorch.org/)\n",
    "- [PyTorch-Lightning](https://pytorch-lightning.readthedocs.io/en/latest/)\n",
    "- [Transformers](https://huggingface.co/transformers/index.html)\n",
    "\n",
    "Used datasets:\n",
    "- SemEval 2010 Task 8 - [paper](https://arxiv.org/pdf/1911.10422.pdf) - [download](https://github.com/sahitya0000/Relation-Classification/blob/master/corpus/SemEval2010_task8_all_data.zip?raw=true)\n",
    "- Google IISc Distant Supervision (GIDS) - [paper](https://arxiv.org/pdf/1804.06987.pdf) - [download](https://drive.google.com/open?id=1gTNAbv8My2QDmP-OHLFtJFlzPDoCG4aI)\n",
    "- Riedel's New York Times - [paper](https://www.researchgate.net/publication/220698997_Modeling_Relations_and_Their_Mentions_without_Labeled_Text) - [download](https://drive.google.com/uc?id=1D7bZPvrSAbIPaFSG7ZswYQcPA3tmouCw&export=download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Install dependencies\n",
    "\n",
    "This project uses [Python 3.7+](https://www.python.org/downloads/release/python-378/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests==2.23.0 in /opt/conda/lib/python3.7/site-packages (2.23.0)\n",
      "Requirement already satisfied: numpy==1.18.5 in /opt/conda/lib/python3.7/site-packages (1.18.5)\n",
      "Requirement already satisfied: pandas==1.0.3 in /opt/conda/lib/python3.7/site-packages (1.0.3)\n",
      "Requirement already satisfied: scikit-learn==0.23.1 in /opt/conda/lib/python3.7/site-packages (0.23.1)\n",
      "Collecting pytorch-lightning==0.8.4\n",
      "  Downloading pytorch_lightning-0.8.4-py3-none-any.whl (304 kB)\n",
      "\u001B[K     |████████████████████████████████| 304 kB 2.8 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: torch==1.5.1 in /opt/conda/lib/python3.7/site-packages (1.5.1)\n",
      "Collecting transformers==3.0.2\n",
      "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
      "\u001B[K     |████████████████████████████████| 769 kB 8.1 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: sklearn==0.0 in /opt/conda/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: tqdm==4.45.0 in /opt/conda/lib/python3.7/site-packages (4.45.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests==2.23.0) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests==2.23.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests==2.23.0) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests==2.23.0) (2020.6.20)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (2.8.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.1) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.1) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.1) (0.14.1)\n",
      "Requirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (0.18.2)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (5.3.1)\n",
      "Requirement already satisfied: tensorboard>=1.14 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (2.2.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (0.0.43)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (3.0.10)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (2020.4.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (20.1)\n",
      "Collecting tokenizers==0.8.1.rc1\n",
      "  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001B[K     |████████████████████████████████| 3.0 MB 17.6 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==1.0.3) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.9.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (46.1.3.post20200325)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (3.2.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.14.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.34.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (3.12.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.30.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.2) (7.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning==0.8.4) (1.2.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (3.1.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning==0.8.4) (3.0.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (0.4.8)\n",
      "\u001B[31mERROR: allennlp 1.0.0 has requirement transformers<2.12,>=2.9, but you'll have transformers 3.0.2 which is incompatible.\u001B[0m\n",
      "Installing collected packages: pytorch-lightning, tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.7.0\n",
      "    Uninstalling tokenizers-0.7.0:\n",
      "      Successfully uninstalled tokenizers-0.7.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 2.11.0\n",
      "    Uninstalling transformers-2.11.0:\n",
      "      Successfully uninstalled transformers-2.11.0\n",
      "Successfully installed pytorch-lightning-0.8.4 tokenizers-0.8.1rc1 transformers-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install requests==2.23.0 numpy==1.18.5 pandas==1.0.3 \\\n",
    "    scikit-learn==0.23.1 pytorch-lightning==0.8.4 torch==1.5.1 \\\n",
    "    transformers==3.0.2 sklearn==0.0 tqdm==4.45.0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import TextIO, Iterable\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from pandas import DataFrame\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer as LightningTrainer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# --- Directory ---\n",
    "ROOT_DIR = os.path.abspath('.')\n",
    "PROCESSED_DATA_DIR = os.path.join(ROOT_DIR, 'data/processed') \n",
    "METADATA_FILE_NAME = os.path.join(PROCESSED_DATA_DIR, 'metadata.json')\n",
    "CHECKPOINT_DIR = os.path.join(ROOT_DIR, 'checkpoint')\n",
    "\n",
    "# in local environment\n",
    "RAW_DATA_DIR =  os.path.join(ROOT_DIR, 'data/raw')\n",
    "\n",
    "# in Kaggle environment\n",
    "# 3 datasets should already been added to the notebook\n",
    "RAW_DATA_DIR = os.path.join(ROOT_DIR, '../input')\n",
    "\n",
    "# --- Datasets ---\n",
    "DATASET_MAPPING = {\n",
    "    'SemEval2010Task8': {\n",
    "        'dir': os.path.join(RAW_DATA_DIR,'semeval2010-task-8'),\n",
    "        'extract_dir': os.path.join(RAW_DATA_DIR, 'SemEval2010_task8_all_data'),\n",
    "        'url': 'https://github.com/sahitya0000/Relation-Classification/'\n",
    "               'blob/master/corpus/SemEval2010_task8_all_data.zip?raw=true',\n",
    "        'num_classes': 10,\n",
    "    },\n",
    "    'GIDS': {\n",
    "        'dir': os.path.join(RAW_DATA_DIR,'gids-dataset'),\n",
    "        'extract_dir': os.path.join(RAW_DATA_DIR, 'gids_data'),\n",
    "        'url': 'https://drive.google.com/uc?id=1gTNAbv8My2QDmP-OHLFtJFlzPDoCG4aI&export=download',\n",
    "        'num_classes': 5\n",
    "    },\n",
    "    'NYT': {\n",
    "        'dir': os.path.join(RAW_DATA_DIR,'nyt-relation-extraction'),\n",
    "        'extract_dir': os.path.join(RAW_DATA_DIR, 'riedel_data'),\n",
    "        'url': 'https://drive.google.com/uc?id=1D7bZPvrSAbIPaFSG7ZswYQcPA3tmouCw&export=download',\n",
    "        'num_classes': 53 # TODO merge some relations\n",
    "    }\n",
    "}\n",
    "\n",
    "# change this variable to switch dataset in later tasks\n",
    "DATASET_NAME = 'GIDS'\n",
    "\n",
    "# --- BERT ---\n",
    "SUB_START_CHAR = '{'\n",
    "SUB_END_CHAR = '}'\n",
    "OBJ_START_CHAR = '['\n",
    "OBJ_END_CHAR = ']'\n",
    "\n",
    "# --- BERT Model ---\n",
    "# See https://huggingface.co/transformers/pretrained_models.html for the full list\n",
    "\n",
    "BERT_VARIANT_MAPPING = {\n",
    "    'bert': {\n",
    "        'model': BertModel,\n",
    "        'tokenizer': BartTokenizerFast,\n",
    "        'pretrain_weight': 'bert-base-uncased',\n",
    "        'hidden_state_size': 768\n",
    "    },\n",
    "    'distilbert': {\n",
    "        'model': DistilBertModel,\n",
    "        'tokenizer': DistilBertTokenizerFast,\n",
    "        'pretrain_weight': 'distilbert-base-uncased',\n",
    "        'hidden_state_size': 768\n",
    "    },\n",
    "    'roberta': {\n",
    "        'model': RobertaModel,\n",
    "        'tokenizer': RobertaTokenizerFast,\n",
    "        'pretrain_weight': 'roberta-base',\n",
    "        'hidden_state_size': 768\n",
    "    },\n",
    "}\n",
    "\n",
    "# change this variable to switch BERT variant in later tasks\n",
    "BERT_VARIANT = 'distilbert'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "\n",
    "This part **CAN BE SKIPPED** if this notebook is running on Kaggle environment since the dataset has already been included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install `gdown` to download files from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown==3.11.1\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some download util functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download_from_url(url: str, save_path: str, chunk_size: int = 2048):\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        print(f\"Downloading...\\nFrom: {url}\\nTo: {save_path}\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        for data in tqdm(response.iter_content(chunk_size=chunk_size)):\n",
    "            f.write(data)\n",
    "\n",
    "def download_from_google_drive(url: str, save_path: str):\n",
    "    gdown.download(url, save_path, use_cookies=False)\n",
    "\n",
    "def extract_zip(zip_file_path: str, extract_dir: str, remove_zip_file: bool = True):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        print(\"Extracting to \" + extract_dir)\n",
    "        for member in tqdm(zip_ref.infolist()):\n",
    "            zip_ref.extract(member, extract_dir)\n",
    "\n",
    "    if remove_zip_file:\n",
    "        print(\"Removing zip file\")\n",
    "        os.unlink(zip_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The download function itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download(dataset_name: str, dataset_url: str, dataset_dir: str, dataset_extract_dir: str, force_redownload: bool):\n",
    "    print(f\"\\n---> Downloading dataset {dataset_name} <---\")\n",
    "    \n",
    "    # create raw data dir\n",
    "    if not os.path.exists(RAW_DATA_DIR):\n",
    "        print(\"Creating raw data directory \" + RAW_DATA_DIR)\n",
    "        os.makedirs(RAW_DATA_DIR)\n",
    "    \n",
    "    # check data has been downloaded\n",
    "    if os.path.exists(dataset_dir):\n",
    "        if force_redownload:\n",
    "            print(f\"Removing old raw data {dataset_dir}\")\n",
    "            shutil.rmtree(dataset_dir)\n",
    "        else:\n",
    "            print(f\"Directory {dataset_dir} exists, skip downloading.\")\n",
    "            return\n",
    "\n",
    "\n",
    "    # download\n",
    "    tmp_file_path = os.path.join(RAW_DATA_DIR, dataset_name + '.zip')\n",
    "    if urlparse(dataset_url).netloc == 'drive.google.com':\n",
    "        download_from_google_drive(dataset_url, tmp_file_path)\n",
    "    else:\n",
    "        download_from_url(dataset_url, tmp_file_path)\n",
    "\n",
    "    # unzip\n",
    "    extract_zip(tmp_file_path, RAW_DATA_DIR)\n",
    "\n",
    "    # rename\n",
    "    os.rename(dataset_extract_dir, dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Download all datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download_all_dataset():\n",
    "    for dataset_name, dataset_info in DATASET_MAPPING.items():\n",
    "        download(\n",
    "            dataset_name,\n",
    "            dataset_url=dataset_info['url'],\n",
    "            dataset_dir=dataset_info['dir'],\n",
    "            dataset_extract_dir=dataset_info['extract_dir'],\n",
    "            force_redownload=False\n",
    "        )\n",
    "\n",
    "download_all_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> Preprocessing SemEval2010Task8 dataset <---\n",
      "Processing training data\n",
      "\n",
      "Processing test data\n",
      "\n",
      "Encoding labels to integers\n",
      "Splitting train & validate data\n",
      "Saving to json files\n",
      "Saving metadata\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=8000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c641c216b3e4094a8d59a833d7230e8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2717.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cee7b80765e94232b0b92c5c551dee9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class AbstractPreprocessor(ABC):\n",
    "    DATASET_NAME = ''\n",
    "    RANDOM_SEED = 2020\n",
    "    VAL_DATA_PROPORTION = 0.2\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.SUB_START_ID, self.SUB_END_ID, self.OBJ_START_ID, self.OBJ_END_ID \\\n",
    "            = tokenizer.convert_tokens_to_ids([SUB_START_CHAR, SUB_END_CHAR, OBJ_START_CHAR, OBJ_END_CHAR])\n",
    "\n",
    "    def preprocess_data(self, reprocess: bool):\n",
    "        print(f\"\\n---> Preprocessing {self.DATASET_NAME} dataset <---\")\n",
    "        \n",
    "        # create processed data dir\n",
    "        if not os.path.exists(PROCESSED_DATA_DIR):\n",
    "            print(\"Creating processed data directory \" + PROCESSED_DATA_DIR)\n",
    "            os.makedirs(PROCESSED_DATA_DIR)\n",
    "\n",
    "        # stop preprocessing if file existed\n",
    "        json_file_names = [self.get_json_file_name(k) for k in ('train', 'val', 'test')]\n",
    "        existed_files = [fn for fn in json_file_names if os.path.exists(fn)]\n",
    "        if existed_files:\n",
    "            file_text = \"- \" + \"\\n- \".join(existed_files)\n",
    "            if not reprocess:\n",
    "                print(\"The following files already exist:\")\n",
    "                print(file_text)\n",
    "                print(\"Preprocessing is skipped. See option --reprocess.\")\n",
    "                return\n",
    "            else:\n",
    "                print(\"The following files will be overwritten:\")\n",
    "                print(file_text)\n",
    "\n",
    "        self._preprocess_data()\n",
    "\n",
    "    @abstractmethod\n",
    "    def _preprocess_data(self):\n",
    "        pass\n",
    "\n",
    "    def _find_sub_obj_pos(self, input_ids_list: Iterable) -> DataFrame:\n",
    "        return DataFrame({\n",
    "            'sub_start_pos': [self._index(s, self.SUB_START_ID) + 1 for s in input_ids_list],\n",
    "            'sub_end_pos': [self._index(s, self.SUB_END_ID) for s in input_ids_list],\n",
    "            'obj_start_pos': [self._index(s, self.OBJ_START_ID) + 1 for s in input_ids_list],\n",
    "            'obj_end_pos': [self._index(s, self.OBJ_END_ID) for s in input_ids_list],\n",
    "        })\n",
    "\n",
    "    def _index(self, l: list, e: int) -> int:\n",
    "        try:\n",
    "            return l.index(e)\n",
    "        except ValueError:\n",
    "            return -1\n",
    "\n",
    "    def _remove_invalid_sentences(self, data: DataFrame) -> DataFrame:\n",
    "        seq_max_len = self.tokenizer.model_max_length\n",
    "        return data.loc[\n",
    "            (data['sub_end_pos'] < seq_max_len)\n",
    "            & (data['obj_end_pos'] < seq_max_len)\n",
    "            & (data['sub_end_pos'] > -1)\n",
    "            & (data['obj_end_pos'] > -1)\n",
    "        ]\n",
    "\n",
    "    def _get_label_mapping(self, le: LabelEncoder):\n",
    "        id_to_label = dict(enumerate(le.classes_))\n",
    "        label_to_id = {v: k for k, v in id_to_label.items()}\n",
    "        return {\n",
    "            'id_to_label': id_to_label,\n",
    "            'label_to_id': label_to_id\n",
    "        }\n",
    "\n",
    "    def _append_data_to_file(self, data: DataFrame, file: TextIO):\n",
    "        lines = \"\"\n",
    "        for _, row in data.iterrows():\n",
    "            lines += row.to_json() + \"\\n\"\n",
    "        file.write(lines)\n",
    "\n",
    "    def _save_metadata(self, metadata: dict):\n",
    "        # create metadata file\n",
    "        if not os.path.exists(METADATA_FILE_NAME):\n",
    "            print(f\"Create metadata file at {METADATA_FILE_NAME}\")\n",
    "            with open(METADATA_FILE_NAME, 'w') as f:\n",
    "                f.write(\"{}\\n\")\n",
    "\n",
    "        # add metadata\n",
    "        print(\"Saving metadata\")\n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            root_metadata = json.load(f)\n",
    "        with open(METADATA_FILE_NAME, 'w') as f:\n",
    "            root_metadata[self.DATASET_NAME] = metadata\n",
    "            json.dump(root_metadata, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def get_json_file_name(cls, key: str) -> str:\n",
    "        return os.path.join(PROCESSED_DATA_DIR, f'{cls.DATASET_NAME.lower()}_{key}.json')\n",
    "\n",
    "class SemEval2010Task8Preprocessor(AbstractPreprocessor):\n",
    "    DATASET_NAME = 'SemEval2010Task8'\n",
    "    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING['SemEval2010Task8']['dir'],\n",
    "                                       'SemEval2010_task8_training/TRAIN_FILE.TXT')\n",
    "    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING['SemEval2010Task8']['dir'],\n",
    "                                      'SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT')\n",
    "    RAW_TRAIN_DATA_SIZE = 8000\n",
    "    RAW_TEST_DATA_SIZE = 2717\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        print(\"Processing training data\")\n",
    "        train_data = self._get_data_from_file(\n",
    "            self.RAW_TRAIN_FILE_NAME,\n",
    "            self.RAW_TRAIN_DATA_SIZE\n",
    "        )\n",
    "\n",
    "        print(\"Processing test data\")\n",
    "        test_data = self._get_data_from_file(\n",
    "            self.RAW_TEST_FILE_NAME,\n",
    "            self.RAW_TEST_DATA_SIZE\n",
    "        )\n",
    "\n",
    "        print(\"Encoding labels to integers\")\n",
    "        le = LabelEncoder()\n",
    "        train_data['label'] = le.fit_transform(train_data['label'])\n",
    "        test_data['label'] = le.transform(test_data['label'])\n",
    "\n",
    "        print(\"Splitting train & validate data\")\n",
    "        train_data, val_data = train_test_split(train_data, shuffle=True, random_state=self.RANDOM_SEED)\n",
    "\n",
    "        print(\"Saving to json files\")\n",
    "        with open(self.get_json_file_name('train'), 'w') as f:\n",
    "            self._append_data_to_file(train_data, f)\n",
    "        with open(self.get_json_file_name('val'), 'w') as f:\n",
    "            self._append_data_to_file(val_data, f)\n",
    "        with open(self.get_json_file_name('test'), 'w') as f:\n",
    "            self._append_data_to_file(test_data, f)\n",
    "\n",
    "        self._save_metadata({\n",
    "            'train_size': len(train_data) ,\n",
    "            'val_size': len(val_data),\n",
    "            'test_size': len(test_data),\n",
    "            **self._get_label_mapping(le)\n",
    "        })\n",
    "\n",
    "    def _get_data_from_file(self, file_name: str, dataset_size: int) -> DataFrame:\n",
    "        raw_sentences = []\n",
    "        labels = []\n",
    "        with open(file_name) as f:\n",
    "            for _ in tqdm(range(dataset_size)):\n",
    "                raw_sentences.append(self._process_sentence(f.readline()))\n",
    "                labels.append(self._process_label(f.readline()))\n",
    "                f.readline()\n",
    "                f.readline()\n",
    "        tokens = self.tokenizer(raw_sentences, truncation=True, padding=True)\n",
    "        data = DataFrame(tokens.data)\n",
    "        data['label'] = labels\n",
    "        sub_obj_position = self._find_sub_obj_pos(data['input_ids'])\n",
    "        data = pd.concat([data, sub_obj_position], axis=1)\n",
    "        data = self._remove_invalid_sentences(data)\n",
    "        return data\n",
    "\n",
    "    def _process_sentence(self, sentence: str) -> str:\n",
    "        # TODO distinguish e1 e2 sub obj\n",
    "        return sentence.split(\"\\t\")[1][1:-2] \\\n",
    "            .replace(\"<e1>\", SUB_START_CHAR) \\\n",
    "            .replace(\"</e1>\", SUB_END_CHAR) \\\n",
    "            .replace(\"<e2>\", OBJ_START_CHAR) \\\n",
    "            .replace(\"</e2>\", OBJ_END_CHAR)\n",
    "\n",
    "    def _process_label(self, label: str) -> str:\n",
    "        return \"Other\" if label == 'Other\\n' else label[:-8]\n",
    "\n",
    "class LargeDatasetPreprocessor(AbstractPreprocessor):\n",
    "    PROCESS_BATCH_SIZE = 2**12\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        pass\n",
    "\n",
    "    def _process_batch(self, le: LabelEncoder, in_file: TextIO) -> DataFrame:\n",
    "        raw_sentences = []\n",
    "        labels = []\n",
    "\n",
    "        for _ in range(self.PROCESS_BATCH_SIZE):\n",
    "            dt = in_file.readline()\n",
    "            if dt == \"\": break # EOF\n",
    "            dt = json.loads(dt)\n",
    "\n",
    "            # add subject markup\n",
    "            sub = dt['sub']  # TODO keep _ or not?\n",
    "            obj = dt['obj']\n",
    "            new_sub = SUB_START_CHAR + ' ' + sub.replace(\"_\", \"\") + ' ' + SUB_END_CHAR\n",
    "            new_obj = OBJ_START_CHAR + ' ' +  obj.replace(\"_\", \"\") + ' ' + OBJ_END_CHAR\n",
    "            self._replace_once(dt['sent'], sub, new_sub)\n",
    "            self._replace_once(dt['sent'], obj, new_obj)\n",
    "            raw_sentences.append(\" \".join(dt['sent']))\n",
    "            labels.append(dt['rel'])\n",
    "\n",
    "        if not raw_sentences:\n",
    "            return DataFrame()\n",
    "\n",
    "        tokens = self.tokenizer(raw_sentences, truncation=True, padding=True)\n",
    "        data = DataFrame(tokens.data)\n",
    "        data['label'] = le.fit_transform(labels)\n",
    "        sub_obj_position = self._find_sub_obj_pos(data['input_ids'])\n",
    "        data = pd.concat([data, sub_obj_position], axis=1)\n",
    "        data = self._remove_invalid_sentences(data)\n",
    "        return data\n",
    "\n",
    "    def _replace_once(self, arr: list, element, replacement):\n",
    "        for i, e in enumerate(arr):\n",
    "            if e == element:\n",
    "                arr[i] = replacement\n",
    "                return\n",
    "            if e[:-1] == element and e[-1] in ',.?!;:':\n",
    "                arr[i] = replacement + e[-1]\n",
    "                return\n",
    "\n",
    "    def _process_subset(self, le: LabelEncoder, in_file_name, out_file_name, data_size) -> int:\n",
    "        total_data_size = 0\n",
    "        with open(in_file_name) as in_file, open(out_file_name, 'w') as out_file:\n",
    "            batch_count = math.ceil(data_size / self.PROCESS_BATCH_SIZE)\n",
    "            for _ in tqdm(range(batch_count)):\n",
    "                data = self._process_batch(le, in_file)\n",
    "                self._append_data_to_file(data, out_file)\n",
    "                total_data_size += len(data)\n",
    "        return total_data_size\n",
    "\n",
    "class GIDSPreprocessor(LargeDatasetPreprocessor):\n",
    "    DATASET_NAME = 'GIDS'\n",
    "    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING['GIDS']['dir'], 'gids_train.json')\n",
    "    RAW_VAL_FILE_NAME = os.path.join(DATASET_MAPPING['GIDS']['dir'], 'gids_dev.json')\n",
    "    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING['GIDS']['dir'], 'gids_test.json')\n",
    "    TRAIN_SIZE = 11297\n",
    "    VAL_SIZE = 1864\n",
    "    TEST_SIZE = 5663\n",
    "    PROCESS_BATCH_SIZE = 1024\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        print(\"Process train dataset\")\n",
    "        actual_train_size = self._process_subset(\n",
    "            le,\n",
    "            self.RAW_TRAIN_FILE_NAME,\n",
    "            self.get_json_file_name('train'),\n",
    "            self.TRAIN_SIZE\n",
    "        )\n",
    "\n",
    "        print(\"Process val dataset\")\n",
    "        actual_val_size = self._process_subset(\n",
    "            le,\n",
    "            self.RAW_VAL_FILE_NAME,\n",
    "            self.get_json_file_name('val'),\n",
    "            self.VAL_SIZE\n",
    "        )\n",
    "        \n",
    "        print(\"Process test dataset\")\n",
    "        actual_test_size = self._process_subset(\n",
    "            le, \n",
    "            self.RAW_TEST_FILE_NAME, \n",
    "            self.get_json_file_name('test'),\n",
    "            self.TEST_SIZE\n",
    "        )\n",
    "\n",
    "        self._save_metadata({\n",
    "            'train_size': actual_train_size,\n",
    "            'val_size': actual_val_size,\n",
    "            'test_size': actual_test_size,\n",
    "            **self._get_label_mapping(le)\n",
    "        })\n",
    "\n",
    "class NYTPreprocessor(LargeDatasetPreprocessor):\n",
    "    DATASET_NAME = 'NYT'\n",
    "    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING['NYT']['dir'], 'riedel_train.json')\n",
    "    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING['NYT']['dir'], 'riedel_test.json')\n",
    "    TRAIN_SIZE = 570084\n",
    "    TEST_SIZE = 172448\n",
    "    PROCESS_BATCH_SIZE = 4096 * 4\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        le = LabelEncoder()\n",
    "        actual_train_size = 0\n",
    "        actual_val_size = 0\n",
    "\n",
    "        print(\"Process train & val dataset\")\n",
    "        batch_count = math.ceil(self.TRAIN_SIZE / self.PROCESS_BATCH_SIZE)\n",
    "        with open(self.RAW_TRAIN_FILE_NAME) as in_file,\\\n",
    "                open(self.get_json_file_name('train'), 'w') as train_file,\\\n",
    "                open(self.get_json_file_name('val'), 'w') as val_file:\n",
    "                    for _ in tqdm(range(batch_count)):\n",
    "                        data = self._process_batch(le, in_file)\n",
    "                        train_data, val_data = train_test_split(data, shuffle=True, random_state=self.RANDOM_SEED)\n",
    "                        self._append_data_to_file(train_data, train_file)\n",
    "                        self._append_data_to_file(val_data, val_file)\n",
    "                        actual_train_size += len(train_data)\n",
    "                        actual_val_size += len(val_data)\n",
    "\n",
    "        print(\"Process test dataset\")\n",
    "        actual_test_size = self._process_subset(\n",
    "            le, \n",
    "            self.RAW_TEST_FILE_NAME, \n",
    "            self.get_json_file_name('test'),\n",
    "            self.TEST_SIZE\n",
    "        )\n",
    "\n",
    "        self._save_metadata({\n",
    "            'train_size': actual_train_size,\n",
    "            'val_size': actual_val_size,\n",
    "            'test_size': actual_test_size,\n",
    "            **self._get_label_mapping(le)\n",
    "        })\n",
    "\n",
    "def get_preprocessor_class(dataset_name: str):\n",
    "    return globals()[f'{dataset_name}Preprocessor']\n",
    "\n",
    "def get_preprocessor(dataset_name: str)-> AbstractPreprocessor:\n",
    "    bert_model_info = BERT_VARIANT_MAPPING[BERT_VARIANT]\n",
    "    bert_pretrain_weight = bert_model_info['pretrain_weight']\n",
    "    tokenizer = bert_model_info['tokenizer'].from_pretrained(bert_pretrain_weight)\n",
    "    preprocessors_class = get_preprocessor_class(dataset_name)\n",
    "    return preprocessors_class(tokenizer)\n",
    "\n",
    "preprocessor = get_preprocessor('SemEval2010Task8')\n",
    "preprocessor.preprocess_data(reprocess=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model\n",
    "\n",
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GenericDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, dataset_name: str, subset: str, batch_size: int):\n",
    "        if subset not in ['train', 'val', 'test']:\n",
    "            raise ValueError('subset must be train, val or test')\n",
    "            \n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            metadata = json.load(f)\n",
    "        self.length = math.ceil(metadata[dataset_name][f'{subset}_size'] / batch_size)\n",
    "        \n",
    "        preprocessor_class = get_preprocessor_class(dataset_name)\n",
    "        self.file = open(preprocessor_class.get_json_file_name(subset))\n",
    "\n",
    "    def __del__(self):\n",
    "        self.file.close()\n",
    "\n",
    "    def __iter__(self):\n",
    "        def get_data():\n",
    "            for line in self.file:\n",
    "                data = json.loads(line)\n",
    "                input_data = {k: torch.tensor(v) for k, v in data.items() if k != 'label'}\n",
    "                yield input_data, data['label']\n",
    "\n",
    "        return get_data()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Torch Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BERTModule(LightningModule):\n",
    "\n",
    "    def __init__(self, bert_variant, dataset_name, batch_size, learning_rate,\n",
    "                 bert_cls_size, bert_entity_size):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.cls_stream = torch.cuda.Stream()\n",
    "        self.obj_stream = torch.cuda.Stream()\n",
    "        self.sub_stream = torch.cuda.Stream()\n",
    "\n",
    "        bert_info = BERT_VARIANT_MAPPING[bert_variant]\n",
    "        bert_model_class = bert_info['model']\n",
    "        bert_pretrain_weight = bert_info['pretrain_weight']\n",
    "        self.bert = bert_model_class.from_pretrained(bert_pretrain_weight, output_attentions=True)\n",
    "        \n",
    "        self.cls_linear = nn.Linear(self.bert.config.hidden_size, bert_cls_size)\n",
    "        self.cls_activate = nn.PReLU()\n",
    "        self.sub_linear = nn.Linear(self.bert.config.hidden_size, bert_entity_size)\n",
    "        self.sub_activate = nn.PReLU()\n",
    "        self.obj_linear = nn.Linear(self.bert.config.hidden_size, bert_entity_size)\n",
    "        self.obj_activate = nn.PReLU()\n",
    "        \n",
    "        dataset_info = DATASET_MAPPING[dataset_name]\n",
    "        self.linear = nn.Linear(bert_cls_size + 2 * bert_entity_size, dataset_info['num_classes'])\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader('train')\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader('val')\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader('test')\n",
    "\n",
    "    def __get_dataloader(self, subset: str) -> DataLoader:\n",
    "        print(f\"Loading {subset} data\")\n",
    "        return DataLoader(\n",
    "            GenericDataset(self.hparams.dataset_name, subset),\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            #shuffle=(subset == 'train'),\n",
    "            num_workers=1\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self) -> Optimizer:\n",
    "        return AdamW(\n",
    "            [p for p in self.parameters() if p.requires_grad],\n",
    "            lr=self.hparams.learning_rate,\n",
    "            eps=1e-08\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, sub_start_pos, sub_end_pos,\n",
    "                obj_start_pos, obj_end_pos) -> Tensor:\n",
    "        bert_output, _ = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        bert_cls = bert_output[:, 0]\n",
    "        bert_sub = torch.mean(bert_output[:, sub_start_pos, sub_end_pos], dim=1)\n",
    "        bert_obj = torch.mean(bert_output[:, obj_start_pos, obj_end_pos], dim=1)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        with torch.cuda.stream(self.cls_stream):\n",
    "            cls_output = self.cls_activate(self.cls_linear(bert_cls))\n",
    "        with torch.cuda.stream(self.sub_stream):\n",
    "            sub_output = self.sub_activate(self.sub_linear(bert_sub))\n",
    "        with torch.cuda.stream(self.obj_stream):\n",
    "            obj_output = self.obj_activate(self.obj_linear(bert_obj))\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        linear_input = torch.cat((cls_output, sub_output, obj_output), dim=1)\n",
    "        logits = self.linear(linear_input)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_nb) -> dict:\n",
    "        input_data, label = batch\n",
    "        y_hat = self(**input_data)\n",
    "\n",
    "        loss = F.cross_entropy(y_hat, label)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb) -> dict:\n",
    "        input_data, label = batch\n",
    "        y_hat = self(**input_data)\n",
    "\n",
    "        loss = F.cross_entropy(y_hat, label)\n",
    "\n",
    "        _, y_hat = torch.max(y_hat, dim=1)\n",
    "        y_hat = y_hat.cpu()\n",
    "        label = label.cpu()\n",
    "\n",
    "        return {\n",
    "            'val_loss': loss,\n",
    "            'val_pre': torch.tensor(precision_score(label, y_hat, average='micro')),\n",
    "            'val_rec': torch.tensor(recall_score(label, y_hat, average='micro')),\n",
    "            'val_acc': torch.tensor(accuracy_score(label, y_hat)),\n",
    "            'val_f1': torch.tensor(f1_score(label, y_hat, average='micro'))\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> dict:\n",
    "        avg_val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_val_pre = torch.stack([x['val_pre'] for x in outputs]).mean()\n",
    "        avg_val_rec = torch.stack([x['val_rec'] for x in outputs]).mean()\n",
    "        avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        avg_val_f1 = torch.stack([x['val_f1'] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            'avg_val_loss': avg_val_loss,\n",
    "            'avg_val_pre': avg_val_pre,\n",
    "            'avg_val_rec': avg_val_rec,\n",
    "            'avg_val_acc': avg_val_acc,\n",
    "            'avg_val_f1': avg_val_f1,\n",
    "        }\n",
    "        return {'val_loss': avg_val_loss, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def test_step(self, batch, batch_nb) -> dict:\n",
    "        input_data, label = batch\n",
    "        y_hat = self(**input_data)\n",
    "\n",
    "        a, y_hat = torch.max(y_hat, dim=1)\n",
    "        \n",
    "        y_hat = y_hat.cpu()\n",
    "        label = label.cpu()\n",
    "        \n",
    "        test_pre = precision_score(label, y_hat, average='micro')\n",
    "        test_rec = recall_score(label, y_hat, average='micro')\n",
    "        test_acc = accuracy_score(label, y_hat)\n",
    "        test_f1 = f1_score(label, y_hat, average='micro')\n",
    "\n",
    "        return {\n",
    "            'test_pre': torch.tensor(test_pre),\n",
    "            'test_rec': torch.tensor(test_rec),\n",
    "            'test_acc': torch.tensor(test_acc),\n",
    "            'test_f1': torch.tensor(test_f1),\n",
    "        }\n",
    "\n",
    "    def test_epoch_end(self, outputs) -> dict:\n",
    "        avg_test_pre = torch.stack([x['test_pre'] for x in outputs]).mean()\n",
    "        avg_test_rec = torch.stack([x['test_rec'] for x in outputs]).mean()\n",
    "        avg_test_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
    "        avg_test_f1 = torch.stack([x['test_f1'] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            'avg_test_pre': avg_test_pre,\n",
    "            'avg_test_rec': avg_test_rec,\n",
    "            'avg_test_acc': avg_test_acc,\n",
    "            'avg_test_f1': avg_test_f1,\n",
    "        }\n",
    "        return {'progress_bar': tensorboard_logs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claiming back memory\n",
    "\n",
    "See [this](https://stackoverflow.com/a/61707643/7342188) and [this](https://stackoverflow.com/a/57860310/7342188)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mZeroDivisionError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-bc757c3fda29>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;36m1\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mZeroDivisionError\u001B[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "GPUS = 1\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 4\n",
    "\n",
    "trainer = LightningTrainer(\n",
    "    gpus=GPUS,\n",
    "    min_epochs=MIN_EPOCHS,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    default_root_dir=CHECKPOINT_DIR,\n",
    "    reload_dataloaders_every_epoch=True # needed as we loop over a file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Create a model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-05\n",
    "\n",
    "BERT_CLS_SIZE = 64\n",
    "BERT_ENTITY_SIZE = 64\n",
    "\n",
    "model = BERTModule(\n",
    "    bert_variant=BERT_VARIANT,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    bert_cls_size=BERT_CLS_SIZE,\n",
    "    bert_entity_size=BERT_ENTITY_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name   | Type            | Params\n",
      "-------------------------------------------\n",
      "0 | bert   | DistilBertModel | 66 M  \n",
      "1 | linear | Linear          | 3 K   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading val data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading val data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4532b6ba9b644419b952b47e7814e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading val data\n",
      "Loading train data\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.test(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}