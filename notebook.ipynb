{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Relation extraction with BERT\n\n---\n\nThe goal of this notebook is to show how to use [BERT](https://arxiv.org/abs/1810.04805)\nto [extract relation](https://en.wikipedia.org/wiki/Relationship_extraction) from text.\n\nUsed libraries:\n- [PyTorch](https://pytorch.org/)\n- [PyTorch-Lightning](https://pytorch-lightning.readthedocs.io/en/latest/)\n- [Transformers](https://huggingface.co/transformers/index.html)\n\nUsed datasets:\n- SemEval 2010 Task 8 - [paper](https://arxiv.org/pdf/1911.10422.pdf) - [download](https://github.com/sahitya0000/Relation-Classification/blob/master/corpus/SemEval2010_task8_all_data.zip?raw=true)\n- Google IISc Distant Supervision (GIDS) - [paper](https://arxiv.org/pdf/1804.06987.pdf) - [download](https://drive.google.com/open?id=1gTNAbv8My2QDmP-OHLFtJFlzPDoCG4aI)\n- Riedel's New York Times - [paper](https://www.researchgate.net/publication/220698997_Modeling_Relations_and_Their_Mentions_without_Labeled_Text) - [download](https://drive.google.com/uc?id=1D7bZPvrSAbIPaFSG7ZswYQcPA3tmouCw&export=download)"},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## Install dependencies\n\nThis project uses [Python 3.7+](https://www.python.org/downloads/release/python-378/)"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"!pip install requests==2.23.0 numpy==1.18.5 pandas==1.0.3 \\\n    scikit-learn==0.23.1 pytorch-lightning==0.8.4 torch==1.5.1 \\\n    transformers==3.0.2 sklearn==0.0 tqdm==4.45.0 \n","execution_count":1,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: requests==2.23.0 in /opt/conda/lib/python3.7/site-packages (2.23.0)\nRequirement already satisfied: numpy==1.18.5 in /opt/conda/lib/python3.7/site-packages (1.18.5)\nRequirement already satisfied: pandas==1.0.3 in /opt/conda/lib/python3.7/site-packages (1.0.3)\nRequirement already satisfied: scikit-learn==0.23.1 in /opt/conda/lib/python3.7/site-packages (0.23.1)\nCollecting pytorch-lightning==0.8.4\n  Downloading pytorch_lightning-0.8.4-py3-none-any.whl (304 kB)\n\u001b[K     |████████████████████████████████| 304 kB 2.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: torch==1.5.1 in /opt/conda/lib/python3.7/site-packages (1.5.1)\nCollecting transformers==3.0.2\n  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n\u001b[K     |████████████████████████████████| 769 kB 8.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: sklearn==0.0 in /opt/conda/lib/python3.7/site-packages (0.0)\nRequirement already satisfied: tqdm==4.45.0 in /opt/conda/lib/python3.7/site-packages (4.45.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests==2.23.0) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests==2.23.0) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests==2.23.0) (1.24.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests==2.23.0) (2020.6.20)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (2019.3)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (2.8.1)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.1) (1.4.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.1) (2.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.1) (0.14.1)\nRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (0.18.2)\nRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (5.3.1)\nRequirement already satisfied: tensorboard>=1.14 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (2.2.2)\nRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (0.1.91)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (0.0.43)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (3.0.10)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (2020.4.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (20.1)\nCollecting tokenizers==0.8.1.rc1\n  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n\u001b[K     |████████████████████████████████| 3.0 MB 17.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==1.0.3) (1.14.0)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.9.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (46.1.3.post20200325)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.7.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.4.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (3.2.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.14.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.0.1)\nRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.34.2)\nRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (3.12.2)\nRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.30.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.2) (7.1.1)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3.0.2) (2.4.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning==0.8.4) (1.2.0)\nRequirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (4.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (3.1.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning==0.8.4) (3.0.1)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (0.4.8)\n\u001b[31mERROR: allennlp 1.0.0 has requirement transformers<2.12,>=2.9, but you'll have transformers 3.0.2 which is incompatible.\u001b[0m\nInstalling collected packages: pytorch-lightning, tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.7.0\n    Uninstalling tokenizers-0.7.0:\n      Successfully uninstalled tokenizers-0.7.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 2.11.0\n    Uninstalling transformers-2.11.0:\n      Successfully uninstalled transformers-2.11.0\nSuccessfully installed pytorch-lightning-0.8.4 tokenizers-0.8.1rc1 transformers-3.0.2\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Import needed modules"},{"metadata":{"pycharm":{"name":"#%% \n"},"trusted":true},"cell_type":"code","source":"import gc\nimport json\nimport math\nimport os\nimport shutil\nimport zipfile\nfrom abc import ABC, abstractmethod\nfrom urllib.parse import urlparse\n\nimport requests\nimport torch\nfrom pytorch_lightning import LightningModule\nfrom pytorch_lightning import Trainer as LightningTrainer\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch import Tensor, nn\nfrom torch.nn import functional as F\nfrom torch.optim import AdamW\nfrom torch.optim.optimizer import Optimizer\nfrom torch.utils.data import DataLoader, IterableDataset\nfrom tqdm.auto import tqdm\nfrom transformers import *","execution_count":2,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## Define constants"},{"metadata":{"pycharm":{"name":"#%% \n"},"trusted":true},"cell_type":"code","source":"# --- Directory ---\nROOT_DIR = os.path.abspath('.')\nPROCESSED_DATA_DIR = os.path.join(ROOT_DIR, 'data/processed') \nMETADATA_FILE_NAME = os.path.join(PROCESSED_DATA_DIR, 'metadata.json')\nCHECKPOINT_DIR = os.path.join(ROOT_DIR, 'checkpoint')\n\n# in local environment\nRAW_DATA_DIR =  os.path.join(ROOT_DIR, 'data/raw')\n\n# in Kaggle environment\n# 3 datasets should already been added to the notebook\nRAW_DATA_DIR = os.path.join(ROOT_DIR, '../input')\n\n# --- Datasets ---\nDATASET_MAPPING = {\n    'SemEval2010Task8': {\n        'dir': os.path.join(RAW_DATA_DIR,'semeval2010-task-8'),\n        'extract_dir': os.path.join(RAW_DATA_DIR, 'SemEval2010_task8_all_data'),\n        'url': 'https://github.com/sahitya0000/Relation-Classification/'\n               'blob/master/corpus/SemEval2010_task8_all_data.zip?raw=true',\n        'num_classes': 10,\n    },\n    'GIDS': {\n        'dir': os.path.join(RAW_DATA_DIR,'gids-dataset'),\n        'extract_dir': os.path.join(RAW_DATA_DIR, 'gids_data'),\n        'url': 'https://drive.google.com/uc?id=1gTNAbv8My2QDmP-OHLFtJFlzPDoCG4aI&export=download',\n        'num_classes': 5\n    },\n    'NYT': {\n        'dir': os.path.join(RAW_DATA_DIR,'nyt-relation-extraction'),\n        'extract_dir': os.path.join(RAW_DATA_DIR, 'riedel_data'),\n        'url': 'https://drive.google.com/uc?id=1D7bZPvrSAbIPaFSG7ZswYQcPA3tmouCw&export=download',\n        'num_classes': 53 # TODO merge some relations\n    }\n}\n\n# change this variable to switch dataset in later tasks\nDATASET_NAME = 'GIDS'\n\n# --- BERT ---\nSUB_START_CHAR = '{'\nSUB_END_CHAR = '}'\nOBJ_START_CHAR = '['\nOBJ_END_CHAR = ']'\n\n# --- BERT Model ---\n# See https://huggingface.co/transformers/pretrained_models.html for the full list\n\nBERT_VARIANT_MAPPING = {\n    'bert': {\n        'model': BertModel,\n        'tokenizer': BartTokenizerFast,\n        'pretrain_weight': 'bert-base-uncased',\n        'hidden_state_size': 768\n    },\n    'distilbert': {\n        'model': DistilBertModel,\n        'tokenizer': DistilBertTokenizerFast,\n        'pretrain_weight': 'distilbert-base-uncased',\n        'hidden_state_size': 768\n    },\n    'roberta': {\n        'model': RobertaModel,\n        'tokenizer': RobertaTokenizerFast,\n        'pretrain_weight': 'roberta-base',\n        'hidden_state_size': 768\n    },\n}\n\n# change this variable to switch BERT variant in later tasks\nBERT_VARIANT = 'distilbert'","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Download data\n\nThis part **CAN BE SKIPPED** if this notebook is running on Kaggle environment since the dataset has already been included."},{"metadata":{},"cell_type":"markdown","source":"First, we install `gdown` to download files from Google Drive"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install gdown==3.11.1\nimport gdown","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some download util functions:"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"def download_from_url(url: str, save_path: str, chunk_size: int = 2048):\n    with open(save_path, \"wb\") as f:\n        print(f\"Downloading...\\nFrom: {url}\\nTo: {save_path}\")\n        response = requests.get(url, stream=True)\n        for data in tqdm(response.iter_content(chunk_size=chunk_size)):\n            f.write(data)\n\ndef download_from_google_drive(url: str, save_path: str):\n    gdown.download(url, save_path, use_cookies=False)\n\ndef extract_zip(zip_file_path: str, extract_dir: str, remove_zip_file=True):\n    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n        print(\"Extracting to \" + extract_dir)\n        for member in tqdm(zip_ref.infolist()):\n            zip_ref.extract(member, extract_dir)\n\n    if remove_zip_file:\n        print(\"Removing zip file\")\n        os.unlink(zip_file_path)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"The download function itself:"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"def download(dataset_name, dataset_url, dataset_dir, dataset_extract_dir, force_redownload: bool):\n    print(f\"\\n---> Downloading dataset {dataset_name} <---\")\n    \n    # create raw data dir\n    if not os.path.exists(RAW_DATA_DIR):\n        print(\"Creating raw data directory \" + RAW_DATA_DIR)\n        os.makedirs(RAW_DATA_DIR)\n    \n    # check data has been downloaded\n    if os.path.exists(dataset_dir):\n        if force_redownload:\n            print(f\"Removing old raw data {dataset_dir}\")\n            shutil.rmtree(dataset_dir)\n        else:\n            print(f\"Directory {dataset_dir} exists, skip downloading.\")\n            return\n\n\n    # download\n    tmp_file_path = os.path.join(RAW_DATA_DIR, dataset_name + '.zip')\n    if urlparse(dataset_url).netloc == 'drive.google.com':\n        download_from_google_drive(dataset_url, tmp_file_path)\n    else:\n        download_from_url(dataset_url, tmp_file_path)\n\n    # unzip\n    extract_zip(tmp_file_path, RAW_DATA_DIR)\n\n    # rename\n    os.rename(dataset_extract_dir, dataset_dir)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"Download all datasets:"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"collapsed":true},"cell_type":"code","source":"def download_all_dataset():\n    for dataset_name, dataset_info in DATASET_MAPPING.items():\n        download(\n            dataset_name,\n            dataset_url=dataset_info['url'],\n            dataset_dir=dataset_info['dir'],\n            dataset_extract_dir=dataset_info['extract_dir'],\n            force_redownload=False\n        )\n\ndownload_all_dataset()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## Preprocess\n\nThe abstract preprocessor"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"class AbstractPreprocessor(ABC):\n    DATASET_NAME = ''\n    RANDOM_SEED = 2020\n    VAL_DATA_PROPORTION = 0.2\n\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        self.tokenizer = tokenizer\n\n    def preprocess_data(self, reprocess: bool):\n        print(f\"\\n---> Preprocessing {self.DATASET_NAME} dataset <---\")\n        \n        # create processed data dir\n        if not os.path.exists(PROCESSED_DATA_DIR):\n            print(\"Creating processed data directory \" + PROCESSED_DATA_DIR)\n            os.makedirs(PROCESSED_DATA_DIR)\n\n        # stop preprocessing if file existed\n        pickled_file_names = [self.get_json_file_name(k) for k in ('train', 'val', 'test')]\n        existed_files = [fn for fn in pickled_file_names if os.path.exists(fn)]\n        if existed_files:\n            file_text = \"- \" + \"\\n- \".join(existed_files)\n            if not reprocess:\n                print(\"The following files already exist:\")\n                print(file_text)\n                print(\"Preprocessing is skipped. See option --reprocess.\")\n                return\n            else:\n                print(\"The following files will be overwritten:\")\n                print(file_text)\n\n        self._preprocess_data()\n\n    @abstractmethod\n    def _preprocess_data(self):\n        pass\n\n    def _train_val_split(self, original_data):\n        def get_sample(d, idxs):\n            return [d[i] for i in idxs]\n\n        k = list(original_data.keys())[0]\n        indies = list(range(len(original_data[k])))\n        train_indies, val_indies = train_test_split(\n            indies,\n            test_size=self.VAL_DATA_PROPORTION,\n            random_state=self.RANDOM_SEED\n        )\n        train_data = {k: get_sample(v, train_indies) for k, v in original_data.items()}\n        val_data = {k: get_sample(v, val_indies) for k, v in original_data.items()}\n\n        return train_data, val_data\n\n    def _append_data_to_file(self, data: dict, file):\n        keys = list(data.keys())\n        lines = \"\"\n        for values in zip(*data.values()):\n            tmp = {k: v for k, v in zip(keys, values)}\n            lines += json.dumps(tmp) + \"\\n\"\n        file.write(lines)\n\n    def _save_metadata(self, metadata: dict):\n        if set(metadata.keys()) != {'train_size', 'val_size', 'test_size'}:\n            raise ValueError(\"Invalid metadata\")\n\n        # create metadata file\n        if not os.path.exists(METADATA_FILE_NAME):\n            print(f\"Create metadata file at {METADATA_FILE_NAME}\")\n            with open(METADATA_FILE_NAME, 'w') as f:\n                f.write(\"{}\\n\")\n\n        # add metadata\n        print(\"Saving metadata\")\n        with open(METADATA_FILE_NAME) as f:\n            root_metadata = json.load(f)\n        with open(METADATA_FILE_NAME, 'w') as f:\n            root_metadata[self.DATASET_NAME] = metadata\n            json.dump(root_metadata, f, indent=4)\n\n    @classmethod\n    def get_json_file_name(cls, key: str):\n        return os.path.join(PROCESSED_DATA_DIR, f'{cls.DATASET_NAME.lower()}_{key}.json')\n","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Custom preprocessor for each dataset:"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"class SemEval2010Task8Preprocessor(AbstractPreprocessor):\n    DATASET_NAME = 'SemEval2010Task8'\n    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING['SemEval2010Task8']['dir'],\n                                       'SemEval2010_task8_training/TRAIN_FILE.TXT')\n    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING['SemEval2010Task8']['dir'],\n                                      'SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT')\n    RAW_TRAIN_DATA_SIZE = 8000\n    RAW_TEST_DATA_SIZE = 2717\n\n    def _preprocess_data(self):\n        print(\"Processing training data\")\n        train_data = self._get_data_from_file(\n            self.RAW_TRAIN_FILE_NAME,\n            self.RAW_TRAIN_DATA_SIZE\n        )\n\n        print(\"Processing test data\")\n        test_data = self._get_data_from_file(\n            self.RAW_TEST_FILE_NAME,\n            self.RAW_TEST_DATA_SIZE\n        )\n\n        print(\"Encoding labels to integers\")\n        le = LabelEncoder()\n        le.fit(train_data['label'])\n        train_data['label'] = le.transform(train_data['label']).tolist()\n        test_data['label'] = le.transform(test_data['label']).tolist()\n\n        print(\"Splitting train & validate data\")\n        train_data, val_data = self._train_val_split(train_data)\n\n        print(\"Saving to json files\")\n        with open(self.get_json_file_name('train'), 'w') as f:\n            self._append_data_to_file(train_data, f)\n        with open(self.get_json_file_name('val'), 'w') as f:\n            self._append_data_to_file(val_data, f)\n        with open(self.get_json_file_name('test'), 'w') as f:\n            self._append_data_to_file(test_data, f)\n\n        self._save_metadata({\n            'train_size': len(train_data['label']) ,\n            'val_size': len(val_data['label']),\n            'test_size': len(test_data['label'])\n        })\n\n    def _get_data_from_file(self, file_name: str, dataset_size: int):\n        raw_sentences = []\n        labels = []\n        with open(file_name) as f:\n            for _ in tqdm(range(dataset_size)):\n                raw_sentences.append(self._process_sentence(f.readline()))\n                labels.append(self._process_label(f.readline()))\n                f.readline()\n                f.readline()\n        data = self.tokenizer(raw_sentences, truncation=True, padding=True)\n        data['label'] = labels\n        return data\n\n    def _process_sentence(self, sentence: str):\n        # TODO distinguish e1 e2 sub obj\n        return sentence.split(\"\\t\")[1][1:-2] \\\n            .replace(\"<e1>\", SUB_START_CHAR) \\\n            .replace(\"</e1>\", SUB_END_CHAR) \\\n            .replace(\"<e2>\", OBJ_START_CHAR) \\\n            .replace(\"</e2>\", OBJ_END_CHAR)\n\n    def _process_label(self, label: str):\n        return label[:-8]\n\n\nclass LargeDatasetPreprocessor(AbstractPreprocessor):\n    PROCESS_BATCH_SIZE = 2**12\n\n    def _preprocess_data(self):\n        pass\n\n    def _process_batch(self, le: LabelEncoder, in_file):\n        raw_sentences = []\n        labels = []\n        for _ in range(self.PROCESS_BATCH_SIZE):\n            dt = in_file.readline()\n            if dt == \"\": break # EOF\n            dt = json.loads(dt)\n\n            # add subject markup\n            sentence = \" \".join(dt['sent'])\n            new_sub = SUB_START_CHAR + dt['sub'].replace('_', '') + SUB_END_CHAR # TODO keep _ or not?\n            new_obj = OBJ_START_CHAR + dt['obj'].replace('_', '') + OBJ_END_CHAR\n            sentence = sentence.replace(dt['sub'], new_sub).replace(dt['obj'], new_obj)\n            raw_sentences.append(sentence)\n            labels.append(dt['rel'])\n\n        data = self.tokenizer(raw_sentences, truncation=True, padding=True)\n        data['label'] = le.fit_transform(labels).tolist()\n\n        return data\n\n    def _process_subset(self, le: LabelEncoder, in_file_name, out_file_name, data_size):\n        with open(in_file_name) as in_file, open(out_file_name, 'w') as out_file:\n            for _ in tqdm(range(math.ceil(data_size / self.PROCESS_BATCH_SIZE))):\n                data = self._process_batch(le, in_file)\n                self._append_data_to_file(data, out_file)\n\nclass GIDSPreprocessor(LargeDatasetPreprocessor):\n    DATASET_NAME = 'GIDS'\n    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING['GIDS']['dir'], 'gids_train.json')\n    RAW_VAL_FILE_NAME = os.path.join(DATASET_MAPPING['GIDS']['dir'], 'gids_dev.json')\n    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING['GIDS']['dir'], 'gids_test.json')\n    TRAIN_SIZE = 11297\n    VAL_SIZE = 1864\n    TEST_SIZE = 5663\n    PROCESS_BATCH_SIZE = 1024\n\n    def _preprocess_data(self):\n        le = LabelEncoder()\n        \n        print(\"Process train dataset\")\n        self._process_subset(\n            le,\n            self.RAW_TRAIN_FILE_NAME,\n            self.get_json_file_name('train'),\n            self.TRAIN_SIZE\n        )\n        \n        print(\"Process val dataset\")\n        self._process_subset(\n            le,\n            self.RAW_VAL_FILE_NAME,\n            self.get_json_file_name('val'),\n            self.VAL_SIZE\n        )\n        \n        print(\"Process test dataset\")\n        self._process_subset(\n            le, \n            self.RAW_TEST_FILE_NAME, \n            self.get_json_file_name('test'),\n            self.TEST_SIZE\n        )\n\n        self._save_metadata({\n            'train_size': self.TRAIN_SIZE,\n            'val_size': self.VAL_SIZE,\n            'test_size': self.TEST_SIZE\n        })\n\nclass NYTPreprocessor(LargeDatasetPreprocessor):\n    DATASET_NAME = 'NYT'\n    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING['NYT']['dir'], 'riedel_train.json')\n    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING['NYT']['dir'], 'riedel_test.json')\n    TRAIN_SIZE = 570084\n    TEST_SIZE = 172448\n    PROCESS_BATCH_SIZE = 4096 * 4\n\n    def _preprocess_data(self):\n        le = LabelEncoder()\n\n        print(\"Process train & val dataset\")\n        batch_count = math.ceil(self.TRAIN_SIZE / self.PROCESS_BATCH_SIZE)\n        with open(self.RAW_TRAIN_FILE_NAME) as in_file,\\\n                open(self.get_json_file_name('train'), 'w') as train_file,\\\n                open(self.get_json_file_name('val'), 'w') as val_file:\n                    for _ in tqdm(range(batch_count)):\n                        data = self._process_batch(le, in_file)\n                        train_data, val_data = self._train_val_split(data)\n                        self._append_data_to_file(train_data, train_file)\n                        self._append_data_to_file(val_data, val_file)\n\n        print(\"Process test dataset\")\n        self._process_subset(\n            le, \n            self.RAW_TEST_FILE_NAME, \n            self.get_json_file_name('test'),\n            self.TEST_SIZE\n        )\n\n        full_batch_count, extra = divmod(self.TRAIN_SIZE, self.PROCESS_BATCH_SIZE)\n        val_data_per_full_batch = math.ceil(self.PROCESS_BATCH_SIZE * self.VAL_DATA_PROPORTION)\n        val_data_extra = math.ceil(self.VAL_DATA_PROPORTION * extra)\n        val_size = full_batch_count * val_data_per_full_batch + val_data_extra\n\n        self._save_metadata({\n            'train_size': self.TRAIN_SIZE - val_size,\n            'val_size': val_size,\n            'test_size': self.TEST_SIZE\n        })\n","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Factory function to get preprocessor:"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"def get_preprocessor_class(dataset_name: str):\n    return globals()[f'{dataset_name}Preprocessor']\n\ndef get_preprocessor(dataset_name: str)-> AbstractPreprocessor:\n    bert_model_info = BERT_VARIANT_MAPPING[BERT_VARIANT]\n    bert_pretrain_weight = bert_model_info['pretrain_weight']\n    tokenizer = bert_model_info['tokenizer'].from_pretrained(bert_pretrain_weight)\n    preprocessors_class = get_preprocessor_class(dataset_name)\n    return preprocessors_class(tokenizer)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocess data:"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"preprocessor = get_preprocessor(DATASET_NAME)\npreprocessor.preprocess_data(reprocess=True)","execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e10ad295ab74d689725c0a591e937e0"}},"metadata":{}},{"output_type":"stream","text":"\n\n---> Preprocessing GIDS dataset <---\nCreating processed data directory /kaggle/working/data/processed\nProcess train dataset\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0039766032942bfa72e73a665fd3ce3"}},"metadata":{}},{"output_type":"stream","text":"\nProcess val dataset\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"727556ecb8b04ff4b61c6d460e72cf6c"}},"metadata":{}},{"output_type":"stream","text":"\nProcess test dataset\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09e0bd4d4cc942d7a2ba7959d28aa42b"}},"metadata":{}},{"output_type":"stream","text":"\nCreate metadata file at /kaggle/working/data/processed/metadata.json\nSaving metadata\n","name":"stdout"}]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## Model\n\n### Dataset"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"class GenericDataset(IterableDataset):\n\n    def __init__(self, dataset_name: str, subset: str, batch_size: int):\n        if subset not in ['train', 'val', 'test']:\n            raise ValueError('subset must be train, val or test')\n            \n        with open(METADATA_FILE_NAME) as f:\n            metadata = json.load(f)\n        self.length = math.ceil(metadata[dataset_name][f'{subset}_size'] / batch_size)\n        \n        preprocessor_class = get_preprocessor_class(dataset_name)\n        self.file = open(preprocessor_class.get_json_file_name(subset))\n\n    def __del__(self):\n        self.file.close()\n\n    def __iter__(self):\n        def get_data():\n            for line in self.file:\n                data = json.loads(line)\n                yield (\n                    torch.tensor(data['input_ids']),\n                    torch.tensor(data['attention_mask']),\n                    torch.tensor(data['label'])\n                )\n\n        return get_data()\n    \n    def __len__(self):\n        return self.length","execution_count":14,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"### Torch Lightning Module"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"class BERTModule(LightningModule):\n\n    def __init__(self, bert_variant, dataset_name, batch_size, learning_rate):\n        super().__init__()\n        self.save_hyperparameters()\n\n        bert_info = BERT_VARIANT_MAPPING[bert_variant]\n        bert_model_class = bert_info['model']\n        bert_pretrain_weight = bert_info['pretrain_weight']\n        self.bert = bert_model_class.from_pretrained(bert_pretrain_weight, output_attentions=True)\n\n        dataset_info = DATASET_MAPPING[dataset_name]\n        self.num_classes = dataset_info['num_classes']\n        self.linear = nn.Linear(self.bert.config.hidden_size, self.num_classes)\n\n    def train_dataloader(self) -> DataLoader:\n        return self.__get_dataloader('train')\n\n    def val_dataloader(self) -> DataLoader:\n        return self.__get_dataloader('val')\n\n    def test_dataloader(self) -> DataLoader:\n        return self.__get_dataloader('test')\n\n    def __get_dataloader(self, subset: str) -> DataLoader:\n        print(f\"Loading {subset} data\")\n        return DataLoader(\n            GenericDataset(self.hparams.dataset_name, subset),\n            batch_size=self.hparams.batch_size,\n            #shuffle=(subset == 'train'),\n            num_workers=1\n        )\n\n    def configure_optimizers(self) -> Optimizer:\n        return AdamW(\n            [p for p in self.parameters() if p.requires_grad],\n            lr=self.hparams.learning_rate,\n            eps=1e-08\n        )\n\n    def forward(self, input_ids, attention_mask) -> Tensor:\n        bert_output, _ = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        bert_cls = bert_output[:, 0]\n        logits = self.linear(bert_cls)\n        return logits\n\n    def training_step(self, batch, batch_nb) -> dict:\n        input_ids, attention_mask, label = batch\n\n        y_hat = self(input_ids, attention_mask)\n\n        loss = F.cross_entropy(y_hat, label)\n        tensorboard_logs = {'train_loss': loss}\n\n        return {'loss': loss, 'log': tensorboard_logs}\n\n    def validation_step(self, batch, batch_nb) -> dict:\n        input_ids, attention_mask, label = batch\n        \n        y_hat = self(input_ids, attention_mask)\n\n        loss = F.cross_entropy(y_hat, label)\n\n        a, y_hat = torch.max(y_hat, dim=1)\n        y_hat = y_hat.cpu()\n        label = label.cpu()\n\n        return {\n            'val_loss': loss,\n            'val_pre': torch.tensor(precision_score(label, y_hat, average='micro')),\n            'val_rec': torch.tensor(recall_score(label, y_hat, average='micro')),\n            'val_acc': torch.tensor(accuracy_score(label, y_hat)),\n            'val_f1': torch.tensor(f1_score(label, y_hat, average='micro'))\n        }\n\n    def validation_epoch_end(self, outputs) -> dict:\n        avg_val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        avg_val_pre = torch.stack([x['val_pre'] for x in outputs]).mean()\n        avg_val_rec = torch.stack([x['val_rec'] for x in outputs]).mean()\n        avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n        avg_val_f1 = torch.stack([x['val_f1'] for x in outputs]).mean()\n\n        tensorboard_logs = {\n            'avg_val_loss': avg_val_loss,\n            'avg_val_pre': avg_val_pre,\n            'avg_val_rec': avg_val_rec,\n            'avg_val_acc': avg_val_acc,\n            'avg_val_f1': avg_val_f1,\n        }\n        return {'val_loss': avg_val_loss, 'progress_bar': tensorboard_logs}\n\n    def test_step(self, batch, batch_nb) -> dict:\n        input_ids, attention_mask, label = batch\n\n        y_hat = self(input_ids, attention_mask)\n        a, y_hat = torch.max(y_hat, dim=1)\n        \n        y_hat = y_hat.cpu()\n        label = label.cpu()\n        \n        test_pre = precision_score(label, y_hat, average='micro')\n        test_rec = recall_score(label, y_hat, average='micro')\n        test_acc = accuracy_score(label, y_hat)\n        test_f1 = f1_score(label, y_hat, average='micro')\n\n        return {\n            'test_pre': torch.tensor(test_pre),\n            'test_rec': torch.tensor(test_rec),\n            'test_acc': torch.tensor(test_acc),\n            'test_f1': torch.tensor(test_f1),\n        }\n\n    def test_epoch_end(self, outputs) -> dict:\n        avg_test_pre = torch.stack([x['test_pre'] for x in outputs]).mean()\n        avg_test_rec = torch.stack([x['test_rec'] for x in outputs]).mean()\n        avg_test_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n        avg_test_f1 = torch.stack([x['test_f1'] for x in outputs]).mean()\n\n        tensorboard_logs = {\n            'avg_test_pre': avg_test_pre,\n            'avg_test_rec': avg_test_rec,\n            'avg_test_acc': avg_test_acc,\n            'avg_test_f1': avg_test_f1,\n        }\n        return {'progress_bar': tensorboard_logs}","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Claiming back memory\n\nSee [this](https://stackoverflow.com/a/61707643/7342188) and [this](https://stackoverflow.com/a/57860310/7342188)"},{"metadata":{"trusted":true},"cell_type":"code","source":"1 / 0","execution_count":16,"outputs":[{"output_type":"error","ename":"ZeroDivisionError","evalue":"division by zero","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-bc757c3fda29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = None\ngc.collect()\ntorch.cuda.empty_cache()","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Trainer"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"GPUS = 1\nMIN_EPOCHS = 1\nMAX_EPOCHS = 4\n\ntrainer = LightningTrainer(\n    gpus=GPUS,\n    min_epochs=MIN_EPOCHS,\n    max_epochs=MAX_EPOCHS,\n    default_root_dir=CHECKPOINT_DIR,\n    reload_dataloaders_every_epoch=True # needed as we loop over a file\n)","execution_count":18,"outputs":[{"output_type":"stream","text":"GPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## Training\n\nCreate a model object:"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\nLEARNING_RATE = 2e-05\n\n\nmodel = BERTModule(\n    bert_variant=BERT_VARIANT,\n    dataset_name=DATASET_NAME,\n    batch_size=BATCH_SIZE,\n    learning_rate=LEARNING_RATE\n)","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Start training:"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"trainer.fit(model)","execution_count":null,"outputs":[{"output_type":"stream","text":"\n  | Name   | Type            | Params\n-------------------------------------------\n0 | bert   | DistilBertModel | 66 M  \n1 | linear | Linear          | 3 K   \n","name":"stderr"},{"output_type":"stream","text":"Loading val data\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"stream","text":"Loading val data\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4532b6ba9b644419b952b47e7814e12"}},"metadata":{}},{"output_type":"stream","text":"Loading train data\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"stream","text":"Loading val data\nLoading train data\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Testing"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"trainer.test(model)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}