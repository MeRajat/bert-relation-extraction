{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Relation extraction with BERT\n",
    "\n",
    "---\n",
    "\n",
    "The goal of this repo is to show how to use [BERT](https://arxiv.org/abs/1810.04805)\n",
    "to [extract relation](https://en.wikipedia.org/wiki/Relationship_extraction) from text.\n",
    "\n",
    "Used libraries:\n",
    "- [Transformers](https://huggingface.co/transformers/index.html)\n",
    "- [PyTorch-Lightning](https://pytorch-lightning.readthedocs.io/en/latest/)\n",
    "\n",
    "Used datasets:\n",
    "- SemEval 2010 Task 8 - [paper](https://arxiv.org/pdf/1911.10422.pdf) - [download](https://github.com/sahitya0000/Relation-Classification/blob/master/corpus/SemEval2010_task8_all_data.zip?raw=true)\n",
    "-  Google IISc Distant Supervision (GIDS) - [paper](https://arxiv.org/pdf/1804.06987.pdf) - [download](https://drive.google.com/open?id=1gTNAbv8My2QDmP-OHLFtJFlzPDoCG4aI)"
   ]
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Install dependencies\n",
    "\n",
    "This project uses [Python 3.7+](https://www.python.org/downloads/release/python-378/)"
   ]
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true,
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "!pip install requests==2.23.0 numpy==1.18.5 pandas==1.0.3 \\\n",
    "    scikit-learn==0.23.1 pytorch-lightning==0.8.4 torch==1.5.1 \\\n",
    "    transformers==3.0.2 sklearn==0.0 tqdm==4.45.0 \n"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests==2.23.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (2.23.0)\r\n",
      "Requirement already satisfied: numpy==1.18.5 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (1.18.5)\r\n",
      "Requirement already satisfied: pandas==1.0.3 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (1.0.3)\r\n",
      "Requirement already satisfied: scikit-learn==0.23.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (0.23.1)\r\n",
      "Requirement already satisfied: pytorch-lightning==0.8.4 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (0.8.4)\r\n",
      "Requirement already satisfied: torch==1.5.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (1.5.1)\r\n",
      "Requirement already satisfied: transformers==3.0.2 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (3.0.2)\r\n",
      "Requirement already satisfied: sklearn==0.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (0.0)\r\n",
      "Requirement already satisfied: tqdm==4.45.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (4.45.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests==2.23.0) (2.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests==2.23.0) (2020.6.20)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests==2.23.0) (1.25.9)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests==2.23.0) (3.0.4)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from pandas==1.0.3) (2020.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from pandas==1.0.3) (2.8.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from scikit-learn==0.23.1) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from scikit-learn==0.23.1) (0.16.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from scikit-learn==0.23.1) (1.5.0)\r\n",
      "Requirement already satisfied: tensorboard>=1.14 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (2.2.2)\r\n",
      "Requirement already satisfied: future>=0.17.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (0.18.2)\r\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (5.3.1)\r\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (0.1.91)\r\n",
      "Requirement already satisfied: packaging in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (20.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (2020.6.8)\r\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (0.8.1rc1)\r\n",
      "Requirement already satisfied: sacremoses in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (0.0.43)\r\n",
      "Requirement already satisfied: filelock in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (3.0.12)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==1.0.3) (1.15.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.4.1)\r\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.34.2)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.27.2)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.17.2)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.9.0)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.6.0.post3)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.0.1)\r\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (3.12.3)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (49.1.0.post20200704)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (3.2.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from packaging->transformers==3.0.2) (2.4.7)\r\n",
      "Requirement already satisfied: click in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from sacremoses->transformers==3.0.2) (7.1.2)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning==0.8.4) (1.2.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (0.2.7)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (4.6)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (4.1.1)\r\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning==0.8.4) (1.7.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning==0.8.4) (3.0.1)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (0.4.8)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning==0.8.4) (3.1.0)\r\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Import needed modules"
   ]
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "from abc import ABC, abstractmethod\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer as LightningTrainer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import *"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define constants"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Change the following constant to `False` if you are not running on Kaggle environment:"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "KAGGLE = False"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Other constants:"
   ]
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# --- Directory ---\n",
    "ROOT_DIR = os.path.abspath('.')\n",
    "RAW_DATA_DIR = os.path.join(ROOT_DIR, '../input') if KAGGLE else os.path.join(ROOT_DIR, 'data/raw')\n",
    "PROCESSED_DATA_DIR = os.path.join(ROOT_DIR, 'data/processed') \n",
    "CHECKPOINT_DIR = os.path.join(ROOT_DIR, 'checkpoint')\n",
    "\n",
    "# --- Datasets ---\n",
    "DATASET_MAPPING = {\n",
    "    'SemEval2010Task8': {\n",
    "        'dir': os.path.join(RAW_DATA_DIR,'semeval2010-task-8'),\n",
    "        'extract_dir': os.path.join(RAW_DATA_DIR, 'SemEval2010_task8_all_data'),\n",
    "        'url': 'https://github.com/sahitya0000/Relation-Classification/'\n",
    "               'blob/master/corpus/SemEval2010_task8_all_data.zip?raw=true',\n",
    "        'num_classes': 10,\n",
    "    },\n",
    "    'GIDS': {\n",
    "        'dir': os.path.join(RAW_DATA_DIR,'gids-dataset'),\n",
    "        'extract_dir': os.path.join(RAW_DATA_DIR, 'gids_data'),\n",
    "        'url': 'https://drive.google.com/uc?id=1gTNAbv8My2QDmP-OHLFtJFlzPDoCG4aI&export=download',\n",
    "        'num_classes': 5\n",
    "    },\n",
    "    'NYT': {\n",
    "        'dir': os.path.join(RAW_DATA_DIR,'nyt-relation-extraction'),\n",
    "        'extract_dir': os.path.join(RAW_DATA_DIR, 'riedel_data'),\n",
    "        'url': 'https://drive.google.com/uc?id=1D7bZPvrSAbIPaFSG7ZswYQcPA3tmouCw&export=download',\n",
    "        'num_classes': 53 # TODO merge some relations\n",
    "    }\n",
    "}\n",
    "DATASET_NAME = 'GIDS'\n",
    "\n",
    "# --- BERT ---\n",
    "SUB_START_CHAR = '{'\n",
    "SUB_END_CHAR = '}'\n",
    "OBJ_START_CHAR = '['\n",
    "OBJ_END_CHAR = ']'\n",
    "\n",
    "# --- BERT Model ---\n",
    "# See https://huggingface.co/transformers/pretrained_models.html for the full list\n",
    "\n",
    "BERT_VARIANT_MAPPING = {\n",
    "    'bert': {\n",
    "        'model': BertModel,\n",
    "        'tokenizer': BartTokenizerFast,\n",
    "        'pretrain_weight': 'bert-base-uncased',\n",
    "        'hidden_state_size': 768\n",
    "    },\n",
    "    'distilbert': {\n",
    "        'model': DistilBertModel,\n",
    "        'tokenizer': DistilBertTokenizerFast,\n",
    "        'pretrain_weight': 'distilbert-base-uncased',\n",
    "        'hidden_state_size': 768\n",
    "    },\n",
    "    'roberta': {\n",
    "        'model': RobertaModel,\n",
    "        'tokenizer': RobertaTokenizerFast,\n",
    "        'pretrain_weight': 'roberta-base',\n",
    "        'hidden_state_size': 768\n",
    "    },\n",
    "}\n",
    "BERT_VARIANT = 'distilbert'"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Download data\n",
    "\n",
    "This part **CAN BE SKIPPED** if this notebook is running on Kaggle environment since the dataset has already been included."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First, we install `gdown` to download files from Google Drive"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "!pip install gdown==3.11.1\n",
    "import gdown"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown==3.11.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (3.11.1)\r\n",
      "Requirement already satisfied: six in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from gdown==3.11.1) (1.15.0)\r\n",
      "Requirement already satisfied: requests[socks] in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from gdown==3.11.1) (2.23.0)\r\n",
      "Requirement already satisfied: filelock in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from gdown==3.11.1) (3.0.12)\r\n",
      "Requirement already satisfied: tqdm in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from gdown==3.11.1) (4.45.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests[socks]->gdown==3.11.1) (2.10)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests[socks]->gdown==3.11.1) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests[socks]->gdown==3.11.1) (1.25.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests[socks]->gdown==3.11.1) (2020.6.20)\r\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests[socks]->gdown==3.11.1) (1.7.1)\r\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Some download util functions:"
   ]
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def download_from_url(url: str, save_path: str, chunk_size: int = 2048) -> None:\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        print(f\"Downloading...\\nFrom: {url}\\nTo: {save_path}\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        for data in tqdm(response.iter_content(chunk_size=chunk_size)):\n",
    "            f.write(data)\n",
    "\n",
    "def download_from_google_drive(url: str, save_path: str) -> None:\n",
    "    gdown.download(url, save_path, use_cookies=False)\n",
    "\n",
    "def extract_zip(zip_file_path: str, extract_dir: str, remove_zip_file=True):\n",
    "    # TODO https://stackoverflow.com/questions/4341584/extract-zipfile-using-python-display-progress-percentage\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        print(\"Extracting to \" + extract_dir)\n",
    "        zip_ref.extractall(extract_dir)\n",
    "\n",
    "    if remove_zip_file:\n",
    "        print(\"Removing zip file\")\n",
    "        os.unlink(zip_file_path)"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "The download function itself:"
   ]
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def download(dataset_name, dataset_url, dataset_dir, dataset_extract_dir, force_redownload: bool):\n",
    "    print(f\"\\n---> Downloading dataset {dataset_name} <---\")\n",
    "    \n",
    "    # create raw data dir\n",
    "    if not os.path.exists(RAW_DATA_DIR):\n",
    "        print(\"Creating raw data directory \" + RAW_DATA_DIR)\n",
    "        os.makedirs(RAW_DATA_DIR)\n",
    "    \n",
    "    # check data has been downloaded\n",
    "    if os.path.exists(dataset_dir):\n",
    "        if force_redownload:\n",
    "            print(f\"Removing old raw data {dataset_dir}\")\n",
    "            shutil.rmtree(dataset_dir)\n",
    "        else:\n",
    "            print(f\"Directory {dataset_dir} exists, skip downloading.\")\n",
    "            return\n",
    "\n",
    "\n",
    "    # download\n",
    "    tmp_file_path = os.path.join(RAW_DATA_DIR, dataset_name + '.zip')\n",
    "    if urlparse(dataset_url).netloc == 'drive.google.com':\n",
    "        download_from_google_drive(dataset_url, tmp_file_path)\n",
    "    else:\n",
    "        download_from_url(dataset_url, tmp_file_path)\n",
    "\n",
    "    # unzip\n",
    "    extract_zip(tmp_file_path, RAW_DATA_DIR)\n",
    "\n",
    "    # rename\n",
    "    os.rename(dataset_extract_dir, dataset_dir)"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "Download all datasets:"
   ]
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true,
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "def download_all_dataset():\n",
    "    for dataset_name, dataset_info in DATASET_MAPPING.items():\n",
    "        download(\n",
    "            dataset_name,\n",
    "            dataset_url=dataset_info['url'],\n",
    "            dataset_dir=dataset_info['dir'],\n",
    "            dataset_extract_dir=dataset_info['extract_dir'],\n",
    "            force_redownload=False\n",
    "        )\n",
    "\n",
    "download_all_dataset()"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> Downloading dataset SemEval2010Task8 <---\n",
      "Directory /media/dthung1602/WORKING/bert-relation-extraction/data/raw/semeval2010-task-8 exists, skip downloading.\n",
      "\n",
      "---> Downloading dataset GIDS <---\n",
      "Directory /media/dthung1602/WORKING/bert-relation-extraction/data/raw/gids-dataset exists, skip downloading.\n",
      "\n",
      "---> Downloading dataset NYT <---\n",
      "Directory /media/dthung1602/WORKING/bert-relation-extraction/data/raw/nyt-relation-extraction exists, skip downloading.\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Preprocess\n",
    "\n",
    "The abstract preprocessor"
   ]
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "class AbstractPreprocessor(ABC):\n",
    "    DATASET_NAME = ''\n",
    "    RANDOM_SEED = 2020\n",
    "    VAL_DATA_PROPORTION = 0.2\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def preprocess_data(self, reprocess: bool):\n",
    "        print(f\"\\n---> Preprocessing {self.DATASET_NAME} dataset <---\")\n",
    "        \n",
    "        # create processed data dir\n",
    "        if not os.path.exists(PROCESSED_DATA_DIR):\n",
    "            print(\"Creating processed data directory \" + PROCESSED_DATA_DIR)\n",
    "            os.makedirs(PROCESSED_DATA_DIR)\n",
    "\n",
    "        # stop preprocessing if file existed\n",
    "        pickled_file_names = [self.get_json_file_name(k) for k in ('train', 'val', 'test')]\n",
    "        existed_files = [fn for fn in pickled_file_names if os.path.exists(fn)]\n",
    "        if existed_files:\n",
    "            file_text = \"- \" + \"\\n- \".join(existed_files)\n",
    "            if not reprocess:\n",
    "                print(\"The following files already exist:\")\n",
    "                print(file_text)\n",
    "                print(\"Preprocessing is skipped. See option --reprocess.\")\n",
    "                return\n",
    "            else:\n",
    "                print(\"The following files will be overwritten:\")\n",
    "                print(file_text)\n",
    "\n",
    "        self._preprocess_data()\n",
    "\n",
    "    @abstractmethod\n",
    "    def _preprocess_data(self):\n",
    "        pass\n",
    "\n",
    "    def _train_val_split(self, original_data):\n",
    "        def get_sample(d, idxs):\n",
    "            return [d[i] for i in idxs]\n",
    "\n",
    "        k = list(original_data.keys())[0]\n",
    "        indies = list(range(len(original_data[k])))\n",
    "        train_indies, val_indies = train_test_split(\n",
    "            indies,\n",
    "            test_size=self.VAL_DATA_PROPORTION,\n",
    "            random_state=self.RANDOM_SEED\n",
    "        )\n",
    "        train_data = {k: get_sample(v, train_indies) for k, v in original_data.items()}\n",
    "        val_data = {k: get_sample(v, val_indies) for k, v in original_data.items()}\n",
    "\n",
    "        return train_data, val_data\n",
    "\n",
    "    def _append_data_to_file(self, data: dict, file):\n",
    "        keys = list(data.keys())\n",
    "        lines = \"\"\n",
    "        for values in zip(*data.values()):\n",
    "            tmp = {k: v for k, v in zip(keys, values)}\n",
    "            lines += json.dumps(tmp) + \"\\n\"\n",
    "        file.write(lines)\n",
    "\n",
    "    @classmethod\n",
    "    def get_json_file_name(cls, key: str):\n",
    "        return os.path.join(PROCESSED_DATA_DIR, f'{cls.DATASET_NAME.lower()}_{key}.json')"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Custom preprocessor for each dataset:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class SemEval2010Task8Preprocessor(AbstractPreprocessor):\n",
    "    DATASET_NAME = 'SemEval2010Task8'\n",
    "    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING['SemEval2010Task8']['dir'],\n",
    "                                       'SemEval2010_task8_training/TRAIN_FILE.TXT')\n",
    "    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING['SemEval2010Task8']['dir'],\n",
    "                                      'SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT')\n",
    "    RAW_TRAIN_DATA_SIZE = 8000\n",
    "    RAW_TEST_DATA_SIZE = 2717\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        print(\"Processing training data\")\n",
    "        train_data = self._get_data_from_file(\n",
    "            self.RAW_TRAIN_FILE_NAME,\n",
    "            self.RAW_TRAIN_DATA_SIZE\n",
    "        )\n",
    "\n",
    "        print(\"Processing test data\")\n",
    "        test_data = self._get_data_from_file(\n",
    "            self.RAW_TEST_FILE_NAME,\n",
    "            self.RAW_TEST_DATA_SIZE\n",
    "        )\n",
    "\n",
    "        print(\"Encoding labels to integers\")\n",
    "        le = LabelEncoder()\n",
    "        le.fit(train_data['label'])\n",
    "        train_data['label'] = le.transform(train_data['label']).tolist()\n",
    "        test_data['label'] = le.transform(test_data['label']).tolist()\n",
    "\n",
    "        print(\"Splitting train & validate data\")\n",
    "        train_data, val_data = self._train_val_split(train_data)\n",
    "\n",
    "        print(\"Saving to json files\")\n",
    "        with open(self.get_json_file_name('train'), 'w') as f:\n",
    "            self._append_data_to_file(train_data, f)\n",
    "        with open(self.get_json_file_name('val'), 'w') as f:\n",
    "            self._append_data_to_file(val_data, f)\n",
    "        with open(self.get_json_file_name('test'), 'w') as f:\n",
    "            self._append_data_to_file(test_data, f)\n",
    "\n",
    "    def _get_data_from_file(self, file_name: str, dataset_size: int):\n",
    "        raw_sentences = []\n",
    "        labels = []\n",
    "        with open(file_name) as f:\n",
    "            for _ in tqdm(range(dataset_size)):\n",
    "                raw_sentences.append(self._process_sentence(f.readline()))\n",
    "                labels.append(self._process_label(f.readline()))\n",
    "                f.readline()\n",
    "                f.readline()\n",
    "        data = self.tokenizer(raw_sentences, truncation=True, padding=True)\n",
    "        data['label'] = labels\n",
    "        return data\n",
    "\n",
    "    def _process_sentence(self, sentence: str):\n",
    "        # TODO distinguish e1 e2 sub obj\n",
    "        return sentence.split(\"\\t\")[1][1:-2] \\\n",
    "            .replace(\"<e1>\", SUB_START_CHAR) \\\n",
    "            .replace(\"</e1>\", SUB_END_CHAR) \\\n",
    "            .replace(\"<e2>\", OBJ_START_CHAR) \\\n",
    "            .replace(\"</e2>\", OBJ_END_CHAR)\n",
    "\n",
    "    def _process_label(self, label: str):\n",
    "        return label[:-8]\n",
    "\n",
    "\n",
    "class LargeDatasetPreprocessor(AbstractPreprocessor):\n",
    "    PROCESS_BATCH_SIZE = 2**12\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        pass\n",
    "\n",
    "    def _process_batch(self, le: LabelEncoder, in_file):\n",
    "        raw_sentences = []\n",
    "        labels = []\n",
    "        for _ in range(self.PROCESS_BATCH_SIZE):\n",
    "            dt = in_file.readline()\n",
    "            if dt == \"\": break # EOF\n",
    "            dt = json.loads(dt)\n",
    "\n",
    "            # add subject markup\n",
    "            sentence = \" \".join(dt['sent'])\n",
    "            new_sub = SUB_START_CHAR + dt['sub'].replace('_', '') + SUB_END_CHAR # TODO keep _ or not?\n",
    "            new_obj = OBJ_START_CHAR + dt['obj'].replace('_', '') + OBJ_END_CHAR\n",
    "            sentence = sentence.replace(dt['sub'], new_sub).replace(dt['obj'], new_obj)\n",
    "            raw_sentences.append(sentence)\n",
    "            labels.append(dt['rel'])\n",
    "\n",
    "        data = self.tokenizer(raw_sentences, truncation=True, padding=True)\n",
    "        data['labels'] = le.fit_transform(labels).tolist()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _process_subset(self, le: LabelEncoder, in_file_name, out_file_name, data_size):\n",
    "        with open(in_file_name) as in_file, open(out_file_name, 'w') as out_file:\n",
    "            for _ in tqdm(range(math.ceil(data_size/self.PROCESS_BATCH_SIZE))):\n",
    "                data = self._process_batch(le, in_file)\n",
    "                self._append_data_to_file(data, out_file)\n",
    "\n",
    "class GIDSPreprocessor(LargeDatasetPreprocessor):\n",
    "    DATASET_NAME = 'GIDS'\n",
    "    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING['GIDS']['dir'], 'gids_train.json')\n",
    "    RAW_VAL_FILE_NAME = os.path.join(DATASET_MAPPING['GIDS']['dir'], 'gids_dev.json')\n",
    "    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING['GIDS']['dir'], 'gids_test.json')\n",
    "    TRAIN_SIZE = 11297\n",
    "    VAL_SIZE = 1864\n",
    "    TEST_SIZE = 5663\n",
    "    PROCESS_BATCH_SIZE = 1024\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        print(\"Process train dataset\")\n",
    "        self._process_subset(\n",
    "            le,\n",
    "            self.RAW_TRAIN_FILE_NAME,\n",
    "            self.get_json_file_name('train'),\n",
    "            self.TRAIN_SIZE\n",
    "        )\n",
    "        \n",
    "        print(\"Process val dataset\")\n",
    "        self._process_subset(\n",
    "            le,\n",
    "            self.RAW_VAL_FILE_NAME,\n",
    "            self.get_json_file_name('val'),\n",
    "            self.VAL_SIZE\n",
    "        )\n",
    "        \n",
    "        print(\"Process test dataset\")\n",
    "        self._process_subset(\n",
    "            le, \n",
    "            self.RAW_TEST_FILE_NAME, \n",
    "            self.get_json_file_name('test'),\n",
    "            self.TEST_SIZE\n",
    "        )\n",
    "\n",
    "class NYTPreprocessor(LargeDatasetPreprocessor):\n",
    "    DATASET_NAME = 'NYT'\n",
    "    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING['NYT']['dir'], 'riedel_train.json')\n",
    "    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING['NYT']['dir'], 'riedel_test.json')\n",
    "    TRAIN_SIZE = 570084\n",
    "    TEST_SIZE = 172448\n",
    "    PROCESS_BATCH_SIZE = 4096 * 4\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        print(\"Process train & val dataset\")\n",
    "        with open(self.RAW_TRAIN_FILE_NAME) as in_file,\\\n",
    "                open(self.get_json_file_name('train'), 'w') as train_file,\\\n",
    "                open(self.get_json_file_name('val'), 'w') as val_file:\n",
    "                    for _ in tqdm(range(math.ceil(self.TRAIN_SIZE / self.PROCESS_BATCH_SIZE))):\n",
    "                        data = self._process_batch(le, in_file)\n",
    "                        train_data, val_data = self._train_val_split(data)\n",
    "                        self._append_data_to_file(train_data, train_file)\n",
    "                        self._append_data_to_file(val_data, val_file)\n",
    "\n",
    "        print(\"Process test dataset\")\n",
    "        self._process_subset(\n",
    "            le, \n",
    "            self.RAW_TEST_FILE_NAME, \n",
    "            self.get_json_file_name('test'),\n",
    "            self.TEST_SIZE\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Factory function to get preprocessor:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_preprocessor_class(dataset_name: str):\n",
    "    return globals()[f'{dataset_name}Preprocessor']\n",
    "\n",
    "def get_preprocessor(dataset_name: str)-> AbstractPreprocessor:\n",
    "    bert_model_info = BERT_VARIANT_MAPPING[BERT_VARIANT]\n",
    "    bert_pretrain_weight = bert_model_info['pretrain_weight']\n",
    "    tokenizer = bert_model_info['tokenizer'].from_pretrained(bert_pretrain_weight)\n",
    "    preprocessors_class = get_preprocessor_class(dataset_name)\n",
    "    return preprocessors_class(tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocess data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> Preprocessing SemEval2010Task8 dataset <---\n",
      "Processing training data\n",
      "\n",
      "Processing test data\n",
      "\n",
      "Encoding labels to integers\n",
      "Splitting train & validate data\n",
      "Saving to json files\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=8000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b0c9a4e056d4ae08b3ac7b6fd5c805b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2717.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43e3a7e5421841c181d5076114cac14f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessor = get_preprocessor('SemEval2010Task8')\n",
    "preprocessor.preprocess_data(reprocess=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "### Dataset"
   ]
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "class GenericDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, dataset_name: str, subset: str):\n",
    "        preprocessor_class = get_preprocessor_class(dataset_name)\n",
    "        if subset not in ['train', 'val', 'test']:\n",
    "            raise ValueError('subset must be train, val or test')\n",
    "        self.file = open(preprocessor_class.get_json_file_name(subset))\n",
    "\n",
    "    def __del__(self):\n",
    "        self.file.close()\n",
    "\n",
    "    def __iter__(self):\n",
    "        def get_data():\n",
    "            for line in self.file:\n",
    "                yield json.loads(line)\n",
    "\n",
    "        return get_data()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Torch Lightning Module"
   ]
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "class BERTModule(LightningModule):\n",
    "\n",
    "    def __init__(self, bert_variant, dataset_name, batch_size, learning_rate):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        bert_info = BERT_VARIANT_MAPPING[bert_variant]\n",
    "        bert_model_class = bert_info['model']\n",
    "        bert_pretrain_weight = bert_info['pretrain_weight']\n",
    "        self.bert = bert_model_class.from_pretrained(bert_pretrain_weight, output_attentions=True)\n",
    "\n",
    "        dataset_info = DATASET_MAPPING[dataset_name]\n",
    "        self.num_classes = dataset_info['num_classes']\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, self.num_classes)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader('train')\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader('val')\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader('test')\n",
    "\n",
    "    def __get_dataloader(self, subset: str) -> DataLoader:\n",
    "        print(f\"Loading {subset} data\")\n",
    "        return DataLoader(\n",
    "            GenericDataset(self.hparams.dataset_name, subset),\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=(subset == 'train'),\n",
    "            num_workers=multiprocessing.cpu_count() + 1\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self) -> Optimizer:\n",
    "        return AdamW(\n",
    "            [p for p in self.parameters() if p.requires_grad],\n",
    "            lr=self.hparams.learning_rate,\n",
    "            eps=1e-08\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask) -> Tensor:\n",
    "        bert_output, _ = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        bert_cls = bert_output[:, 0]\n",
    "        logits = self.linear(bert_cls)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_nb) -> dict:\n",
    "        input_ids, attention_mask, label = batch\n",
    "\n",
    "        y_hat = self(input_ids, attention_mask)\n",
    "\n",
    "        loss = F.cross_entropy(y_hat, label)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb) -> dict:\n",
    "        input_ids, attention_mask, label = batch\n",
    "\n",
    "        y_hat = self(input_ids, attention_mask)\n",
    "\n",
    "        loss = F.cross_entropy(y_hat, label)\n",
    "\n",
    "        a, y_hat = torch.max(y_hat, dim=1)\n",
    "        y_hat = y_hat.cpu()\n",
    "        label = label.cpu()\n",
    "\n",
    "        return {\n",
    "            'val_loss': loss,\n",
    "            'val_pre': torch.tensor(precision_score(label, y_hat, average='micro')),\n",
    "            'val_rec': torch.tensor(recall_score(label, y_hat, average='micro')),\n",
    "            'val_acc': torch.tensor(accuracy_score(label, y_hat)),\n",
    "            'val_f1': torch.tensor(f1_score(label, y_hat, average='micro'))\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> dict:\n",
    "        avg_val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_val_pre = torch.stack([x['val_pre'] for x in outputs]).mean()\n",
    "        avg_val_rec = torch.stack([x['val_rec'] for x in outputs]).mean()\n",
    "        avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        avg_val_f1 = torch.stack([x['val_f1'] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            'val_loss': avg_val_loss,\n",
    "            'avg_val_pre': avg_val_pre,\n",
    "            'avg_val_rec': avg_val_rec,\n",
    "            'avg_val_acc': avg_val_acc,\n",
    "            'avg_val_f1': avg_val_f1,\n",
    "        }\n",
    "        return {'val_loss': avg_val_loss, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def test_step(self, batch, batch_nb) -> dict:\n",
    "        input_ids, attention_mask, label = batch\n",
    "\n",
    "        y_hat = self(input_ids, attention_mask)\n",
    "        a, y_hat = torch.max(y_hat, dim=1)\n",
    "        \n",
    "        y_hat = y_hat.cpu()\n",
    "        label = label.cpu()\n",
    "        \n",
    "        test_pre = precision_score(label, y_hat, average='micro')\n",
    "        test_rec = recall_score(label, y_hat, average='micro')\n",
    "        test_acc = accuracy_score(label, y_hat)\n",
    "        test_f1 = f1_score(label, y_hat, average='micro')\n",
    "\n",
    "        return {\n",
    "            'test_pre': torch.tensor(test_pre),\n",
    "            'test_rec': torch.tensor(test_rec),\n",
    "            'test_acc': torch.tensor(test_acc),\n",
    "            'test_f1': torch.tensor(test_f1),\n",
    "        }\n",
    "\n",
    "    def test_epoch_end(self, outputs) -> dict:\n",
    "        avg_test_pre = torch.stack([x['test_pre'] for x in outputs]).mean()\n",
    "        avg_test_rec = torch.stack([x['test_rec'] for x in outputs]).mean()\n",
    "        avg_test_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
    "        avg_test_f1 = torch.stack([x['test_f1'] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            'avg_test_pre': avg_test_pre,\n",
    "            'avg_test_rec': avg_test_rec,\n",
    "            'avg_test_acc': avg_test_acc,\n",
    "            'avg_test_f1': avg_test_f1,\n",
    "        }\n",
    "        return {'progress_bar': tensorboard_logs}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Trainer"
   ]
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": "GPUS = 1\nMIN_EPOCHS = 1\nMAX_EPOCHS = 1\n\ntrainer = LightningTrainer(\n    gpus=GPUS,\n    min_epochs=MIN_EPOCHS,\n    max_epochs=MAX_EPOCHS,\n    default_root_dir=CHECKPOINT_DIR,\n)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "BATCH_SIZE = 64\nLEARNING_RATE = 2e-05\n\nmodel = BERTModule(\n    bert_variant=BERT_VARIANT,\n    dataset_name=DATASET_NAME,\n    batch_size=BATCH_SIZE,\n    learning_rate=LEARNING_RATE\n)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": "trainer.fit(model)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": "trainer.test(model)",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}