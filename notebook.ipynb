{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Relation extraction with BERT\n",
    "\n",
    "---\n",
    "\n",
    "The goal of this repo is to show how to use [BERT](https://arxiv.org/abs/1810.04805)\n",
    "to [extract relation](https://en.wikipedia.org/wiki/Relationship_extraction) from text.\n",
    "\n",
    "Used libraries:\n",
    "- [Transformers](https://huggingface.co/transformers/index.html)\n",
    "- [PyTorch-Lightning](https://pytorch-lightning.readthedocs.io/en/latest/)\n",
    "\n",
    "Used datasets:\n",
    "- SemEval 2010 Task 8 - [paper](https://arxiv.org/pdf/1911.10422.pdf) - [download](https://github.com/sahitya0000/Relation-Classification/blob/master/corpus/SemEval2010_task8_all_data.zip?raw=true)\n",
    "-  Google IISc Distant Supervision (GIDS) - [paper](https://arxiv.org/pdf/1804.06987.pdf) - [download](https://drive.google.com/open?id=1gTNAbv8My2QDmP-OHLFtJFlzPDoCG4aI)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install dependencies\n",
    "\n",
    "This project uses [Python 3.7+](https://www.python.org/downloads/release/python-378/)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests==2.24.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (2.24.0)\r\n",
      "Requirement already satisfied: numpy==1.19.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (1.19.0)\r\n",
      "Requirement already satisfied: pandas==1.0.5 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (1.0.5)\r\n",
      "Requirement already satisfied: scikit-learn==0.23.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (0.23.1)\r\n",
      "Requirement already satisfied: pytorch-lightning==0.8.4 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (0.8.4)\r\n",
      "Requirement already satisfied: torch==1.5.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (1.5.1)\r\n",
      "Requirement already satisfied: transformers==3.0.2 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (3.0.2)\r\n",
      "Requirement already satisfied: sklearn==0.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (0.0)\r\n",
      "Requirement already satisfied: tqdm==4.47.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (4.47.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests==2.24.0) (2.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests==2.24.0) (2020.6.20)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests==2.24.0) (1.25.9)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests==2.24.0) (3.0.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from pandas==1.0.5) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from pandas==1.0.5) (2020.1)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from scikit-learn==0.23.1) (0.16.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from scikit-learn==0.23.1) (2.1.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from scikit-learn==0.23.1) (1.5.0)\r\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (5.3.1)\r\n",
      "Requirement already satisfied: future>=0.17.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (0.18.2)\r\n",
      "Requirement already satisfied: tensorboard>=1.14 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (2.2.2)\r\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (0.1.91)\r\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (0.8.1rc1)\r\n",
      "Requirement already satisfied: sacremoses in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (0.0.43)\r\n",
      "Requirement already satisfied: packaging in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (20.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (2020.6.8)\r\n",
      "Requirement already satisfied: filelock in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from transformers==3.0.2) (3.0.12)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==1.0.5) (1.15.0)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.17.2)\r\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (3.12.3)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.27.2)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.6.0.post3)\r\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.34.2)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.9.0)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (49.1.0.post20200704)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.4.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (3.2.2)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.0.1)\r\n",
      "Requirement already satisfied: click in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from sacremoses->transformers==3.0.2) (7.1.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from packaging->transformers==3.0.2) (2.4.7)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (0.2.7)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (4.1.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (4.6)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning==0.8.4) (1.2.0)\r\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning==0.8.4) (1.7.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning==0.8.4) (3.0.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /home/dthung1602/anaconda3/envs/bre/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning==0.8.4) (3.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge gdown --yes\n",
    "!pip install requests==2.24.0 numpy==1.19.0 pandas==1.0.5 \\\n",
    "    scikit-learn==0.23.1 pytorch-lightning==0.8.4 torch==1.5.1 \\\n",
    "    transformers==3.0.2 sklearn==0.0 tqdm==4.47.0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import needed modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import zipfile\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import gdown\n",
    "import requests\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer as LightningTrainer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# --- Directory ---\n",
    "ROOT_DIR = os.path.abspath('.')\n",
    "RAW_DATA_DIR = os.path.join(ROOT_DIR, 'data/raw')\n",
    "PROCESSED_DATA_DIR = os.path.join(ROOT_DIR, 'data/processed') \n",
    "CHECKPOINT_DIR = os.path.join(ROOT_DIR, 'checkpoint')\n",
    "\n",
    "# --- Datasets ---\n",
    "DATASET_MAPPING = {\n",
    "    'SemEval2010Task8': {\n",
    "        'dir': os.path.join(RAW_DATA_DIR,'SemEval2010_task8_all_data'),\n",
    "        'url': 'https://github.com/sahitya0000/Relation-Classification/'\n",
    "               'blob/master/corpus/SemEval2010_task8_all_data.zip?raw=true',\n",
    "        'num_classes': 10,\n",
    "    },\n",
    "    'GIDS': {\n",
    "        'dir': os.path.join(RAW_DATA_DIR,'gids_data'),\n",
    "        'url': 'https://drive.google.com/uc?id=1gTNAbv8My2QDmP-OHLFtJFlzPDoCG4aI&export=download',\n",
    "        'num_classes': 5\n",
    "    }\n",
    "}\n",
    "DATASET_NAME = 'SemEval2010Task8'\n",
    "\n",
    "# --- BERT ---\n",
    "SUB_START_CHAR = '{'\n",
    "SUB_END_CHAR = '}'\n",
    "OBJ_START_CHAR = '['\n",
    "OBJ_END_CHAR = ']'\n",
    "\n",
    "# --- BERT Model ---\n",
    "# See https://huggingface.co/transformers/pretrained_models.html for the full list\n",
    "\n",
    "BERT_VARIANT_MAPPING = {\n",
    "    'bert': {\n",
    "        'model': BertModel,\n",
    "        'tokenizer': BartTokenizer,\n",
    "        'pretrain_weight': 'bert-base-uncased',\n",
    "        'available_pretrain_weights': ['bert-base-uncased', 'bert-base-cased']\n",
    "    },\n",
    "    'distilbert': {\n",
    "        'model': DistilBertModel,\n",
    "        'tokenizer': DistilBertTokenizer,\n",
    "        'pretrain_weight': 'distilbert-base-uncased',\n",
    "        'available_pretrain_weights': ['distilbert-base-uncased', 'distilbert-base-cased']\n",
    "    },\n",
    "    'roberta': {\n",
    "        'model': RobertaModel,\n",
    "        'tokenizer': RobertaTokenizer,\n",
    "        'pretrain_weight': 'roberta-base',\n",
    "        'available_pretrain_weights': ['roberta-base', 'distilroberta-base']\n",
    "    },\n",
    "}\n",
    "BERT_VARIANT = 'distilbert'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create subdirectories"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if not os.path.exists(RAW_DATA_DIR):\n",
    "    print(\"Creating raw data directory \" + RAW_DATA_DIR)\n",
    "    os.makedirs(RAW_DATA_DIR)\n",
    "\n",
    "if not os.path.exists(PROCESSED_DATA_DIR):\n",
    "    print(\"Creating processed data directory \" + PROCESSED_DATA_DIR)\n",
    "    os.makedirs(PROCESSED_DATA_DIR)\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    print(f\"Creating checkpoint directory \"+ CHECKPOINT_DIR)\n",
    "    os.makedirs(CHECKPOINT_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download data\n",
    "\n",
    "First, we define some download util functions:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def download_from_url(url: str, save_path: str, chunk_size: int = 2048) -> None:\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        print(f\"Downloading...\\nFrom: {url}\\nTo: {save_path}\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        for data in tqdm(response.iter_content(chunk_size=chunk_size)):\n",
    "            f.write(data)\n",
    "\n",
    "def download_from_google_drive(url: str, save_path: str) -> None:\n",
    "    gdown.download(url, save_path, use_cookies=False)\n",
    "\n",
    "def extract_zip(zip_file_path: str, extract_dir: str, remove_zip_file=True):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        print(\"Extracting to \" + extract_dir)\n",
    "        zip_ref.extractall(extract_dir)\n",
    "\n",
    "    if remove_zip_file:\n",
    "        print(\"Removing zip file\")\n",
    "        os.unlink(zip_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The download function itself:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def download(dataset_name, dataset_url, dataset_dir, force_redownload: bool):\n",
    "    print(f\"\\n---> Downloading dataset {dataset_name} <---\")\n",
    "\n",
    "    # check data has been downloaded\n",
    "    if os.path.exists(dataset_dir):\n",
    "        if force_redownload:\n",
    "            print(f\"Removing old raw data {dataset_dir}\")\n",
    "            shutil.rmtree(dataset_dir)\n",
    "        else:\n",
    "            print(f\"Directory {dataset_dir} exists, skip downloading.\")\n",
    "            return\n",
    "\n",
    "\n",
    "    # download\n",
    "    tmp_file_path = os.path.join(RAW_DATA_DIR, dataset_name + '.zip')\n",
    "    if urlparse(dataset_url).netloc == 'drive.google.com':\n",
    "        download_from_google_drive(dataset_url, tmp_file_path)\n",
    "    else:\n",
    "        download_from_url(dataset_url, tmp_file_path)\n",
    "\n",
    "    # unzip\n",
    "    extract_zip(tmp_file_path, RAW_DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download all datasets:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> Downloading dataset SemEval2010Task8 <---\n",
      "Directory /media/dthung1602/WORKING/bert-relation-extraction/data/raw/SemEval2010_task8_all_data exists, skip downloading.\n",
      "\n",
      "---> Downloading dataset GIDS <---\n",
      "Directory /media/dthung1602/WORKING/bert-relation-extraction/data/raw/gids_data exists, skip downloading.\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset_info in DATASET_MAPPING.items():\n",
    "    download(\n",
    "        dataset_name,\n",
    "        dataset_url=dataset_info['url'],\n",
    "        dataset_dir=dataset_info['dir'],\n",
    "        force_redownload=False\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess\n",
    "\n",
    "The abstract preprocessor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class AbstractPreprocessor(ABC):\n",
    "    DATASET_NAME = ''\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def preprocess_data(self, reprocess: bool):\n",
    "        print(f\"\\n---> Preprocessing {self.DATASET_NAME} dataset <---\")\n",
    "\n",
    "        # stop preprocessing if file existed\n",
    "        pickled_file_names = [self.get_pickle_file_name(k) for k in ('train', 'val', 'test')]\n",
    "        existed_files = [fn for fn in pickled_file_names if os.path.exists(fn)]\n",
    "        if existed_files:\n",
    "            file_text = \"- \" + \"\\n- \".join(existed_files)\n",
    "            if not reprocess:\n",
    "                print(\"The following files already exist:\")\n",
    "                print(file_text)\n",
    "                print(\"Preprocessing is skipped. See option --reprocess.\")\n",
    "                return\n",
    "            else:\n",
    "                print(\"The following files will be overwritten:\")\n",
    "                print(file_text)\n",
    "\n",
    "        self._preprocess_data()\n",
    "\n",
    "    @abstractmethod\n",
    "    def _preprocess_data(self):\n",
    "        pass\n",
    "\n",
    "    def _pickle_data(self, data, file_name):\n",
    "        print(f\"Saving to pickle file {file_name}\")\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    @classmethod\n",
    "    def get_pickle_file_name(cls, key: str):\n",
    "        return os.path.join(PROCESSED_DATA_DIR, f'{cls.DATASET_NAME.lower()}_{key}.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each dataset, define a preprocessor:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> Preprocessing SemEval2010Task8 dataset <---\n",
      "Processing training data\n",
      "\n",
      "Processing test data\n",
      "\n",
      "Encoding labels to integers\n",
      "Splitting train & validate data\n",
      "Saving to pickle file /media/dthung1602/WORKING/bert-relation-extraction/data/processed/semeval2010task8_train.pkl\n",
      "Saving to pickle file /media/dthung1602/WORKING/bert-relation-extraction/data/processed/semeval2010task8_val.pkl\n",
      "Saving to pickle file /media/dthung1602/WORKING/bert-relation-extraction/data/processed/semeval2010task8_test.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=8000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f0539ee5a054a15911ff4595e7ef6e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2717.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1f486cd9dd84629a2862f50d9edd43b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SemEval2010Task8Preprocessor(AbstractPreprocessor):\n",
    "    DATASET_NAME = 'SemEval2010Task8'\n",
    "    RAW_TRAIN_FILE_NAME = os.path.join(RAW_DATA_DIR,\n",
    "                                       'SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT')\n",
    "    RAW_TEST_FILE_NAME = os.path.join(RAW_DATA_DIR,\n",
    "                                      'SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT')\n",
    "    RAW_TRAIN_DATA_SIZE = 8000\n",
    "    RAW_TEST_DATA_SIZE = 2717\n",
    "    RANDOM_SEED = 2020\n",
    "    VAL_DATA_PROPORTION = 0.2\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        print(\"Processing training data\")\n",
    "        train_data = self._get_data_from_file(\n",
    "            self.RAW_TRAIN_FILE_NAME,\n",
    "            self.RAW_TRAIN_DATA_SIZE\n",
    "        )\n",
    "\n",
    "        print(\"Processing test data\")\n",
    "        test_data = self._get_data_from_file(\n",
    "            self.RAW_TEST_FILE_NAME,\n",
    "            self.RAW_TEST_DATA_SIZE\n",
    "        )\n",
    "\n",
    "        print(\"Encoding labels to integers\")\n",
    "        le = LabelEncoder()\n",
    "        le.fit(train_data['labels'])\n",
    "        train_data['labels'] = le.transform(train_data['labels']).tolist()\n",
    "        test_data['labels'] = le.transform(test_data['labels']).tolist()\n",
    "\n",
    "        print(\"Splitting train & validate data\")\n",
    "        train_data, val_data = self._train_val_split(train_data)\n",
    "\n",
    "        self._pickle_data(train_data, self.get_pickle_file_name('train'))\n",
    "        self._pickle_data(val_data, self.get_pickle_file_name('val'))\n",
    "        self._pickle_data(test_data, self.get_pickle_file_name('test'))\n",
    "\n",
    "    def _train_val_split(self, original_data):\n",
    "        k = list(original_data.keys())[0]\n",
    "        indies = list(range(len(original_data[k])))\n",
    "        train_indies, val_indies = train_test_split(\n",
    "            indies,\n",
    "            test_size=self.VAL_DATA_PROPORTION,\n",
    "            random_state=self.RANDOM_SEED\n",
    "        )\n",
    "        train_data = {k: self._get_sample(v, train_indies) for k, v in original_data.items()}\n",
    "        val_data = {k: self._get_sample(v, val_indies) for k, v in original_data.items()}\n",
    "\n",
    "        return train_data, val_data\n",
    "\n",
    "    def _get_sample(self, data, indies):\n",
    "        return [data[i] for i in indies]\n",
    "\n",
    "    def _get_data_from_file(self, file_name: str, dataset_size: int):\n",
    "        raw_sentences = []\n",
    "        labels = []\n",
    "        with open(file_name) as f:\n",
    "            for _ in tqdm(range(dataset_size)):\n",
    "                raw_sentences.append(self._process_sentence(f.readline()))\n",
    "                labels.append(self._process_label(f.readline()))\n",
    "                f.readline()\n",
    "                f.readline()\n",
    "        data = self.tokenizer(raw_sentences, truncation=True, padding=True)\n",
    "        data['labels'] = labels\n",
    "        return data\n",
    "\n",
    "    def _process_sentence(self, sentence: str):\n",
    "        # TODO distinguish e1 e2 sub obj\n",
    "        return sentence.split(\"\\t\")[1][1:-2] \\\n",
    "            .replace(\"<e1>\", SUB_START_CHAR) \\\n",
    "            .replace(\"</e1>\", SUB_END_CHAR) \\\n",
    "            .replace(\"<e2>\", OBJ_START_CHAR) \\\n",
    "            .replace(\"</e2>\", OBJ_END_CHAR)\n",
    "\n",
    "    def _process_label(self, label: str):\n",
    "        return label[:-8]\n",
    "\n",
    "\n",
    "class GIDSPreprocessor(AbstractPreprocessor):\n",
    "    DATASET_NAME = 'GIDS'\n",
    "    RAW_TRAIN_FILE_NAME = os.path.join(RAW_DATA_DIR, 'gids_data/gids_train.json')\n",
    "    RAW_VAL_FILE_NAME = os.path.join(RAW_DATA_DIR, 'gids_data/gids_dev.json')\n",
    "    RAW_TEST_FILE_NAME = os.path.join(RAW_DATA_DIR, 'gids_data/gids_test.json')\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        print(\"Processing validate data\")\n",
    "        val_data = self._get_data_from_file(self.RAW_VAL_FILE_NAME)\n",
    "        le = LabelEncoder()\n",
    "        le.fit(val_data['labels'])\n",
    "        val_data['labels'] = le.transform(val_data['labels']).tolist()\n",
    "        self._pickle_data(val_data, self.get_pickle_file_name('val'))\n",
    "        del val_data\n",
    "\n",
    "        print(\"Processing train data\")\n",
    "        train_data = self._get_data_from_file(self.RAW_TRAIN_FILE_NAME)\n",
    "        train_data['labels'] = le.transform(train_data['labels']).tolist()\n",
    "        self._pickle_data(train_data, self.get_pickle_file_name('train'))\n",
    "        del train_data\n",
    "        \n",
    "        print(\"Processing test data\")\n",
    "        test_data = self._get_data_from_file(self.RAW_TEST_FILE_NAME)\n",
    "        test_data['labels'] = le.transform(test_data['labels']).tolist()\n",
    "        self._pickle_data(test_data, self.get_pickle_file_name('test'))\n",
    "        del test_data\n",
    "\n",
    "    def _get_data_from_file(self, file_name: str):\n",
    "        raw_sentences = []\n",
    "        labels = []\n",
    "        with open(file_name) as f:\n",
    "            for line in tqdm(f.readlines()):\n",
    "                dt = json.loads(line)\n",
    "                sentence = \" \".join(dt['sent'])\n",
    "\n",
    "                # add subject markup\n",
    "                new_sub = SUB_START_CHAR + dt['sub'].replace('_', '') + SUB_END_CHAR # TODO keep _ or not?\n",
    "                new_obj = OBJ_START_CHAR + dt['obj'].replace('_', '') + OBJ_END_CHAR\n",
    "                sentence = sentence.replace(dt['sub'], new_sub).replace(dt['obj'], new_obj)\n",
    "                raw_sentences.append(sentence)\n",
    "                labels.append(dt['rel'])\n",
    "        data = self.tokenizer(raw_sentences, truncation=True, padding=True)\n",
    "        data['labels'] = labels\n",
    "        return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Factory function to get preprocessor:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_preprocessor_class(dataset_name: str):\n",
    "    return globals()[f'{dataset_name}Preprocessor']\n",
    "\n",
    "def get_preprocessor(dataset_name: str)-> AbstractPreprocessor:\n",
    "    bert_model_info = BERT_VARIANT_MAPPING[BERT_VARIANT]\n",
    "    bert_pretrain_weight = bert_model_info['pretrain_weight']\n",
    "    tokenizer = bert_model_info['tokenizer'].from_pretrained(bert_pretrain_weight)\n",
    "    preprocessors_class = get_preprocessor_class(dataset_name)\n",
    "    return preprocessors_class(tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocess data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> Preprocessing GIDS dataset <---\n",
      "Processing validate data\n",
      "\n",
      "Saving to pickle file /media/dthung1602/WORKING/bert-relation-extraction/data/processed/gids_val.pkl\n",
      "Processing train data\n",
      "\n",
      "Saving to pickle file /media/dthung1602/WORKING/bert-relation-extraction/data/processed/gids_train.pkl\n",
      "Processing test data\n",
      "\n",
      "Saving to pickle file /media/dthung1602/WORKING/bert-relation-extraction/data/processed/gids_test.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1864.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2c7b21d8a4b4c569a40c73e8fac4c5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=11297.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4ee2ba85f8c482191d219bff385bffd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=5663.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d458457401ec41b49b43f4dadb8da735"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessor = get_preprocessor('GIDS')\n",
    "preprocessor.preprocess_data(reprocess=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "### Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GenericDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_name: str, subset: str):\n",
    "        preprocessor_class = get_preprocessor_class(dataset_name)\n",
    "        if subset not in ['train', 'val', 'test']:\n",
    "            raise ValueError('subset must be train, val or test')\n",
    "        with open(preprocessor_class.get_pickle_file_name(subset), 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        return (torch.tensor(self.data['input_ids'][index]),\n",
    "                torch.tensor(self.data['attention_mask'][index]),\n",
    "                torch.tensor(self.data['labels'][index]))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data['label'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Torch Lightning Module"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BERTModule(LightningModule):\n",
    "\n",
    "    def __init__(self, bert_variant, dataset_name, batch_size, learning_rate):\n",
    "        print(\"---> Start building model <----\")\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        bert_info = BERT_VARIANT_MAPPING[bert_variant]\n",
    "        bert_model_class = bert_info['model']\n",
    "        bert_pretrain_weight = bert_info['pretrain_weight']\n",
    "        self.bert = bert_model_class.from_pretrained(bert_pretrain_weight, output_attentions=True)\n",
    "\n",
    "        dataset_info = DATASET_MAPPING[dataset_name]\n",
    "        self.num_classes = dataset_info['num_classes']\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, self.num_classes)\n",
    "\n",
    "        print(\"Done building model\\n\")\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        print(\"\\n---> Start training <----\")\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader('train')\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader('val')\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader('test')\n",
    "\n",
    "    def __get_dataloader(self, subset: str) -> DataLoader:\n",
    "        print(f\"Loading {subset} data\")\n",
    "        return DataLoader(\n",
    "            GenericDataset(self.hparams.dataset_name, subset),\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=(subset == 'train')\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self) -> Optimizer:\n",
    "        return AdamW(\n",
    "            [p for p in self.parameters() if p.requires_grad],\n",
    "            lr=self.hparams.learning_rate,\n",
    "            eps=1e-08\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask) -> Tensor:\n",
    "        bert_output, _ = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        bert_cls = bert_output[:, 0]\n",
    "        logits = self.linear(bert_cls)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_nb) -> dict:\n",
    "        input_ids, attention_mask, label = batch\n",
    "\n",
    "        y_hat = self(input_ids, attention_mask)\n",
    "\n",
    "        loss = F.cross_entropy(y_hat, label)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb) -> dict:\n",
    "        input_ids, attention_mask, label = batch\n",
    "\n",
    "        y_hat = self(input_ids, attention_mask)\n",
    "\n",
    "        loss = F.cross_entropy(y_hat, label)\n",
    "\n",
    "        a, y_hat = torch.max(y_hat, dim=1)\n",
    "        y_hat = y_hat.cpu()\n",
    "        label = label.cpu()\n",
    "\n",
    "        return {\n",
    "            'val_loss': loss,\n",
    "            'val_pre': torch.tensor(precision_score(label, y_hat, average='micro')),\n",
    "            'val_rec': torch.tensor(recall_score(label, y_hat, average='micro')),\n",
    "            'val_acc': torch.tensor(accuracy_score(label, y_hat)),\n",
    "            'val_f1': torch.tensor(f1_score(label, y_hat, average='micro'))\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> dict:\n",
    "        avg_val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_val_pre = torch.stack([x['val_pre'] for x in outputs]).mean()\n",
    "        avg_val_rec = torch.stack([x['val_rec'] for x in outputs]).mean()\n",
    "        avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        avg_val_f1 = torch.stack([x['val_f1'] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            'val_loss': avg_val_loss,\n",
    "            'avg_val_pre': avg_val_pre,\n",
    "            'avg_val_rec': avg_val_rec,\n",
    "            'avg_val_acc': avg_val_acc,\n",
    "            'avg_val_f1': avg_val_f1,\n",
    "        }\n",
    "        return {'val_loss': avg_val_loss, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def test_step(self, batch, batch_nb) -> dict:\n",
    "        input_ids, attention_mask, label = batch\n",
    "\n",
    "        y_hat = self(input_ids, attention_mask)\n",
    "\n",
    "        a, y_hat = torch.max(y_hat, dim=1)\n",
    "        y_hat = y_hat.cpu()\n",
    "        label = label.cpu()\n",
    "        test_pre = precision_score(label, y_hat, average='micro')\n",
    "        test_rec = recall_score(label, y_hat, average='micro')\n",
    "        test_acc = accuracy_score(label, y_hat)\n",
    "        test_f1 = f1_score(label, y_hat, average='micro')\n",
    "\n",
    "        return {\n",
    "            'test_pre': torch.tensor(test_pre),\n",
    "            'test_rec': torch.tensor(test_rec),\n",
    "            'test_acc': torch.tensor(test_acc),\n",
    "            'test_f1': torch.tensor(test_f1),\n",
    "        }\n",
    "\n",
    "    def test_epoch_end(self, outputs) -> dict:\n",
    "        avg_test_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        avg_test_pre = torch.stack([x['test_pre'] for x in outputs]).mean()\n",
    "        avg_test_rec = torch.stack([x['test_rec'] for x in outputs]).mean()\n",
    "        avg_test_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
    "        avg_test_f1 = torch.stack([x['test_f1'] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            'test_loss': avg_test_loss,\n",
    "            'avg_test_pre': avg_test_pre,\n",
    "            'avg_test_rec': avg_test_rec,\n",
    "            'avg_test_acc': avg_test_acc,\n",
    "            'avg_test_f1': avg_test_f1,\n",
    "        }\n",
    "        return {'test_loss': avg_test_loss, 'progress_bar': tensorboard_logs}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Trainer class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "GPUS = 1\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 4\n",
    "\n",
    "trainer = LightningTrainer(\n",
    "    gpus=GPUS,\n",
    "    min_epochs=MIN_EPOCHS,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    default_root_dir=CHECKPOINT_DIR,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-05\n",
    "\n",
    "model = BERTModule(\n",
    "    bert_variant=BERT_VARIANT,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.test(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}