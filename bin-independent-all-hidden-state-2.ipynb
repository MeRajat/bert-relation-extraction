{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Relation extraction with BERT\n\n---\n\nThe goal of this notebook is to show how to use [BERT](https://arxiv.org/abs/1810.04805)\nto [extract relation](https://en.wikipedia.org/wiki/Relationship_extraction) from text.\n\nUsed libraries:\n- [PyTorch](https://pytorch.org/)\n- [PyTorch-Lightning](https://pytorch-lightning.readthedocs.io/en/latest/)\n- [Transformers](https://huggingface.co/transformers/index.html)\n\nUsed datasets:\n- SemEval 2010 Task 8 - [paper](https://arxiv.org/pdf/1911.10422.pdf) - [download](https://github.com/sahitya0000/Relation-Classification/blob/master/corpus/SemEval2010_task8_all_data.zip?raw=true)\n- Google IISc Distant Supervision (GIDS) - [paper](https://arxiv.org/pdf/1804.06987.pdf) - [download](https://drive.google.com/open?id=1gTNAbv8My2QDmP-OHLFtJFlzPDoCG4aI)\n- Riedel's New York Times - [paper](https://www.researchgate.net/publication/220698997_Modeling_Relations_and_Their_Mentions_without_Labeled_Text) - [download](https://drive.google.com/uc?id=1D7bZPvrSAbIPaFSG7ZswYQcPA3tmouCw&export=download)"},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## Install dependencies\n\nThis project uses [Python 3.7+](https://www.python.org/downloads/release/python-378/)"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"!pip install requests==2.23.0 numpy==1.18.5 pandas==1.0.3 \\\n    scikit-learn==0.23.1 pytorch-lightning==0.8.4 \\\n    transformers==3.0.2 sklearn==0.0 tqdm==4.45.0 neptune-client==0.4.119 \\\n    matplotlib==3.1.0 scikit-plot==0.3.7","execution_count":1,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: requests==2.23.0 in /opt/conda/lib/python3.7/site-packages (2.23.0)\nRequirement already satisfied: numpy==1.18.5 in /opt/conda/lib/python3.7/site-packages (1.18.5)\nRequirement already satisfied: pandas==1.0.3 in /opt/conda/lib/python3.7/site-packages (1.0.3)\nRequirement already satisfied: scikit-learn==0.23.1 in /opt/conda/lib/python3.7/site-packages (0.23.1)\nCollecting pytorch-lightning==0.8.4\n  Downloading pytorch_lightning-0.8.4-py3-none-any.whl (304 kB)\n\u001b[K     |████████████████████████████████| 304 kB 2.9 MB/s eta 0:00:01\n\u001b[?25hCollecting transformers==3.0.2\n  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n\u001b[K     |████████████████████████████████| 769 kB 12.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: sklearn==0.0 in /opt/conda/lib/python3.7/site-packages (0.0)\nRequirement already satisfied: tqdm==4.45.0 in /opt/conda/lib/python3.7/site-packages (4.45.0)\nCollecting neptune-client==0.4.119\n  Downloading neptune-client-0.4.119.tar.gz (90 kB)\n\u001b[K     |████████████████████████████████| 90 kB 6.1 MB/s  eta 0:00:01\n\u001b[?25hCollecting matplotlib==3.1.0\n  Downloading matplotlib-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n\u001b[K     |████████████████████████████████| 13.1 MB 16.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: scikit-plot==0.3.7 in /opt/conda/lib/python3.7/site-packages (0.3.7)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests==2.23.0) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests==2.23.0) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests==2.23.0) (1.24.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests==2.23.0) (2020.6.20)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (2019.3)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.1) (1.4.1)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.1) (0.14.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.1) (2.1.0)\nRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (1.5.1)\nRequirement already satisfied: tensorboard>=1.14 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (2.2.2)\nRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (5.3.1)\nRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==0.8.4) (0.18.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (20.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (2020.4.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (3.0.10)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (0.0.43)\nCollecting tokenizers==0.8.1.rc1\n  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n\u001b[K     |████████████████████████████████| 3.0 MB 49.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (0.1.91)\nCollecting bravado\n  Downloading bravado-10.6.2-py2.py3-none-any.whl (37 kB)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.7/site-packages (from neptune-client==0.4.119) (7.1.1)\nCollecting py3nvml\n  Downloading py3nvml-0.2.6-py3-none-any.whl (55 kB)\n\u001b[K     |████████████████████████████████| 55 kB 2.4 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from neptune-client==0.4.119) (3.0.1)\nRequirement already satisfied: Pillow>=1.1.6 in /opt/conda/lib/python3.7/site-packages (from neptune-client==0.4.119) (7.2.0)\nRequirement already satisfied: PyJWT in /opt/conda/lib/python3.7/site-packages (from neptune-client==0.4.119) (1.7.1)\nRequirement already satisfied: requests-oauthlib>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from neptune-client==0.4.119) (1.2.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from neptune-client==0.4.119) (1.14.0)\nRequirement already satisfied: websocket-client>=0.35.0 in /opt/conda/lib/python3.7/site-packages (from neptune-client==0.4.119) (0.57.0)\nRequirement already satisfied: GitPython>=2.0.8 in /opt/conda/lib/python3.7/site-packages (from neptune-client==0.4.119) (3.1.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.1.0) (1.2.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.1.0) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.1.0) (0.10.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (46.1.3.post20200325)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.7.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.0.1)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.9.0)\nRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (3.12.2)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.14.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.4.1)\nRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (0.34.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (3.2.1)\nRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.4) (1.30.0)\nCollecting msgpack-python\n  Downloading msgpack-python-0.5.6.tar.gz (138 kB)\n\u001b[K     |████████████████████████████████| 138 kB 50.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: simplejson in /opt/conda/lib/python3.7/site-packages (from bravado->neptune-client==0.4.119) (3.17.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from bravado->neptune-client==0.4.119) (3.7.4.1)\nCollecting bravado-core>=5.16.1\n  Downloading bravado_core-5.17.0-py2.py3-none-any.whl (67 kB)\n\u001b[K     |████████████████████████████████| 67 kB 4.5 MB/s  eta 0:00:01\n\u001b[?25hCollecting monotonic\n  Downloading monotonic-1.5-py2.py3-none-any.whl (5.3 kB)\nCollecting xmltodict\n  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=2.0.8->neptune-client==0.4.119) (4.0.4)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (3.1.1)\nRequirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (4.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (0.2.7)\n","name":"stdout"},{"output_type":"stream","text":"Collecting swagger-spec-validator>=2.0.1\n  Downloading swagger_spec_validator-2.7.3-py2.py3-none-any.whl (27 kB)\nCollecting jsonref\n  Downloading jsonref-0.2-py3-none-any.whl (9.3 kB)\nRequirement already satisfied: jsonschema[format]>=2.5.1 in /opt/conda/lib/python3.7/site-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.4.119) (3.2.0)\nRequirement already satisfied: msgpack>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.4.119) (1.0.0)\nRequirement already satisfied: smmap<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client==0.4.119) (3.0.2)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.4) (0.4.8)\nRequirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.4.119) (0.16.0)\nRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.4.119) (19.3.0)\nRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.4.119) (1.6.0)\nCollecting webcolors; extra == \"format\"\n  Downloading webcolors-1.11.1-py3-none-any.whl (9.9 kB)\nCollecting jsonpointer>1.13; extra == \"format\"\n  Downloading jsonpointer-2.0-py2.py3-none-any.whl (7.6 kB)\nCollecting strict-rfc3339; extra == \"format\"\n  Downloading strict-rfc3339-0.7.tar.gz (17 kB)\nCollecting rfc3987; extra == \"format\"\n  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.4.119) (3.1.0)\nBuilding wheels for collected packages: neptune-client, msgpack-python, strict-rfc3339\n  Building wheel for neptune-client (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for neptune-client: filename=neptune_client-0.4.119-py2.py3-none-any.whl size=150019 sha256=9dd5af9ec7ffae47161cfce20d62aa94340d2b22f91ff35cc497f681bfe0a8b2\n  Stored in directory: /root/.cache/pip/wheels/64/3b/7e/69f84d99e2109788f757ef707b3ea51921f16891e42929eb31\n  Building wheel for msgpack-python (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for msgpack-python: filename=msgpack_python-0.5.6-cp37-cp37m-linux_x86_64.whl size=302568 sha256=06750c71b356a2e8744be6824b035057623db82873f228b8345e44e25f7dec08\n  Stored in directory: /root/.cache/pip/wheels/f8/6c/02/92ebc97f3b99ad5bfc675be2c513f9cb3504fdbe338314f377\n  Building wheel for strict-rfc3339 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for strict-rfc3339: filename=strict_rfc3339-0.7-py3-none-any.whl size=18119 sha256=a8060289a44cdf8cf9082a6ff3cefd4dfd2ad7cd34cb1f6265b602e664a87c46\n  Stored in directory: /root/.cache/pip/wheels/f3/1d/9f/2a74caecb81b8beb9a4fbe1754203d4b7cf42ef5d39e0d2311\nSuccessfully built neptune-client msgpack-python strict-rfc3339\n\u001b[31mERROR: plotnine 0.7.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: pandas-profiling 2.6.0 has requirement matplotlib>=3.2.0, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: osmnx 0.15.1 has requirement geopandas>=0.7, but you'll have geopandas 0.6.3 which is incompatible.\u001b[0m\n\u001b[31mERROR: osmnx 0.15.1 has requirement matplotlib>=3.2, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: mizani 0.7.1 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: hypertools 0.6.2 has requirement scikit-learn<0.22,>=0.19.1, but you'll have scikit-learn 0.23.1 which is incompatible.\u001b[0m\n\u001b[31mERROR: allennlp 1.0.0 has requirement transformers<2.12,>=2.9, but you'll have transformers 3.0.2 which is incompatible.\u001b[0m\nInstalling collected packages: pytorch-lightning, tokenizers, transformers, msgpack-python, swagger-spec-validator, jsonref, bravado-core, monotonic, bravado, xmltodict, py3nvml, neptune-client, matplotlib, webcolors, jsonpointer, strict-rfc3339, rfc3987\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.7.0\n    Uninstalling tokenizers-0.7.0:\n      Successfully uninstalled tokenizers-0.7.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 2.11.0\n    Uninstalling transformers-2.11.0:\n      Successfully uninstalled transformers-2.11.0\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.2.1\n    Uninstalling matplotlib-3.2.1:\n      Successfully uninstalled matplotlib-3.2.1\nSuccessfully installed bravado-10.6.2 bravado-core-5.17.0 jsonpointer-2.0 jsonref-0.2 matplotlib-3.1.0 monotonic-1.5 msgpack-python-0.5.6 neptune-client-0.4.119 py3nvml-0.2.6 pytorch-lightning-0.8.4 rfc3987-1.3.8 strict-rfc3339-0.7 swagger-spec-validator-2.7.3 tokenizers-0.8.1rc1 transformers-3.0.2 webcolors-1.11.1 xmltodict-0.12.0\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"To train the model with float16, we need `pytorch >= 1.6.0` to make use of the built-in `amp` module. However, `pytorch 1.6.0` currently is not compatible with `NVIDIA` driver on Kaggle.\n\nHere we will install `pytorch` nightly build `1.7.0.dev20200701`"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --pre torch==1.7.0.dev20200701+cu101 -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html","execution_count":2,"outputs":[{"output_type":"stream","text":"Looking in links: https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\nCollecting torch==1.7.0.dev20200701+cu101\n  Downloading https://download.pytorch.org/whl/nightly/cu101/torch-1.7.0.dev20200701%2Bcu101-cp37-cp37m-linux_x86_64.whl (844.4 MB)\n\u001b[K     |████████████████████████████████| 844.4 MB 15 kB/s  eta 0:00:011    |█                               | 24.1 MB 37.4 MB/s eta 0:00:22 MB 37.4 MB/s eta 0:00:22     |█▏                              | 31.9 MB 37.4 MB/s eta 0:00:22     |█▎                              | 33.4 MB 25.8 MB/s eta 0:00:3232     |█▉                              | 47.5 MB 25.8 MB/s eta 0:00:31/s eta 0:00:19     |███████████████▋                | 413.3 MB 23.8 MB/s eta 0:00:19     |████████████████▋               | 438.1 MB 25.7 MB/s eta 0:00:16     |████████████████▊               | 442.4 MB 25.7 MB/s eta 0:00:16��█▌            | 514.0 MB 58.2 MB/s eta 0:00:06     |███████████████████████████▎    | 720.6 MB 43.6 MB/s eta 0:00:03     |████████████████████████████▎   | 744.9 MB 54.7 MB/s eta 0:00:02     |████████████████████████████▎   | 745.6 MB 54.7 MB/s eta 0:00:02     |████████████████████████████▎   | 747.1 MB 54.7 MB/s eta 0:00:02     |███████████████████████████████▏| 821.9 MB 40.7 MB/s eta 0:00:01     |███████████████████████████████▍| 827.6 MB 5.1 MB/s eta 0:00:04     |███████████████████████████████▉| 839.9 MB 5.1 MB/s eta 0:00:01████████| 842.5 MB 5.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==1.7.0.dev20200701+cu101) (0.18.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.7.0.dev20200701+cu101) (1.18.5)\n\u001b[31mERROR: kornia 0.3.1 has requirement torch==1.5.0, but you'll have torch 1.7.0.dev20200701+cu101 which is incompatible.\u001b[0m\n\u001b[31mERROR: allennlp 1.0.0 has requirement torch<1.6.0,>=1.5.0, but you'll have torch 1.7.0.dev20200701+cu101 which is incompatible.\u001b[0m\n\u001b[31mERROR: allennlp 1.0.0 has requirement transformers<2.12,>=2.9, but you'll have transformers 3.0.2 which is incompatible.\u001b[0m\nInstalling collected packages: torch\n  Attempting uninstall: torch\n    Found existing installation: torch 1.5.1\n    Uninstalling torch-1.5.1:\n      Successfully uninstalled torch-1.5.1\nSuccessfully installed torch-1.7.0.dev20200701+cu101\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Import needed modules"},{"metadata":{"pycharm":{"name":"#%% \n"},"trusted":true,"scrolled":false},"cell_type":"code","source":"import gc\nimport json\nimport math\nimport os\nimport shutil\nimport zipfile\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\nfrom random import randint\nfrom typing import TextIO, Iterable, Tuple\nfrom urllib.parse import urlparse\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport requests\nimport torch\nfrom matplotlib.figure import Figure\nfrom pandas import DataFrame\nfrom pytorch_lightning import LightningModule, seed_everything\nfrom pytorch_lightning import Trainer as LightningTrainer\nfrom pytorch_lightning.logging.neptune import NeptuneLogger\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, \\\n    roc_curve, roc_auc_score, precision_recall_curve, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import column_or_1d\nfrom torch import Tensor, nn\nfrom torch.nn import functional as F\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.utils.data import DataLoader, IterableDataset\nfrom tqdm.auto import tqdm\nfrom transformers import *","execution_count":3,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## Define constants"},{"metadata":{"pycharm":{"name":"#%% \n"},"trusted":true,"scrolled":false},"cell_type":"code","source":"# --- Random seed ---\nSEED = 2020\nseed_everything(SEED)\n\n# --- Neptune logger ---\nNEPTUNE_API_TOKEN=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiYjhmNDI4YzQtMjM3Yy00NWFiLTgzNDgtNWJmODJhNjVkZjdiIn0=\"\nNEPTUNE_PROJECT_NAME=\"hung/bert-relation-extraction\"\n\n# --- Directory ---\nROOT_DIR = os.path.abspath('.')\nPROCESSED_DATA_DIR = os.path.join(ROOT_DIR, 'data/processed') \nMETADATA_FILE_NAME = os.path.join(PROCESSED_DATA_DIR, 'metadata.json')\nCHECKPOINT_DIR = os.path.join(ROOT_DIR, 'checkpoint')\n\nKAGGLE_ENV = bool(os.getenv(\"KAGGLE_URL_BASE\"))\nLOCAL_ENV = not KAGGLE_ENV\nif KAGGLE_ENV:\n    # in Kaggle environment\n    # 3 datasets should already been added to the notebook\n    RAW_DATA_DIR = os.path.join(ROOT_DIR, '../input')\nelse:\n    # in local environment\n    RAW_DATA_DIR =  os.path.join(ROOT_DIR, 'data/raw')\n\n# --- Datasets ---\nDATASET_MAPPING = {\n    'SemEval2010Task8': {\n        'dir': os.path.join(RAW_DATA_DIR,'semeval2010-task-8'),\n        'extract_dir': os.path.join(RAW_DATA_DIR, 'SemEval2010_task8_all_data'),\n        'url': 'https://github.com/sahitya0000/Relation-Classification/'\n               'blob/master/corpus/SemEval2010_task8_all_data.zip?raw=true',\n        'fit_in_memory': True\n    },\n    'GIDS': {\n        'dir': os.path.join(RAW_DATA_DIR,'gids-dataset'),\n        'extract_dir': os.path.join(RAW_DATA_DIR, 'gids_data'),\n        'url': 'https://drive.google.com/uc?id=1gTNAbv8My2QDmP-OHLFtJFlzPDoCG4aI&export=download',\n        'fit_in_memory': True\n    },\n    'NYT': {\n        'dir': os.path.join(RAW_DATA_DIR,'nyt-relation-extraction'),\n        'extract_dir': os.path.join(RAW_DATA_DIR, 'riedel_data'),\n        'url': 'https://drive.google.com/uc?id=1D7bZPvrSAbIPaFSG7ZswYQcPA3tmouCw&export=download',\n        'fit_in_memory': False\n    }\n}\n\n# change this variable to switch dataset in later tasks\nDATASET_NAME = 'SemEval2010Task8'\n\n# --- BERT ---\nSUB_START_CHAR = '['\nSUB_END_CHAR = ']'\nOBJ_START_CHAR = '{'\nOBJ_END_CHAR = '}'\n\n# --- BERT Model ---\n# See https://huggingface.co/transformers/pretrained_models.html for the full list\nAVAILABLE_PRETRAINED_MODELS = [\n    'distilbert-base-uncased', # 0\n    'distilbert-base-cased',   # 1\n    'bert-base-uncased',       # 2\n    'distilgpt2',              # 3\n    'gpt2',                    # 4\n    'distilroberta-base',      # 5\n    'roberta-base',            # 6\n    'albert-base-v1',          # 7\n    'albert-base-v2',          # 8\n]\n\n# change this variable to switch pretrained language model in later tasks\nPRETRAINED_MODEL = AVAILABLE_PRETRAINED_MODELS[2]\n\n# if e1 is not related to e2, should \"e2 not related to e1\" be added to the training set\nADD_REVERSE_RELATIONSHIP = False","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Download data\n\nThis part **CAN BE SKIPPED** if this notebook is running on Kaggle environment since the dataset has already been included."},{"metadata":{},"cell_type":"markdown","source":"First, we install `gdown` to download files from Google Drive"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"!if [ -z \"KAGGLE_URL_BASE\" ]; then pip install gdown==3.11.1 ; else echo \"gdown is not installed\" ;  fi\n\nif LOCAL_ENV:\n    import gdown\nelse:\n    print(\"gdown is not imported\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some download util functions:"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"scrolled":false},"cell_type":"code","source":"def download_from_url(url: str, save_path: str, chunk_size: int = 2048):\n    with open(save_path, \"wb\") as f:\n        print(f\"Downloading...\\nFrom: {url}\\nTo: {save_path}\")\n        response = requests.get(url, stream=True)\n        for data in tqdm(response.iter_content(chunk_size=chunk_size)):\n            f.write(data)\n\ndef download_from_google_drive(url: str, save_path: str):\n    gdown.download(url, save_path, use_cookies=False)\n\ndef extract_zip(zip_file_path: str, extract_dir: str, remove_zip_file: bool = True):\n    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n        print(\"Extracting to \" + extract_dir)\n        for member in tqdm(zip_ref.infolist()):\n            zip_ref.extract(member, extract_dir)\n\n    if remove_zip_file:\n        print(\"Removing zip file\")\n        os.unlink(zip_file_path)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"The download function itself:"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"scrolled":false},"cell_type":"code","source":"def download(dataset_name: str, dataset_url: str, dataset_dir: str, dataset_extract_dir: str, force_redownload: bool):\n    print(f\"\\n---> Downloading dataset {dataset_name} <---\")\n    \n    # create raw data dir\n    if not os.path.exists(RAW_DATA_DIR):\n        print(\"Creating raw data directory \" + RAW_DATA_DIR)\n        os.makedirs(RAW_DATA_DIR)\n    \n    # check data has been downloaded\n    if os.path.exists(dataset_dir):\n        if force_redownload:\n            print(f\"Removing old raw data {dataset_dir}\")\n            shutil.rmtree(dataset_dir)\n        else:\n            print(f\"Directory {dataset_dir} exists, skip downloading.\")\n            return\n\n\n    # download\n    tmp_file_path = os.path.join(RAW_DATA_DIR, dataset_name + '.zip')\n    if urlparse(dataset_url).netloc == 'drive.google.com':\n        download_from_google_drive(dataset_url, tmp_file_path)\n    else:\n        download_from_url(dataset_url, tmp_file_path)\n\n    # unzip\n    extract_zip(tmp_file_path, RAW_DATA_DIR)\n\n    # rename\n    os.rename(dataset_extract_dir, dataset_dir)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"Download all datasets:"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"scrolled":false},"cell_type":"code","source":"def download_all_dataset():\n    for dataset_name, dataset_info in DATASET_MAPPING.items():\n        download(\n            dataset_name,\n            dataset_url=dataset_info['url'],\n            dataset_dir=dataset_info['dir'],\n            dataset_extract_dir=dataset_info['extract_dir'],\n            force_redownload=False\n        )\n\nif LOCAL_ENV:\n    download_all_dataset()\nelse:\n    print(\"Skip downloading\")","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## Preprocess\n\nFirst, we define a custom label encoder. What this label encoder offers but `sklearn.preprocessing.LabelEncoder` fails\nto provide:\n- Order preservation: labels will be encoded in order they appear in the dataset. Labels appears earlier will have\n  smaller id. We need this to ensure the `no relation` class always becomes `0`\n- Multiple fit: `sklearn.preprocessing.LabelEncoder` forgets what is fit in the last time `fit` is called while our\n  encoder keeps adding new labels to existing ones. This is useful when we process large dataset in batches."},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"scrolled":false},"cell_type":"code","source":"class OrdinalLabelEncoder:\n    def __init__(self, init_labels=None):\n        if init_labels is None:\n            init_labels = []\n        self.mapping = OrderedDict({l: i for i, l in enumerate(init_labels)})\n\n    @property\n    def classes_(self):\n        return list(self.mapping.keys())\n\n    def fit_transform(self, y):\n        return self.fit(y).transform(y)\n\n    def fit(self, y):\n        y = column_or_1d(y, warn=True)\n        new_classes = pd.Series(y).unique()\n        for cls in new_classes:\n            if cls not in self.mapping:\n                self.mapping[cls] = len(self.mapping)\n        return self\n\n    def transform(self, y):\n        y = column_or_1d(y, warn=True)\n        return [self.mapping[value] for value in y]\n    ","execution_count":5,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"Preprocessor classes:"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"scrolled":true},"cell_type":"code","source":"class AbstractPreprocessor(ABC):\n    DATASET_NAME = ''\n    VAL_DATA_PROPORTION = 0.2\n    NO_RELATION_LABEL = ''\n\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        self.tokenizer = tokenizer\n        self.SUB_START_ID, self.SUB_END_ID, self.OBJ_START_ID, self.OBJ_END_ID \\\n            = tokenizer.convert_tokens_to_ids([SUB_START_CHAR, SUB_END_CHAR, OBJ_START_CHAR, OBJ_END_CHAR])\n\n    def preprocess_data(self, reprocess: bool):\n        print(f\"\\n---> Preprocessing {self.DATASET_NAME} dataset <---\")\n        \n        # create processed data dir\n        if not os.path.exists(PROCESSED_DATA_DIR):\n            print(\"Creating processed data directory \" + PROCESSED_DATA_DIR)\n            os.makedirs(PROCESSED_DATA_DIR)\n\n        # stop preprocessing if file existed\n        json_file_names = [self.get_dataset_file_name(k) for k in ('train', 'val', 'test')]\n        existed_files = [fn for fn in json_file_names if os.path.exists(fn)]\n        if existed_files:\n            file_text = \"- \" + \"\\n- \".join(existed_files)\n            if not reprocess:\n                print(\"The following files already exist:\")\n                print(file_text)\n                print(\"Preprocessing is skipped. See option --reprocess.\")\n                return\n            else:\n                print(\"The following files will be overwritten:\")\n                print(file_text)\n\n        self._preprocess_data()\n        self._create_secondary_data_files()\n\n        print(\"---> Done ! <---\")\n\n    @abstractmethod\n    def _preprocess_data(self):\n        pass\n\n    def _create_secondary_data_files(self):\n        \"\"\"\n        From the primary data file, create a data file with binary labels\n        and a data file with only sentences classified as \"related\"\n        \"\"\"\n\n        with open(METADATA_FILE_NAME) as f:\n            root_metadata = json.load(f)\n            metadata = root_metadata[self.DATASET_NAME]\n\n        related_only_count = {\n            'train': 0,\n            'val': 0,\n            'test': 0,\n        }\n\n        for key in ['train', 'test', 'val']:\n            print(f\"Creating secondary files for {key} data\")\n\n            origin_file = open(self.get_dataset_file_name(key))\n            bin_file = open(self.get_dataset_file_name(f'{key}_binary'), \"w\")\n            related_file = open(self.get_dataset_file_name(f'{key}_related_only'), \"w\")\n\n            total = metadata[f'{key}_size']\n\n            for line in tqdm(origin_file, total=total):\n                data = json.loads(line)\n                if data['label'] != 0:\n                    related_only_count[key] += 1\n                    data['label'] -= 1 # label in \"related_only\" files is 1 less than the original label\n                    related_file.write(json.dumps(data) + \"\\n\")\n                    data['label'] = 1 # in binary dataset, all \"related\" classes have label 1\n                    bin_file.write(json.dumps(data) + \"\\n\")\n                else:\n                    bin_file.write(json.dumps(data) + \"\\n\")\n\n            origin_file.close()\n            bin_file.close()\n            related_file.close()\n\n        print(\"Updating metadata.json\")\n        for key in ['train', 'test', 'val']:\n            metadata[f'{key}_related_only_size'] = related_only_count[key]\n        root_metadata[self.DATASET_NAME] = metadata\n        with open(METADATA_FILE_NAME, \"w\") as f:\n            json.dump(root_metadata, f, indent=4)\n\n    def _find_sub_obj_pos(self, input_ids_list: Iterable) -> DataFrame:\n        \"\"\"\n        Find subject and object position in a sentence\n        \"\"\"\n        sub_start_pos = [self._index(s, self.SUB_START_ID) + 1 for s in input_ids_list]\n        sub_end_pos = [self._index(s, self.SUB_END_ID, sub_start_pos[i]) for i, s in enumerate(input_ids_list)]\n        obj_start_pos = [self._index(s, self.OBJ_START_ID) + 1 for s in input_ids_list]\n        obj_end_pos = [self._index(s, self.OBJ_END_ID, obj_start_pos[i]) for i, s in enumerate(input_ids_list)]\n        return DataFrame({\n            'sub_start_pos': sub_start_pos,\n            'sub_end_pos': sub_end_pos,\n            'obj_start_pos': obj_start_pos,\n            'obj_end_pos': obj_end_pos,\n        })\n\n    @staticmethod\n    def _index(lst: list, ele: int, start: int = 0) -> int:\n        \"\"\"\n        Find an element in a list. Returns -1 if not found instead of raising an exception.\n        \"\"\"\n        try:\n            return lst.index(ele, start)\n        except ValueError:\n            return -1\n\n    def _remove_invalid_sentences(self, data: DataFrame) -> DataFrame:\n        \"\"\"\n        Remove sentences without subject/object or whose subject/object\n        is beyond the maximum length the model supports\n        \"\"\"\n        seq_max_len = self.tokenizer.model_max_length\n        return data.loc[\n            (data['sub_end_pos'] < seq_max_len)\n            & (data['obj_end_pos'] < seq_max_len)\n            & (data['sub_end_pos'] > -1)\n            & (data['obj_end_pos'] > -1)\n        ]\n\n    @staticmethod\n    def _get_label_mapping(le: LabelEncoder):\n        \"\"\"\n        Returns a mapping from id to label and vise versa from the label encoder\n        \"\"\"\n        # all labels\n        id_to_label = dict(enumerate(le.classes_))\n        label_to_id = {v: k for k, v in id_to_label.items()}\n\n        # for the related_only dataset\n        # ignore id 0, which represent no relation\n        id_to_label_related_only = {k - 1: v for k, v in id_to_label.items() if k != 0}\n        label_to_id_related_only = {v: k for k, v in id_to_label_related_only.items()}\n\n        return {\n            'id_to_label': id_to_label,\n            'label_to_id': label_to_id,\n            'id_to_label_related_only': id_to_label_related_only,\n            'label_to_id_related_only': label_to_id_related_only,            \n        }\n\n    @staticmethod\n    def _append_data_to_file(data: DataFrame, file: TextIO):\n        \"\"\"Append data from a dataframe to an opened file\"\"\"\n        lines = \"\"\n        for _, row in data.iterrows():\n            lines += row.to_json() + \"\\n\"\n        file.write(lines)\n\n    def _save_metadata(self, metadata: dict):\n        \"\"\"Save metadata to metadata.json\"\"\"\n        # create metadata file\n        if not os.path.exists(METADATA_FILE_NAME):\n            print(f\"Create metadata file at {METADATA_FILE_NAME}\")\n            with open(METADATA_FILE_NAME, 'w') as f:\n                f.write(\"{}\\n\")\n\n        # add metadata\n        print(\"Saving metadata\")\n        with open(METADATA_FILE_NAME) as f:\n            root_metadata = json.load(f)\n        with open(METADATA_FILE_NAME, 'w') as f:\n            root_metadata[self.DATASET_NAME] = metadata\n            json.dump(root_metadata, f, indent=4)\n\n    def _get_label_encoder(self) -> OrdinalLabelEncoder:\n        \"\"\"\n        Factory method for label encoder\n        Ensure that \"no relation\" has id 0\n        \"\"\"\n        return OrdinalLabelEncoder([self.NO_RELATION_LABEL])\n\n    @classmethod\n    def get_dataset_file_name(cls, key: str) -> str:\n        return os.path.join(PROCESSED_DATA_DIR, f'{cls.DATASET_NAME.lower()}_{key}.json')\n\n\nclass SemEval2010Task8Preprocessor(AbstractPreprocessor):\n    DATASET_NAME = 'SemEval2010Task8'\n    NO_RELATION_LABEL = 'Other'\n    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING['SemEval2010Task8']['dir'],\n                                       'SemEval2010_task8_training/TRAIN_FILE.TXT')\n    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING['SemEval2010Task8']['dir'],\n                                      'SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT')\n    RAW_TRAIN_DATA_SIZE = 8000\n    RAW_TEST_DATA_SIZE = 2717\n\n    def _preprocess_data(self):\n        print(\"Processing training data\")\n        train_data = self._get_data_from_file(\n            self.RAW_TRAIN_FILE_NAME,\n            self.RAW_TRAIN_DATA_SIZE\n        )\n\n        print(\"Processing test data\")\n        test_data = self._get_data_from_file(\n            self.RAW_TEST_FILE_NAME,\n            self.RAW_TEST_DATA_SIZE\n        )\n\n        print(\"Encoding labels to integers\")\n        le = self._get_label_encoder()\n        train_data['label'] = le.fit_transform(train_data['label'])\n        test_data['label'] = le.transform(test_data['label'])\n\n        print(\"Splitting train & validate data\")\n        train_data, val_data = train_test_split(train_data, shuffle=True, random_state=SEED)\n\n        print(\"Saving to json files\")\n        with open(self.get_dataset_file_name('train'), 'w') as f:\n            self._append_data_to_file(train_data, f)\n        with open(self.get_dataset_file_name('val'), 'w') as f:\n            self._append_data_to_file(val_data, f)\n        with open(self.get_dataset_file_name('test'), 'w') as f:\n            self._append_data_to_file(test_data, f)\n\n        self._save_metadata({\n            'train_size': len(train_data),\n            'val_size': len(val_data),\n            'test_size': len(test_data),\n            'no_relation_label': self.NO_RELATION_LABEL,\n            **self._get_label_mapping(le)\n        })\n\n    def _get_data_from_file(self, file_name: str, dataset_size: int, reverse: bool = ADD_REVERSE_RELATIONSHIP) -> DataFrame:\n        raw_sentences = []\n        labels = []\n        with open(file_name) as f:\n            for _ in tqdm(range(dataset_size)):\n                sent = f.readline()\n                label, sub, obj = self._process_label(f.readline())\n                labels.append(label)\n                raw_sentences.append(self._process_sentence(sent, sub, obj))\n                if label == 'Other' and reverse:\n                    labels.append(label)\n                    raw_sentences.append(self._process_sentence(sent, obj, sub))\n                f.readline()\n                f.readline()\n        tokens = self.tokenizer(raw_sentences, truncation=True, padding=\"max_length\")\n        data = DataFrame(tokens.data)\n        data['label'] = labels\n        sub_obj_position = self._find_sub_obj_pos(data['input_ids'])\n        data = pd.concat([data, sub_obj_position], axis=1)\n        data = self._remove_invalid_sentences(data)\n        return data\n\n    @staticmethod\n    def _process_sentence(sentence: str, sub: int, obj: int) -> str:\n        return sentence.split(\"\\t\")[1][1:-2] \\\n            .replace(f\"<e{sub}>\", SUB_START_CHAR) \\\n            .replace(f\"</e{sub}>\", SUB_END_CHAR) \\\n            .replace(f\"<e{obj}>\", OBJ_START_CHAR) \\\n            .replace(f\"</e{obj}>\", OBJ_END_CHAR)\n\n    @staticmethod\n    def _process_label(label: str) -> Tuple[str, int, int]:\n        label = label.strip()\n        if label == 'Other':\n            return label, 1, 2\n        nums = list(filter(str.isdigit, label))\n        return label, int(nums[0]), int(nums[1])\n\n\nclass GID_NYT_BasePreprocessor(AbstractPreprocessor):\n    \"\"\"Base preprocessor class for GIDS and NYT datasets\"\"\"\n\n    PROCESS_BATCH_SIZE = 2**12\n    NO_RELATION_LABEL = 'NA'\n\n    def _preprocess_data(self):\n        pass\n\n    def _process_batch(self, le: LabelEncoder, in_file: TextIO) -> DataFrame:\n        \"\"\"\n        Process one batch\n        \"\"\"\n        raw_sentences = []\n        labels = []\n\n        for _ in range(self.PROCESS_BATCH_SIZE):\n            dt = in_file.readline()\n            if dt == \"\": break # EOF\n            dt = json.loads(dt)\n\n            # add subject markup\n            sub = dt['sub']  # TODO keep _ or not?\n            obj = dt['obj']\n            new_sub = SUB_START_CHAR + ' ' + sub.replace(\"_\", \"\") + ' ' + SUB_END_CHAR\n            new_obj = OBJ_START_CHAR + ' ' +  obj.replace(\"_\", \"\") + ' ' + OBJ_END_CHAR\n            self._replace_word_once(dt['sent'], sub, new_sub)\n            self._replace_word_once(dt['sent'], obj, new_obj)\n            raw_sentences.append(\" \".join(dt['sent']))\n            labels.append(dt['rel'])\n\n        if not raw_sentences:\n            return DataFrame()\n\n        tokens = self.tokenizer(raw_sentences, truncation=True, padding=\"max_length\")\n        data = DataFrame(tokens.data)\n        data['label'] = le.fit_transform(labels)\n        sub_obj_position = self._find_sub_obj_pos(data['input_ids'])\n        data = pd.concat([data, sub_obj_position], axis=1)\n        data = self._remove_invalid_sentences(data)\n        return data\n\n    @staticmethod\n    def _replace_word_once(arr: list, element: str, replacement: str):\n        \"\"\"\n        Replace a word in a list of words by another\n        Also take care of punctuations\n        \"\"\"\n        for i, e in enumerate(arr):\n            # exact match\n            if e == element:\n                arr[i] = replacement\n                return\n            # check for elements that ends with a special character\n            if e[:-1] == element and e[-1] in ',.?!;:':\n                arr[i] = replacement + e[-1]\n                return\n\n    def _process_file(self, le: LabelEncoder, in_file_name: str, out_file_name: str, data_size: int) -> int:\n        \"\"\"\n        Process a file in batches\n        Return the total data size\n        \"\"\"\n        total_data_size = 0\n        batch_count = math.ceil(data_size / self.PROCESS_BATCH_SIZE)\n\n        with open(in_file_name) as in_file, open(out_file_name, 'w') as out_file:\n            for _ in tqdm(range(batch_count)):\n                data = self._process_batch(le, in_file)\n                self._append_data_to_file(data, out_file)\n                total_data_size += len(data)\n\n        return total_data_size\n\n\nclass GIDSPreprocessor(GID_NYT_BasePreprocessor):\n    DATASET_NAME = 'GIDS'\n    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING['GIDS']['dir'], 'gids_train.json')\n    RAW_VAL_FILE_NAME = os.path.join(DATASET_MAPPING['GIDS']['dir'], 'gids_dev.json')\n    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING['GIDS']['dir'], 'gids_test.json')\n    TRAIN_SIZE = 11297\n    VAL_SIZE = 1864\n    TEST_SIZE = 5663\n    PROCESS_BATCH_SIZE = 1024\n\n    def _preprocess_data(self):\n        le = self._get_label_encoder()\n        \n        print(\"Process train dataset\")\n        actual_train_size = self._process_file(\n            le,\n            self.RAW_TRAIN_FILE_NAME,\n            self.get_dataset_file_name('train'),\n            self.TRAIN_SIZE\n        )\n\n        print(\"Process val dataset\")\n        actual_val_size = self._process_file(\n            le,\n            self.RAW_VAL_FILE_NAME,\n            self.get_dataset_file_name('val'),\n            self.VAL_SIZE\n        )\n        \n        print(\"Process test dataset\")\n        actual_test_size = self._process_file(\n            le, \n            self.RAW_TEST_FILE_NAME, \n            self.get_dataset_file_name('test'),\n            self.TEST_SIZE\n        )\n\n        self._save_metadata({\n            'train_size': actual_train_size,\n            'val_size': actual_val_size,\n            'test_size': actual_test_size,\n            'no_relation_label': self.NO_RELATION_LABEL,\n            **self._get_label_mapping(le)\n        })\n\n\nclass NYTPreprocessor(GID_NYT_BasePreprocessor):\n    DATASET_NAME = 'NYT'\n    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING['NYT']['dir'], 'riedel_train.json')\n    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING['NYT']['dir'], 'riedel_test.json')\n    TRAIN_SIZE = 570084\n    TEST_SIZE = 172448\n    PROCESS_BATCH_SIZE = 4096 * 4\n\n    def _preprocess_data(self):\n        le = self._get_label_encoder()\n        actual_train_size = 0\n        actual_val_size = 0\n\n        print(\"Process train & val dataset\")\n        batch_count = math.ceil(self.TRAIN_SIZE / self.PROCESS_BATCH_SIZE)\n        with open(self.RAW_TRAIN_FILE_NAME) as in_file,\\\n                open(self.get_dataset_file_name('train'), 'w') as train_file,\\\n                open(self.get_dataset_file_name('val'), 'w') as val_file:\n                    for _ in tqdm(range(batch_count)):\n                        data = self._process_batch(le, in_file)\n                        train_data, val_data = train_test_split(data, shuffle=True, random_state=SEED)\n                        self._append_data_to_file(train_data, train_file)\n                        self._append_data_to_file(val_data, val_file)\n                        actual_train_size += len(train_data)\n                        actual_val_size += len(val_data)\n\n        print(\"Process test dataset\")\n        actual_test_size = self._process_file(\n            le, \n            self.RAW_TEST_FILE_NAME, \n            self.get_dataset_file_name('test'),\n            self.TEST_SIZE\n        )\n\n        self._save_metadata({\n            'train_size': actual_train_size,\n            'val_size': actual_val_size,\n            'test_size': actual_test_size,\n            'no_relation_label': self.NO_RELATION_LABEL,\n            **self._get_label_mapping(le)\n        })\n        \n\ndef get_preprocessor_class(dataset_name: str = DATASET_NAME):\n    return globals()[f'{dataset_name}Preprocessor']\n        \ndef get_preprocessor(dataset_name: str = DATASET_NAME)-> AbstractPreprocessor:\n    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL, use_fast=True)\n    # some tokenizer, like GPTTokenizer, doesn't have pad_token\n    # in this case, we use eos token as pad token\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        \n    preprocessors_class = get_preprocessor_class(dataset_name)\n    return preprocessors_class(tokenizer)","execution_count":6,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"preprocessor = get_preprocessor()\npreprocessor.preprocess_data(reprocess=True)","execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecd186c17629495a873dda1d2e9d58d6"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2756b3736e1f4820a5c8832b1cdec060"}},"metadata":{}},{"output_type":"stream","text":"\n\n---> Preprocessing SemEval2010Task8 dataset <---\nCreating processed data directory /kaggle/working/data/processed\nProcessing training data\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=8000.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c052c08f8b94e03a651bcdab02043f0"}},"metadata":{}},{"output_type":"stream","text":"\nProcessing test data\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=2717.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cee7e009bea84c98af9390989dcd1949"}},"metadata":{}},{"output_type":"stream","text":"\nEncoding labels to integers\nSplitting train & validate data\nSaving to json files\nCreate metadata file at /kaggle/working/data/processed/metadata.json\nSaving metadata\nCreating secondary files for train data\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=6000.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ca89d574cd9481e889ad888609ebf3b"}},"metadata":{}},{"output_type":"stream","text":"\nCreating secondary files for test data\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=2717.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c3c94566f73430bacde85af50f6c234"}},"metadata":{}},{"output_type":"stream","text":"\nCreating secondary files for val data\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a022b93094744eb29ebbf04ac9fbdacf"}},"metadata":{}},{"output_type":"stream","text":"\nUpdating metadata.json\n---> Done ! <---\n","name":"stdout"}]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## Model\n\n### Dataset"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"scrolled":false},"cell_type":"code","source":"class GenericDataset(IterableDataset):\n    \"\"\"A generic dataset for train/val/test data for SemEval, GIDS & NYT\"\"\"\n\n    def __init__(self, dataset_name: str, subset: str, batch_size: int, label_transform: str):\n        assert subset in ['train', 'val', 'test']\n        assert label_transform in ['none', 'binary', 'related_only']\n\n        file_name = subset if label_transform == 'none' \\\n            else f'{subset}_{label_transform}'\n\n        preprocessor_class = get_preprocessor_class()\n        with open(METADATA_FILE_NAME) as f:\n            metadata = json.load(f)[dataset_name]\n\n        size = metadata[f'{subset}_related_only_size'] \\\n            if label_transform is 'related_only' \\\n            else metadata[f'{subset}_size']\n\n        self.subset = subset\n        self.batch_size = batch_size\n        self.length = math.ceil(size / batch_size)\n        self.fit_in_memory = DATASET_MAPPING[dataset_name]['fit_in_memory']\n        self.file = open(preprocessor_class.get_dataset_file_name(file_name))\n        \n        print(\"__init__\", self.subset, self.batch_size, self.length, self.fit_in_memory)\n\n    def __del__(self):\n        if self.file:\n            self.file.close()\n\n    def __do_smart_batching(self):\n        # https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e\n        data = [json.loads(line) for line in self.file]\n        data = sorted(data, key=lambda x: sum(x['attention_mask']))\n\n        new_data = []\n\n        while len(data) > 0:\n            idx = 0 if len(data) < self.batch_size else randint(0, len(data) - self.batch_size)\n            batch = data[idx:idx + self.batch_size]\n            max_len = max([sum(b['attention_mask']) for b in batch])\n\n            for b in batch:\n                input_data = {}\n                for k, v in b.items():\n                    if k != 'label':\n                        if isinstance(v, list):\n                            input_data[k] = torch.tensor(v[:max_len])\n                        else:\n                            input_data[k] = torch.tensor(v)\n                label = torch.tensor(b['label'])\n                new_data.append((input_data, label))\n\n            del data[idx:idx+ self.batch_size]\n        print(\"smart: \", len(new_data))\n        print(\"smart: \", new_data[0])\n        yield from new_data\n\n    @staticmethod\n    def __loop_through(target):\n        for line in target:\n            data = json.loads(line)\n            input_data = {k: torch.tensor(v) for k, v in data.items() if k != 'label'}\n            label = torch.tensor(data['label'])\n            yield input_data, label\n\n    def __iter__(self):\n        target = self.file\n        if self.fit_in_memory:\n#             if self.subset == 'train':\n#                 return self.__do_smart_batching()\n            target = self.file.readlines()\n            print(\"loop: \", len(target))\n            print(\"loop: \",target[0])\n        return self.__loop_through(target)\n    \n    def __len__(self):\n        return self.length\n\n    def as_batches(self):\n        input_data = []\n        label = []\n        \n        def create_batch():\n            return (\n                {k: torch.stack([x[k] for x in input_data]).cuda() for k in input_data[0].keys()},\n                torch.tensor(label).cuda()\n            )\n        \n        for ip, l in self:\n            input_data.append(ip)\n            label.append(l)\n            if len(input_data) == self.batch_size:\n                yield create_batch()\n                input_data.clear()\n                label.clear()\n\n        yield create_batch()","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"dataset = GenericDataset(\n            dataset_name=DATASET_NAME,\n            subset=\"val\",\n            batch_size=16,\n            label_transform=\"binary\",\n        )\nprint(len(dataset))\nds = [d for d in dataset]\nprint(len(ds))","execution_count":12,"outputs":[{"output_type":"stream","text":"__init__ val 16 125 True\n125\nloop:  2000\nloop:  {\"input_ids\": [101, 24062, 1005, 1055, 1031, 20228, 10136, 1033, 2024, 2019, 6827, 8875, 1999, 1996, 1063, 3751, 2937, 1065, 1005, 1055, 6994, 13711, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"token_type_ids\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"label\": 1, \"sub_start_pos\": 5, \"sub_end_pos\": 7, \"obj_start_pos\": 15, \"obj_end_pos\": 17}\n\n2000\n","name":"stdout"}]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"### Torch Lightning Module"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"scrolled":false},"cell_type":"code","source":"class BaseClassifier(LightningModule, ABC):\n    def __init__(self, dataset_label_transform: str):\n        super().__init__()\n        assert dataset_label_transform in ['none', 'binary', 'related_only']\n        self.dataset_label_transform = dataset_label_transform\n\n    @abstractmethod\n    def loss_function(self, logits: Tensor, label: Tensor) -> Tensor:\n        \"\"\"\n        Calculate the loss of the model\n        It MUST take care of the last activation layer\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def log_metrics(self, epoch_type: str, logits: Tensor, label: Tensor) -> dict:\n        pass\n\n    def train_dataloader(self) -> DataLoader:\n        return self.__get_dataloader('train')\n\n    def val_dataloader(self) -> DataLoader:\n        return self.__get_dataloader('val')\n\n    def test_dataloader(self) -> DataLoader:\n        return self.__get_dataloader('test')\n\n    def __get_dataloader(self, subset: str) -> DataLoader:\n        batch_size = self.hparams.batch_size\n        dataset = GenericDataset(\n            dataset_name=self.hparams.dataset_name,\n            subset=subset,\n            batch_size=batch_size,\n            label_transform=self.dataset_label_transform,\n        )\n        return DataLoader(\n            dataset,\n            batch_size=batch_size,\n            num_workers=1\n        )\n\n    def configure_optimizers(self):\n        optimizer = AdamW(\n            [p for p in self.parameters() if p.requires_grad],\n            lr=self.hparams.learning_rate,\n            weight_decay=self.hparams.weight_decay\n        )\n        scheduler = LambdaLR(optimizer, lambda epoch: self.hparams.decay_lr_speed[epoch])\n        return [optimizer], [scheduler]\n    \n    def training_step(self, batch: Tuple[dict, Tensor], batch_nb: int) -> dict:\n        input_data, label = batch\n        logits = self(**input_data)\n\n        loss = self.loss_function(logits, label)\n        log = {'train_loss': loss}\n\n        return {'loss': loss, 'log': log}\n\n    def __eval_step(self, batch:  Tuple[dict, Tensor]) -> dict:\n        input_data, label = batch\n        logits = self(**input_data)\n\n        return {\n            'logits': logits,\n            'label': label,\n        }\n    \n    def validation_step(self, batch: Tuple[dict, Tensor], batch_nb: int) -> dict:\n        return self.__eval_step(batch)\n    \n    def test_step(self, batch: Tuple[dict, Tensor], batch_nb: int) -> dict:\n        return self.__eval_step(batch)\n\n    def __eval_epoch_end(self, epoch_type: str, outputs: Iterable[dict]) -> dict:\n        assert epoch_type in ['test', 'val']\n        \n        logits = torch.cat([x['logits'] for x in outputs]).cpu().float()\n        label = torch.cat([x['label'] for x in outputs]).cpu().float()\n        \n        logs = self.log_metrics(epoch_type, logits, label)\n        \n        return {'progress_bar': logs}\n    \n    def validation_epoch_end(self, outputs: Iterable[dict]) -> dict:\n        return self.__eval_epoch_end('val', outputs)\n\n    def test_epoch_end(self, outputs: Iterable[dict]) -> dict:\n        return self.__eval_epoch_end('test', outputs)\n    \n    @staticmethod\n    def plot_confusion_matrix(predicted_label: Tensor, label: Tensor) -> Figure:\n        result = confusion_matrix(label, predicted_label)\n        display = ConfusionMatrixDisplay(result)\n        fig, ax = plt.subplots(figsize=(16, 12))\n        display.plot(cmap=plt.cm.get_cmap(\"Blues\"), ax=ax)\n        return fig\n    \n    def log_confusion_matrix(self, prefix: str, predicted_label: Tensor, label: Tensor):\n        fig = self.plot_confusion_matrix(predicted_label, label)\n        self.logger.experiment.log_image(f'{prefix}_confusion_matrix', fig)\n\n\nclass BinaryClassifier(BaseClassifier):\n\n    def __init__(self, pretrained_language_model, dataset_name, batch_size, learning_rate, decay_lr_speed,\n                 linear_size, dropout_p, activation_function, weight_decay, use_fp16):\n        super().__init__(dataset_label_transform=\"binary\")\n        self.save_hyperparameters()\n        self.thresholds = {}\n\n        self.language_model = AutoModel.from_pretrained(pretrained_language_model)\n        config = self.language_model.config\n        self.max_seq_len = config.max_position_embeddings\n        self.hidden_size = config.hidden_size\n        \n        self.dropout = nn.Dropout(p=dropout_p)\n        self.linear = nn.Linear(self.hidden_size * self.max_seq_len, linear_size)\n        self.activation_function = getattr(nn, activation_function)()\n        self.linear_output = nn.Linear(linear_size, 1)\n        \n        self.fp_dtype = torch.float16 if use_fp16 else torch.float32\n\n    @staticmethod\n    def yhat_to_label(y_hat: Tensor, threshold: float) -> Tensor:\n        return (y_hat > threshold).long()\n\n    def loss_function(self, logits: Tensor, label: Tensor) -> Tensor:\n        return F.binary_cross_entropy_with_logits(logits, label.float())\n\n    def forward(self, sub_start_pos, sub_end_pos,\n                obj_start_pos, obj_end_pos, *args, **kwargs) -> Tensor:\n        print(kwargs['input_ids'].shape)\n        language_model_output = self.language_model(*args, **kwargs)\n        if isinstance(language_model_output, tuple):\n            language_model_output = language_model_output[0]\n        \n        print(language_model_output.shape)\n        \n        bz, seq_len, _ = language_model_output.shape\n        x = torch.zeros([bz, self.max_seq_len, self.hidden_size], dtype=self.fp_dtype).cuda()\n        print(x.shape)\n        x[:,:seq_len,:] = language_model_output\n        print(x.shape)\n\n        x = x.reshape([bz, -1])\n        print(x.shape)\n        x = self.dropout(x)\n        print(x.shape)\n        x = self.linear(x)\n        print(x.shape)\n        x = self.activation_function(x)\n        print(x.shape)\n        # x = self.dropout(x) # ??\n        logits = self.linear_output(x).reshape(-1)\n        print(logits.shape)\n\n        return logits\n    \n    def log_metrics(self, epoch_type: str, logits: Tensor, label: Tensor) -> dict:\n        y_hat = torch.sigmoid(logits)\n        \n        if epoch_type == 'val':\n            self.__find_thresholds(y_hat, label)\n        \n        self.__log_output_distribution(epoch_type, y_hat, label)\n        \n        logs = {\n            f'{epoch_type}_avg_loss': float(self.loss_function(logits, label)),\n            f'{epoch_type}_roc_auc': self.__roc_auc_score(label, y_hat)\n        }\n        \n        for criteria, threshold in self.thresholds.items():\n            prefix = f\"{epoch_type}_{criteria}\"\n            predicted_label = self.yhat_to_label(y_hat, threshold)\n            self.log_confusion_matrix(prefix, predicted_label, label)\n            \n            logs[f'{prefix}_acc'] = accuracy_score(label, predicted_label)\n            logs[f'{prefix}_pre'] = precision_score(label, predicted_label, average='binary')\n            logs[f'{prefix}_rec'] = recall_score(label, predicted_label, average='binary')\n            logs[f'{prefix}_f1'] = f1_score(label, predicted_label, average='binary')\n\n        for k, v in logs.items():\n            self.logger.experiment.log_metric(k, v)\n            \n        return logs\n    \n    def __roc_auc_score(self, label: Tensor, y_hat: Tensor) -> float:\n        try:\n            return float(roc_auc_score(label, y_hat))\n        except ValueError:\n            return 0\n    \n    def __find_thresholds(self, y_hat: Tensor, label: Tensor):\n        \"\"\"\n        Find 3 classification thresholds based on 3 criteria:\n        - The one that yields highest accuracy\n        - The \"best point\" in the ROC curve\n        - The one that yields highest f1\n        The results are logged and stored in self.threshold\n        \"\"\"\n        # best accuracy\n        best_acc = 0\n        best_acc_threshold = None\n        for y in y_hat:\n            y_predicted = self.yhat_to_label(y_hat, threshold=y)\n            acc = accuracy_score(label, y_predicted)\n            if best_acc < acc:\n                best_acc = acc\n                best_acc_threshold = y\n        self.thresholds['best_acc'] = best_acc_threshold\n\n        # ROC curve\n        # https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n        fpr, tpr, thresholds = roc_curve(label, y_hat)\n        gmeans = tpr * (1 - fpr)\n        ix = np.argmax(gmeans)\n        self.thresholds['best_roc'] = thresholds[ix]\n        self.logger.experiment.log_metric(\"best_roc_threshold\", thresholds[ix])\n\n        fig, ax = plt.subplots(figsize=(16, 12))\n        ax.plot([0,1], [0,1], linestyle='--', label='No Skill')\n        ax.plot(fpr, tpr, marker='.', label='Logistic')\n        ax.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n        ax.set_xlabel('False Positive Rate')\n        ax.set_ylabel('True Positive Rate')\n        ax.legend()\n        self.logger.experiment.log_image('roc_curve', fig)\n\n        # precision recall curve\n        # https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n        pre, rec, thresholds = precision_recall_curve(label, y_hat)\n        f1s = 2 * pre * rec / (pre + rec)\n        ix = np.argmax(f1s)\n        self.thresholds['best_f1'] = thresholds[ix]\n        self.logger.experiment.log_metric(\"best_f1_threshold\", thresholds[ix])\n\n        fig, ax = plt.subplots(figsize=(16, 12))\n        no_skill = len(label[label == 1]) / len(label)\n        ax.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n        ax.plot(rec, pre, marker='.', label='Logistic')\n        ax.scatter(rec[ix], pre[ix], marker='o', color='black', label='Best F1')\n        ax.set_xlabel('Recall')\n        ax.set_ylabel('Precision')\n        ax.legend()\n        self.logger.experiment.log_image('pre_rec_curve', fig)\n\n        # log thresholds\n        for k, v in self.thresholds.items():\n            self.logger.experiment.log_metric(f'threshold_{k}', v)\n\n    def __log_output_distribution(self, epoch_type: str, y_hat: Tensor, label: Tensor):\n        \"\"\"\n        Log the distribution of the model output and 3 thresholds with log scale and linear scale\n        \"\"\"\n        y_neg = y_hat[label == 0].numpy()\n        y_pos = y_hat[label == 1].numpy()\n\n        for scale in ['linear', 'log']:\n            fig, ax = plt.subplots(figsize=(16, 12))\n            ax.set_yscale(scale)\n            ax.hist([y_neg, y_pos], stacked=True, bins=50, label=[\"No relation\", \"Related\"])\n            ylim = ax.get_ylim()\n            for k, v in self.thresholds.items():\n                ax.plot([v, v], ylim, linestyle='--', label=f'{k} threshold')\n            ax.legend()\n            self.logger.experiment.log_image(f'{epoch_type}_distribution_{scale}_scale', fig)\n        \n\nclass RelationClassifier(BaseClassifier):\n\n    def __init__(self, pretrained_language_model, dataset_name, batch_size, learning_rate, decay_lr_speed,\n                 dropout_p, weight_decay):\n        super().__init__(dataset_label_transform=\"related_only\")\n        self.save_hyperparameters()\n\n        with open(METADATA_FILE_NAME) as f:\n            num_classes = len(json.load(f)[dataset_name]['label_to_id'])\n\n        self.language_model = AutoModel.from_pretrained(pretrained_language_model)\n        self.dropout = nn.Dropout(p=dropout_p)\n        self.linear = nn.Linear(self.language_model.config.hidden_size, num_classes)\n\n    @staticmethod\n    def logits_to_label(logits: Tensor) -> Tensor:\n        return torch.argmax(logits, dim=-1)\n\n    def loss_function(self, logits: Tensor, label: Tensor) -> Tensor:\n        return F.cross_entropy(logits, label)\n    \n    def forward(self, sub_start_pos, sub_end_pos,\n                obj_start_pos, obj_end_pos, *args, **kwargs) -> Tensor:\n        \n        language_model_output = self.language_model(*args, **kwargs)\n        if isinstance(language_model_output, tuple):\n            language_model_output = language_model_output[0]\n        x = torch.mean(language_model_output, dim=1)\n        x = self.dropout(x)\n        logits = self.linear(x)\n\n        return logits\n\n    def log_metrics(self, epoch_type: str, logits: Tensor, label: Tensor) -> dict:\n        predicted_label = self.logits_to_label(logits)\n        self.log_confusion_matrix(epoch_type, predicted_label, label)\n        \n        logs = {\n            f'{epoch_type}_avg_loss': float(self.loss_function(logits, label)),\n            f'{epoch_type}_acc': accuracy_score(label, predicted_label),\n            f'{epoch_type}_pre_micro': precision_score(label, predicted_label, average='micro'),\n            f'{epoch_type}_rec_micro': recall_score(label, predicted_label, average='micro'),\n            f'{epoch_type}_f1_micro': f1_score(label, predicted_label, average='micro'),\n            f'{epoch_type}_pre_macro': precision_score(label, predicted_label, average='macro'),\n            f'{epoch_type}_rec_macro': recall_score(label, predicted_label, average='macro'),\n            f'{epoch_type}_f1_macro': f1_score(label, predicted_label, average='macro'),\n        }\n        \n        for k, v in logs.items():\n            self.logger.experiment.log_metric(k, v)\n            \n        return logs\n","execution_count":28,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Claiming back memory & disk space\n\nSee [this](https://stackoverflow.com/a/61707643/7342188) and [this](https://stackoverflow.com/a/57860310/7342188)"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"1 / 0","execution_count":32,"outputs":[{"output_type":"error","ename":"ZeroDivisionError","evalue":"division by zero","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-bc757c3fda29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"]}]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"binary_classifier = bin_trainer = None","execution_count":33,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"relation_classifier = trainer = None","execution_count":34,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","execution_count":35,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"bin_logger = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"logger = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"pycharm":{"name":"#%%\n"},"scrolled":false},"cell_type":"code","source":"try:\n    shutil.rmtree(CHECKPOINT_DIR)\nexcept:\n    pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training binary classifier\n"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"bin_logger = NeptuneLogger(\n    api_key=NEPTUNE_API_TOKEN,\n    project_name=NEPTUNE_PROJECT_NAME,\n    close_after_fit=False,\n)","execution_count":16,"outputs":[{"output_type":"stream","text":"https://ui.neptune.ai/hung/bert-relation-extraction/e/BERT-141\n","name":"stdout"},{"output_type":"stream","text":"NeptuneLogger will work in online mode\n","name":"stderr"}]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"scrolled":false},"cell_type":"code","source":"GPUS = 1\n\nBIN_MIN_EPOCHS = 1\nBIN_MAX_EPOCHS = 1\n\nUSE_16_BIT_FP = False\n\nfp_options = {\"amp_level\": \"O1\", \"precision\": 16} if USE_16_BIT_FP else {}\n\nbin_trainer = LightningTrainer(\n    gpus=GPUS,\n    min_epochs=BIN_MIN_EPOCHS,\n    max_epochs=BIN_MAX_EPOCHS,\n    default_root_dir=CHECKPOINT_DIR,\n    reload_dataloaders_every_epoch=True, # needed as we loop over a file,\n    logger=bin_logger,\n    checkpoint_callback=False,\n    **fp_options,\n)","execution_count":36,"outputs":[{"output_type":"stream","text":"GPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n","name":"stderr"}]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"BIN_BATCH_SIZE = 16\nBIN_LEARNING_RATE = 2e-05\nBIN_LEARNING_RATE_DECAY_SPEED = [1, 1, 0.75, 0.5, 0.5, 0.25, 0.25, 0.1, 0.075, 0.05, 0.025, 0.01]\n\nBIN_LINEAR_SIZE = 1024\n\nBIN_DROPOUT_P = 0.2\nBIN_ACTIVATION_FUNCTION = \"PReLU\"\nBIN_WEIGHT_DECAY = 0.01 # default = 0.01\n\nbinary_classifier = BinaryClassifier(\n    pretrained_language_model=PRETRAINED_MODEL,\n    dataset_name=DATASET_NAME,\n    batch_size=BIN_BATCH_SIZE,\n    learning_rate=BIN_LEARNING_RATE,\n    decay_lr_speed=BIN_LEARNING_RATE_DECAY_SPEED,\n    linear_size=BIN_LINEAR_SIZE,\n    dropout_p=BIN_DROPOUT_P,\n    activation_function=BIN_ACTIVATION_FUNCTION,\n    weight_decay=BIN_WEIGHT_DECAY,\n    use_fp16=USE_16_BIT_FP,\n)\n","execution_count":37,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"_kg_hide-output":true,"_kg_hide-input":false,"scrolled":false},"cell_type":"code","source":"bin_trainer.fit(binary_classifier)","execution_count":39,"outputs":[{"output_type":"stream","text":"\n  | Name                | Type      | Params\n--------------------------------------------------\n0 | language_model      | BertModel | 109 M \n1 | dropout             | Dropout   | 0     \n2 | linear              | Linear    | 402 M \n3 | activation_function | PReLU     | 1     \n4 | linear_output       | Linear    | 1 K   \n","name":"stderr"},{"output_type":"stream","text":"__init__ val 16 125 True\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd83938884ec4d4aa80660a1e20de87e"}},"metadata":{}},{"output_type":"stream","text":"loop:  2000\nloop:  {\"input_ids\": [101, 24062, 1005, 1055, 1031, 20228, 10136, 1033, 2024, 2019, 6827, 8875, 1999, 1996, 1063, 3751, 2937, 1065, 1005, 1055, 6994, 13711, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"token_type_ids\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"label\": 1, \"sub_start_pos\": 5, \"sub_end_pos\": 7, \"obj_start_pos\": 15, \"obj_end_pos\": 17}\n\ntorch.Size([16, 512])\ntorch.Size([16, 512, 768])\ntorch.Size([16, 512, 768])\ntorch.Size([16, 512, 768])\ntorch.Size([16, 393216])\ntorch.Size([16, 393216])\n","name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"expected scalar type Float but found Half","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-c8f7278293b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbin_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tpu\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no-cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\u001b[0m in \u001b[0;36msingle_gpu_train\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreinit_scheduler_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_schedulers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_core_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1137\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m                                           \u001b[0mmax_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m                                           False)\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;31m# allow no returns from eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, model, dataloaders, max_batches, test_mode)\u001b[0m\n\u001b[1;32m    291\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0;31m# on dp / ddp2 might still want to do something with the batch parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\u001b[0m in \u001b[0;36mevaluation_forward\u001b[0;34m(self, model, batch, batch_idx, dataloader_idx, test_mode)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-d836b0d41566>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_nb)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_nb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__eval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_nb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-d836b0d41566>\u001b[0m in \u001b[0;36m__eval_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         return {\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-d836b0d41566>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sub_start_pos, sub_end_pos, obj_start_pos, obj_end_pos, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1672\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Half"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = torch.tensor(list(range(24))).reshape((2, 4, 3)).cuda()\nprint(a)\n\nb = torch.zeros([2, 6, 3])\nprint(b)\n\nb[:,:4,:] = a\nprint(b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_trainer.test(binary_classifier)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"bin_logger.experiment.stop()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train relation classifier"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"logger = NeptuneLogger(\n    api_key=NEPTUNE_API_TOKEN,\n    project_name=NEPTUNE_PROJECT_NAME,\n    close_after_fit=False,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"MIN_EPOCHS = 1\nMAX_EPOCHS = 1\n\nUSE_16_BIT_FP = True\n\nfp_options = {\"amp_level\": \"O1\", \"precision\": 16} if USE_16_BIT_FP else {}\n\ntrainer = LightningTrainer(\n    gpus=GPUS,\n    min_epochs=MIN_EPOCHS,\n    max_epochs=MAX_EPOCHS,\n    default_root_dir=CHECKPOINT_DIR,\n    reload_dataloaders_every_epoch=True, # needed as we loop over a file,\n    logger=logger,\n    **fp_options,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"BATCH_SIZE = 8\nLEARNING_RATE = 2e-05\nLEARNING_RATE_DECAY_SPEED = [1, 1, 0.75, 0.5, 0.25, 0.1, 0.075, 0.05, 0.025, 0.01]\n\nLINEAR_SIZE = 256\n\nDROPOUT_P = 0.2\nACTIVATION_FUNCTION = \"Tanh\"\nWEIGHT_DECAY = 0.01 # default = 0.01\n\nrelation_classifier = RelationClassifier(\n    pretrained_language_model=PRETRAINED_MODEL,\n    dataset_name=DATASET_NAME,\n    batch_size=BATCH_SIZE,\n    learning_rate=LEARNING_RATE,\n    decay_lr_speed=LEARNING_RATE_DECAY_SPEED,\n    # linear_size=LINEAR_SIZE,\n    dropout_p=DROPOUT_P,\n    # activation_function=ACTIVATION_FUNCTION,\n    weight_decay=WEIGHT_DECAY,\n)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"scrolled":false},"cell_type":"code","source":"trainer.fit(relation_classifier)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"scrolled":false},"cell_type":"code","source":"trainer.test(relation_classifier)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing 2 classifiers together"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"scrolled":false},"cell_type":"code","source":"def test_together(b_classifier: BinaryClassifier, r_classifier: RelationClassifier, dataset_name: str = DATASET_NAME,\n                  bin_batch_size = BIN_BATCH_SIZE, batch_size: int = BATCH_SIZE) -> dict:\n    b_classifier.freeze()\n    r_classifier.freeze()\n\n    true_answer = []\n\n    # run binary classifier\n    print(\"Running binary classifier\")\n    dataset = GenericDataset(dataset_name, subset='test', batch_size=bin_batch_size, label_transform='none')\n    binary_classify_results = { criteria: [] for criteria in b_classifier.thresholds.keys() }\n\n    for input_data, true_label  in tqdm(dataset.as_batches(), total=len(dataset)):\n        # append true answers\n        true_answer += true_label.tolist()\n\n        # run bin classifier\n        logits = b_classifier(**input_data)\n        y_hat = torch.sigmoid(logits)\n        for criteria, threshold in b_classifier.thresholds.items():\n            label = b_classifier.yhat_to_label(y_hat, threshold)\n            binary_classify_results[criteria] += label.tolist()\n\n    # run relation classifier\n    print(\"Running relation classifier\")\n    dataset = GenericDataset(dataset_name, subset='test', batch_size=batch_size, label_transform='none')\n    relation_classify_result = []\n\n    for input_data, true_label  in tqdm(dataset.as_batches(), total=len(dataset)):\n        logits = r_classifier(**input_data)\n        label = r_classifier.logits_to_label(logits) + 1\n        relation_classify_result += label.tolist()\n\n    # combine results\n    print(\"Combining results\")\n    proposed_answer = {}\n    for criteria in b_classifier.thresholds.keys():\n        results = zip(relation_classify_result, binary_classify_results[criteria])\n        final_label = [relation_result if bin_result else 0 for relation_result, bin_result in results]\n        proposed_answer[criteria] = final_label\n\n    # log metric\n    final_metrics = {}\n    for criteria in b_classifier.thresholds.keys():\n        pa = proposed_answer[criteria]\n        \n        final_metrics.update({\n            f'test_combined_{criteria}_acc': accuracy_score(true_answer, pa),\n            f'test_combined_{criteria}_pre_micro': precision_score(true_answer, pa, average='micro'),\n            f'test_combined_{criteria}_rec_micro': recall_score(true_answer, pa, average='micro'),\n            f'test_combined_{criteria}_f1_micro': f1_score(true_answer, pa, average='micro'),\n            f'test_combined_{criteria}_pre_macro': precision_score(true_answer, pa, average='macro'),\n            f'test_combined_{criteria}_rec_macro': recall_score(true_answer, pa, average='macro'),\n            f'test_combined_{criteria}_f1_macro': f1_score(true_answer, pa, average='macro'),\n        })\n        \n        fig = BaseClassifier.plot_confusion_matrix(pa, true_answer)\n        r_classifier.logger.experiment.log_image(f\"test_combined_{criteria}_confusion_matrix\", fig)\n\n    for k, v in final_metrics.items():\n        print(f\"{k}: {v * 100}\")\n\n    for k, v in final_metrics.items():\n        r_classifier.logger.experiment.log_metric(k, v)\n\n    return proposed_answer","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"scrolled":true},"cell_type":"code","source":"final_answer = test_together(binary_classifier, relation_classifier)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"},"trusted":true},"cell_type":"markdown","source":"## Run the official scorer\n\nSome datasets comes with official scorers. We will run them in this session."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class AbstractScorer(ABC):\n    @abstractmethod\n    def score(self, proposed_answer: dict):\n        pass\n\nclass SemEval2010Task8Scorer(AbstractScorer):\n    RESULT_FILE = \"semeval2010_task8_official_score_{}.txt\"\n    PROPOSED_ANSWER_FILE = \"semeval2010_task8_proposed_answer.txt\"\n    SCORER = os.path.join(DATASET_MAPPING['SemEval2010Task8']['dir'], \"SemEval2010_task8_scorer-v1.2/semeval2010_task8_scorer-v1.2.pl\")\n    FORMAT_CHECKER = os.path.join(DATASET_MAPPING['SemEval2010Task8']['dir'], \"SemEval2010_task8_scorer-v1.2/semeval2010_task8_format_checker.pl\")\n    ANSWER_KEY = os.path.join(DATASET_MAPPING['SemEval2010Task8']['dir'], \"SemEval2010_task8_testing_keys/TEST_FILE_KEY.TXT\")\n\n    def score(self, proposed_answer: dict):\n\n        # write test_result to file\n        with open(METADATA_FILE_NAME) as f:\n            metadata = json.load(f)\n            id_to_label = {int(k): v for k, v in metadata[DATASET_NAME]['id_to_label'].items()}\n\n        for criteria, answer in proposed_answer.items():\n            result_file = self.RESULT_FILE.format(criteria)\n            i = 8001\n            with open(self.PROPOSED_ANSWER_FILE, \"w\") as f:\n                for r in answer:\n                    f.write(f\"{i}\\t{id_to_label[r]}\\n\")\n                    i += 1\n\n            # call the official scorer\n            os.system(f\"perl {self.FORMAT_CHECKER} {self.PROPOSED_ANSWER_FILE}\")\n            os.system(f\"perl {self.SCORER} {self.PROPOSED_ANSWER_FILE} {self.ANSWER_KEY} > {result_file}\")\n\n            # log the official score\n            with open(result_file) as f:\n                result = f.read()\n                print(f\">>> Binary classifier with {criteria} threshold <<<\")\n                print(result)\n                print(\"\\n\\n\")\n            logger.experiment.log_artifact(result_file)\n\ndef get_official_scorer(dataset_name: str = DATASET_NAME) -> AbstractScorer:\n    return globals().get(dataset_name + \"Scorer\")()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"scrolled":false},"cell_type":"code","source":"scorer = get_official_scorer()\nif scorer:\n    scorer.score(final_answer)\nelse:\n    print(\"No official scorer found\")","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## Clean up"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"logger.experiment.stop()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}